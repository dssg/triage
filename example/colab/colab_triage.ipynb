{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dssg/triage/blob/master/example/colab/colab_triage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ix00QRsvd45"
      },
      "source": [
        "# Colab Triage Tutorial\n",
        "\n",
        "## Problem Overview\n",
        "\n",
        "This notebook provides a quick, interactive tutorial for [triage](http://www.datasciencepublicpolicy.org/our-work/tools-guides/triage/), a python machine learning pipeline for social good problems, using a sample of the data provided by [DonorsChoose](https://www.donorschoose.org/) to the [2014 KDD Cup](https://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose/data). Public schools in the United States face large disparities in funding, often resulting in teachers and staff members filling these gaps by purchasing classroom supplies out of their own pockets. DonorsChoose is an online crowdfunding platform that tries to help fill this gap by allowing teachers to seek funding for projects and resources from the community (projects can include classroom basics like books and markers, larger items like lab equipment or musical instruments, specific experiences like field trips or guest speakers). Projects on DonorsChoose expire after 4 months, and if the target funding level isn't reached, the project receives no funding. Since its launch in 2000, the platform has helped fund over 2 million projects at schools across the US, but about 1/3 of the projects that are posted nevertheless fail to meet their goal and go unfunded.\n",
        "\n",
        "For the purposes of this tutorial, we'll imagine that DonorsChoose has hired a digital content expert who will review projects and help teachers improve their postings and increase their chances of reaching their funding threshold. Because this individualized review is a labor-intensive process, the digital content expert has time to review only 10% of the projects posted to the platform on a given day. \n",
        "\n",
        "### The Modeling Problem\n",
        "\n",
        "You are a data scientist working with DonorsChoose, and your task is to help this content expert focus their limited resources on projects that most need the help. As such, you want to build a model to identify projects that are least likely to be fully funded before they expire and pass them off to the digital content expert for review.\n",
        "\n",
        "In building that model, our unit of analysis (what triage calls a **cohort**) will be new projects right at the time they're posted, while the **label** we're seeing to predict is whether or not the project reaches its funding goal in the subsequent 4 months (before it expires), making our task a binary classification problem. In order to make this prediction, we'll develop **features** that include information we know about the project when it's posted as well as historical performance of other projects posted by this teacher, school, etc.\n",
        "\n",
        "### Outline of the Tutorial\n",
        "\n",
        "The remainder of this tutorial will focus on how to use `triage` to solve this problem. Starting from scratch, we'll:\n",
        "- **Install our tools**, including triage and a postgres server.\n",
        "- **Explore the data** to get familiar with its structure.\n",
        "- **Formulate the project** to make sure the models we build meet the needs of the context (and see how to configure `triage` along the way).\n",
        "- **Build models**, using `triage` to run the modeling pipeline.\n",
        "- **Look at the results** to ensure they make sense.\n",
        "- **Select the model to deploy** using the `audition` component of `triage`.\n",
        "- **Audit our models for bias** using `aequitas`.\n",
        "- **Lear about next steps** and where to go from here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtAMABPn971u"
      },
      "source": [
        "## Getting Set Up\n",
        "\n",
        "We'll need a few dependencies to run triage in a colab notebook:\n",
        "- A local postgresql server (we'll use version 11)\n",
        "- A simplified dataset loaded into this database (we'll use data from DonorsChoose)\n",
        "- Triage and its dependencies (we'll use the current version in pypi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-htIBoS7N4gK"
      },
      "outputs": [],
      "source": [
        "# Install and start postgresql-11 server\n",
        "!sudo apt-get -y -qq update\n",
        "!wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n",
        "!echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" |sudo tee  /etc/apt/sources.list.d/pgdg.list\n",
        "!sudo apt-get -y -qq update\n",
        "!sudo apt-get -y -qq install postgresql-11 postgresql-client-11\n",
        "!sudo service postgresql start\n",
        "\n",
        "# Setup a password `postgres` for username `postgres`\n",
        "!sudo -u postgres psql -U postgres -c \"ALTER USER postgres PASSWORD 'postgres';\"\n",
        "\n",
        "# Setup a database with name `donors_choose` to be used\n",
        "!sudo -u postgres psql -U postgres -c 'DROP DATABASE IF EXISTS donors_choose;'\n",
        "\n",
        "!sudo -u postgres psql -U postgres -c 'CREATE DATABASE donors_choose;'\n",
        "\n",
        "# Environment variables for connecting to the database\n",
        "%env DEMO_DATABASE_NAME=donors_choose\n",
        "%env DEMO_DATABASE_HOST=localhost\n",
        "%env DEMO_DATABASE_PORT=5432\n",
        "%env DEMO_DATABASE_USER=postgres\n",
        "%env DEMO_DATABASE_PASS=postgres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mWNhJ2rOVtS"
      },
      "outputs": [],
      "source": [
        "# Download sampled DonorsChoose data and load it into our postgres server\n",
        "!curl -s -OL https://dsapp-public-data-migrated.s3.us-west-2.amazonaws.com/donors_sampled_20210920_v3.dmp\n",
        "!PGPASSWORD=$DEMO_DATABASE_PASS pg_restore -h $DEMO_DATABASE_HOST -p $DEMO_DATABASE_PORT -d $DEMO_DATABASE_NAME -U $DEMO_DATABASE_USER -O -j 8 donors_sampled_20210920_v3.dmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5E-9VRjRlSk"
      },
      "outputs": [],
      "source": [
        "# Install triage and its dependencies\n",
        "%%capture\n",
        "!pip install triage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mQ1nY6lksXD"
      },
      "source": [
        "ðŸ›‘  &nbsp;&nbsp;**WARNING! Be sure to restart the session before moving on to the next section! Otherwise, Colab won't recognize that `triage` has been installed. To restart the session, go to `Runtime` menu and select the option `Restart session`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reskRriKlcpO"
      },
      "source": [
        "## A Quick Look at the DonorsChoose Data\n",
        "\n",
        "Before getting into triage, let's just take a quick look at the data we'll be using here. To get started, we'll need to connect to the database we just created..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvDxGoeCmSKQ"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy.engine.url import URL\n",
        "from triage.util.db import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "db_url = URL(\n",
        "            'postgres',\n",
        "            host='localhost',\n",
        "            username='postgres',\n",
        "            database='donors_choose',\n",
        "            password='postgres',\n",
        "            port=5432,\n",
        "        )\n",
        "\n",
        "db_engine = create_engine(db_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfgHR1NfnMI4"
      },
      "source": [
        "The DonorsChoose dataset contains four main tables we'll need here:\n",
        "- **Projects** contains information about each project as well as some details about the teacher posting it and their school and district\n",
        "- **Essays** contains the detailed descriptions that the teacher post describing their project and needs\n",
        "- **Resources** contains detailed information about the specific number, type, and cost of resources being asked for in the project\n",
        "- **Donations** contains information about the donations received by each project on a transactional level, as well as some details about the donor\n",
        "\n",
        "Let's take a look at the projects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhElc5PMprk0"
      },
      "outputs": [],
      "source": [
        "pd.read_sql('SELECT COUNT(*) FROM data.projects', db_engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkrUfqnmqs0m"
      },
      "outputs": [],
      "source": [
        "pd.read_sql('SELECT * FROM data.projects LIMIT 5', db_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knvi1ZB8rANB"
      },
      "source": [
        "Note that the `projectid_str` column can be used to link out to the other tables. For instance, let's look at what we can find out about project `30c034618e67d00c641f9b5b7775c0f4`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNl81xoErL7g"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"SELECT * FROM data.essays WHERE projectid_str='30c034618e67d00c641f9b5b7775c0f4'\", db_engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdLzQlGXraVX"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"\"\"\n",
        "  SELECT project_resource_type,\n",
        "        COUNT(*) AS num_distinct_resources, \n",
        "        SUM(item_quantity) AS num_total_resources,\n",
        "        AVG(item_unit_price) AS avg_price,\n",
        "        SUM(item_unit_price * item_quantity) AS total_cost\n",
        "  FROM data.resources \n",
        "  WHERE projectid_str='30c034618e67d00c641f9b5b7775c0f4'\n",
        "  GROUP BY 1;\n",
        "  \"\"\", db_engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAhWr6FJ9656"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"\"\"\n",
        "  SELECT \n",
        "    COUNT(*) AS num_donations,\n",
        "    SUM(donation_to_project) AS total_donation,\n",
        "    SUM(CASE WHEN is_teacher_acct THEN 1 ELSE 0 END) AS num_teacher_donation\n",
        "  FROM data.donations \n",
        "  WHERE projectid_str='30c034618e67d00c641f9b5b7775c0f4'\n",
        "  ;\n",
        "  \"\"\", db_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIj8GvQF_1cV"
      },
      "source": [
        "## Formulating the project\n",
        "\n",
        "Now that we're familiar with the available data, let's turn to the prediction problem at hand. Because reviewing and offering suggestions to posted projects will be time and resource-intensive, we might assume that DonorsChoose can only help a fraction of all projects that get posted, let's suppose 10%. Then, we might formulate our problem along the lines of:\n",
        "\n",
        "**Each day, for all the projects posted on that day, can we identify the 10% of projects with the highest risk of not being fully funded within 4 months to prioritize for review by digital content experts.**\n",
        "\n",
        "With this formulation in mind, we can define a cohort and label for our analysis. `triage` will allow us to define these directly as a SQL query, so let's start there..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv-YbkWuOFnI"
      },
      "source": [
        "### Defining the Cohort and Label\n",
        "\n",
        "Because most models to inform important decisions will need to generalize into the future, `triage` focuses on respecting the temporal nature of the data (discussed in more detail below). The `cohort` is the set of relevant entities for model training/prediction at a given point in time, which `triage` referrs to as an `as_of_date`.\n",
        "\n",
        "ðŸš§ &nbsp;&nbsp;NOTE: In `triage`, an `as_of_date` is taken to be midnight at the **beginning** of that date.\n",
        "\n",
        "Here, the cohort is relatively straightforward: we simply want to identify all of the projects that were posted, right on the day of posting. Although we were looking at the identifier `projectid_str` above, `triage` looks for a column called `entity_id` to uniquely identify entities to its models. We've already added this column to this dataset, so we'll use that below.\n",
        "\n",
        "ðŸš§ &nbsp;&nbsp;NOTE: `triage` expects entities in the data to be identified by an **integer column** called `entity_id`.\n",
        "\n",
        "For modeling, we also need to consider the outcome we care about. Returning to our formulation, we described trying to identify projects which will not be fully funded within the four months they are active on the platform.\n",
        "\n",
        "As with the cohort, notice that labels are calculated relative to a given point in time (the `as_of_date` described above) and over a specific time horizon (here, 4 months from posting). In triage, this time horizon is referred to as a `label_timespan` and is also available as a parameter to your label definition.\n",
        "\n",
        "With those details in mind, let's look at an example of how we might define the cohort and label from our data for this project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odpo6nluPGLk"
      },
      "outputs": [],
      "source": [
        "example_as_of_date = '2012-08-07'\n",
        "example_label_timespan = '4month'\n",
        "\n",
        "pd.read_sql(\"\"\"\n",
        "    WITH cohort_query AS (\n",
        "      SELECT distinct(entity_id)\n",
        "      FROM data.projects\n",
        "      WHERE date_posted = '{as_of_date}'::date - interval '1day'\n",
        "    )\n",
        "    , cohort_donations AS (\n",
        "      SELECT \n",
        "        c.entity_id, \n",
        "        COALESCE(SUM(d.donation_to_project), 0) AS total_donation\n",
        "      FROM cohort_query c\n",
        "      LEFT JOIN data.donations d \n",
        "        ON c.entity_id = d.entity_id\n",
        "        AND d.donation_timestamp \n",
        "          BETWEEN '{as_of_date}'::date - interval '1day'\n",
        "          AND '{as_of_date}'::date + interval '{label_timespan}'\n",
        "      GROUP BY 1\n",
        "    )\n",
        "    SELECT c.entity_id,\n",
        "    CASE \n",
        "      WHEN COALESCE(d.total_donation, 0) >= p.total_asking_price THEN 0\n",
        "      ELSE 1\n",
        "    END AS outcome  \n",
        "    FROM cohort_query c\n",
        "    JOIN data.projects p USING(entity_id)\n",
        "    LEFT JOIN cohort_donations d using(entity_id)\n",
        "  ;\n",
        "  \"\"\".format(as_of_date=example_as_of_date, label_timespan=example_label_timespan), db_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLeE7N-NSWam"
      },
      "source": [
        "Here, we start by defining our cohort as described above, then find all the donations to those projects within the label timespan (e.g., the following 4 months after posting), and finally compare that to the total price of the project to create a binary classification label for whether or not the project was fully funded.\n",
        "\n",
        "Notice here that because we will intervene on projects at risk for **NOT** being fully funded, we define this as our class 1 label while those that do reach their funding goal are given class 0.\n",
        "\n",
        "In `triage` we'll be able to use `{as_of_date}` as a placeholder for time just as we're doing here. Also note that because the `as_of_date` is taken to be midnight, we're looking at the projects posted the previous day (hence subtracting the 1 day interval in the query).\n",
        "\n",
        "For `triage`, we use a yaml format for configuration (described further below) and we'll be able to provide this query directly:\n",
        "```\n",
        "label_config:\n",
        "  query: |\n",
        "    WITH cohort_query AS (\n",
        "      SELECT distinct(entity_id)\n",
        "      FROM data.projects\n",
        "      WHERE date_posted = '{as_of_date}'::date - interval '1day'\n",
        "    )\n",
        "    , cohort_donations AS (\n",
        "      SELECT \n",
        "        c.entity_id, \n",
        "        COALESCE(SUM(d.donation_to_project), 0) AS total_donation\n",
        "      FROM cohort_query c\n",
        "      LEFT JOIN data.donations d \n",
        "        ON c.entity_id = d.entity_id\n",
        "        AND d.donation_timestamp \n",
        "          BETWEEN '{as_of_date}'::date - interval '1day'\n",
        "          AND '{as_of_date}'::date + interval '{label_timespan}'\n",
        "      GROUP BY 1\n",
        "    )\n",
        "    SELECT c.entity_id,\n",
        "    CASE \n",
        "      WHEN COALESCE(d.total_donation, 0) >= p.total_asking_price THEN 0\n",
        "      ELSE 1\n",
        "    END AS outcome  \n",
        "    FROM cohort_query c\n",
        "    JOIN data.projects p USING(entity_id)\n",
        "    LEFT JOIN cohort_donations d using(entity_id)\n",
        "\n",
        "  name: 'fully_funded'\n",
        "```\n",
        "\n",
        "For more details these two pieces of the modeling pipeline, see the [cohort and label deep dive in the triage docs](https://dssg.github.io/triage/experiments/cohort-labels/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RTN8p6VWQ8z"
      },
      "source": [
        "### Dealing with Time\n",
        "\n",
        "As noted above, `triage` is designed for problems where the desire to generalize to future data and therefore is careful to respect the temporal nature of the problem. This is particularly salient in two places: defining the validation strategy for model evaluation and ensuring that features only make use of information available at the time of analysis/prediction.\n",
        "\n",
        "For validation, the idea is generally simple: models should be trained on historical data and validated on future data. As such, `triage` constructs validation splits that reflect this process by using a certain point in time as the cut-off between training and validation and then moving this cut-off back through the data to generate multiple splits. The implementation is a bit more complicated and relies on several parameters, the details of which we won't go deep into here, but you can find a much deeper discussion in the [longer \"dirty duck\" tutorial](https://dssg.github.io/triage/dirtyduck/triage_intro/) as well as in the [experiment config docs](https://dssg.github.io/triage/experiments/experiment-config/).\n",
        "\n",
        "![temporal figure](https://dssg.github.io/triage/experiments/temporal_config_graph.png)\n",
        "\n",
        "In short, these parameters are (illustrated across three training/validation splits in the figure above):\n",
        "- feature start/end times: what range of history is feature information available for?\n",
        "- label start/end times: what range of history is outcome (label) data available for?\n",
        "- model update frequency: what is the interval between refreshes of the model?\n",
        "- test durations: over what time period will the model be in use for making predictions?\n",
        "- max training history: how much historical data should be used for model training (that is, for rows/examples)?\n",
        "- training/test as_of_date frequencies: within a training or validation (test) set, how frequently should cohorts be sampled?\n",
        "- training/test label timespans: over what time horizon are labels (outcomes) collected?\n",
        "\n",
        "As with the cohorts and labels, these parameters are specified to `triage` via its yaml configuration file. Here's what this will look like for our setting:\n",
        "```\n",
        "temporal_config:\n",
        "\n",
        "    # first date our feature data is good\n",
        "    feature_start_time: '2000-01-01'\n",
        "    feature_end_time: '2013-06-01'\n",
        "\n",
        "    # first date our label data is good\n",
        "    # donorschoose: as far back as we have good donation data\n",
        "    label_start_time: '2011-09-02'\n",
        "    label_end_time: '2013-06-01'\n",
        "\n",
        "    model_update_frequency: '4month'\n",
        "\n",
        "    # length of time defining a test set\n",
        "    test_durations: ['3month']\n",
        "    # defines how far back a training set reaches\n",
        "    max_training_histories: ['1y']\n",
        "\n",
        "    # we sample every day, since new projects are posted\n",
        "    # every day\n",
        "    training_as_of_date_frequencies: ['1day']\n",
        "    test_as_of_date_frequencies: ['1day']\n",
        "    \n",
        "    # when posted project timeout\n",
        "    label_timespans: ['3month']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQtPjBJUaaVJ"
      },
      "source": [
        "### Model Evaluation Metrics\n",
        "\n",
        "The temporal configuration described above will create several training and validation splits that can be used to estimate the generalization performance of your models and select a model specification to use going forward. In order to do so, of course, you need to choose an appropriate metric (or metrics) by which to evaluate your models. `triage` can use any of the metrics specified by `sklearn` and in general you'll want to focus on those that best reflect the goals, constraints, and deployment scenario of your project. For instance, in our example project, DonorsChoose can help only 10% of the projects posted to the site, so a metric like precision in the top 10% would reflect how efficiently these limited resources are being allocated to projects that would not be fully funded without additional support.\n",
        "\n",
        "Although we might want to focus on `precision@10%` as our primary metric, often it can be helpful to look at both precision and recall at a range of thresholds (both percentiles and absolute numbers) both for the purposes of debugging and understanding how sensitive your results are to the available resources, describing a \"menu\" of policy choices.\n",
        "\n",
        "The `scoring` section of the yaml configuration file allows you specify separate evaluation metrics for both the training and validation set results, indicating both the type of metric (e.g., `precision`, `recall`, etc) and, where needed, the thresholds at which to calculate them. Here's what that looks like for our example project:\n",
        "\n",
        "```\n",
        "scoring:\n",
        "    testing_metric_groups:\n",
        "        -\n",
        "          metrics: [precision@, recall@]\n",
        "          thresholds:\n",
        "              percentiles: [1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
        "                  10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
        "                  20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
        "                  30, 31, 32, 33, 34, 35, 36, 37, 38, 39, \n",
        "                  40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
        "                  50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
        "                  60, 61, 62, 63, 64, 65, 66, 67, 68, 69,\n",
        "                  70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
        "                  80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
        "                  90, 91, 92, 93, 94, 95, 96, 97, 98, 99,\n",
        "                  100]\n",
        "              top_n: [25, 50, 100]\n",
        "\n",
        "    training_metric_groups:\n",
        "        -\n",
        "          metrics: [precision@, recall@]\n",
        "          thresholds:\n",
        "              percentiles: [1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
        "                  10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
        "                  20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
        "                  30, 31, 32, 33, 34, 35, 36, 37, 38, 39, \n",
        "                  40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
        "                  50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
        "                  60, 61, 62, 63, 64, 65, 66, 67, 68, 69,\n",
        "                  70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
        "                  80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
        "                  90, 91, 92, 93, 94, 95, 96, 97, 98, 99,\n",
        "                  100]\n",
        "              top_n: [25, 50, 100]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cY_UC4Taey9"
      },
      "source": [
        "### Defining Features\n",
        "\n",
        "Feature generation is typically the most important aspect of how well your machine learning models will work, so `triage` provides considerable flexibility for feature definition. However, this also means that this section of the configuration file can be particularly complicated and may require some experimentation to get familiar with. A few resources may be helpful for a deeper look at how features work in `triage`:\n",
        "- [Feature Definition in the Quickstart Guide](https://dssg.github.io/triage/triage_project_workflow/#define-some-additional-features)\n",
        "- [Feature Generation in the triage Documentation](https://dssg.github.io/triage/experiments/experiment-config/#feature-generation)\n",
        "- [Features in the Example Configuration File](https://github.com/dssg/triage/blob/master/example/config/experiment.yaml#L102)\n",
        "\n",
        "Features in `triage` are defined in blocks, grouping together features drawn from the same data source and allowing several related features to be constructed in a very compact format. Each of these blocks is a list item under the `feature_aggregations` section of your yaml configuration file and contains the following information:\n",
        "- A `prefix` that is used to identify the group of features.\n",
        "- A `from_obj` that specifies the underlying information used to construct the features in this group. This can be either a table or a query in itself (in the later case, be sure to give it an alias) and must contain both an `entity_id` column as well as a date column indicating when the information was known, identified to the feature config as the `knowledge_date_column`.\n",
        "- Information about how missing values should be imputed (see the documentation for details and available options here).\n",
        "- Definitions of the feature quantities/columns themselves, specified either as `aggregates` or `categoricals`, including the `metrics` for aggregations over time (e.g., `sum`, `max`, `avg`, etc).\n",
        "- Time ranges over which to calculate feature information, called `intervals` (e.g., last 6 months, last 5 years, etc.)\n",
        "- A level of aggregation for feature information (`groups`) -- this will almost always be just `entity_id`.\n",
        "\n",
        "ðŸš§ &nbsp;&nbsp;NOTE: All features in `triage` are temporal aggregates. Just as `triage` is designed to carefully account for time in temporal cross-validation, it also does so in feature construction focusing on what information was known at training or validation time. Even features you might generally consider \"static\" need to be associated with a knowledge date for these purposes as well as an aggregation metric. This is also true for categoricals, which are first one-hot encoded from each instance then aggregated over the given time interval with the specified metric. For instance, if a patient has had several hospital stays with different primary diagnosis codes at each stay, a categorical feature using a `sum` aggregation would yield a count of how many stays had a given diagnosis while a `max` aggregation would provide an indicator of whether a given diagnosis was ever present. \n",
        "\n",
        "For aggregations of numeric features, the resulting feature names will have the format: \n",
        "`{prefix}_entity_id_{interval}_{quantity}_{metric}`\n",
        "\n",
        "For categoricals, the feature names will include each categorical value after one-hot encoding:\n",
        "`{prefix}_entity_id_{interval}_{quantity}_{value}_{metric}`\n",
        "\n",
        "ðŸš§ &nbsp;&nbsp;WARNING: Because `triage`'s features are stored in a `postgres` database, this naming convention can sometimes run afoul of the database's 63 character limit for column names, leading to truncation. When this happens, you might encounter errors indicating a given feature column appears to be missing. This can be common with categoricals with particularly long values, so recoding can be useful in those cases (as can choosing shorter prefix names).\n",
        "\n",
        "For illustrative purposes here, we'll start with a single feature group including one categorical and continuous aggregate feature: the primary resource type for the project and the amount being asked for. Because these are both specified once at project posting time, we simply aggegate them over all time (that is, using `all` for our `interval`). Here's how we specify this in our feature configuration:\n",
        "\n",
        "```\n",
        "feature_aggregations:\n",
        "  -\n",
        "    prefix: 'project_features'\n",
        "    from_obj: 'data.projects'\n",
        "    knowledge_date_column: 'date_posted'\n",
        "\n",
        "    aggregates_imputation:\n",
        "      all:\n",
        "        type: 'zero'\n",
        "\n",
        "    categoricals_imputation:\n",
        "      all:\n",
        "        type: 'null_category'          \n",
        "\n",
        "    categoricals:\n",
        "      -\n",
        "        column: 'resource_type'\n",
        "        metrics:\n",
        "          - 'max' \n",
        "        choice_query: 'select distinct resource_type from data.projects'\n",
        "    \n",
        "    aggregates:\n",
        "      -\n",
        "        quantity: 'total_asking_price'\n",
        "        metrics:\n",
        "          - 'sum'\n",
        "      \n",
        "    # Since our time-aggregate features are precomputed, feature interval is \n",
        "    # irrelvant. We keep 'all' as a default.\n",
        "    intervals: ['all'] \n",
        "    groups: ['entity_id']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YNaIVySaj2Z"
      },
      "source": [
        "### Model and Hyperparameter Grid\n",
        "\n",
        "You specify the types of models you want to explore, along with their hyperparameters, in the `grid_config` section of your yaml configuration file. Because there's generally no way to know a priori what model specification will work best for a given problem, `triage` makes it easy to run and explore an extensive grid by providing lists of values for each hyperparameter and training models for the full cross-product of these values.\n",
        "\n",
        "Currently, `triage` can work with any classifiction method with an `sklearn`-style interface. In addition to machine learning algorithms found in standard packages, `triage` includes a couple of built-in methods you might find useful:\n",
        "- `ScaledLogisticRegression` wraps the `sklearn`  logistic regression with a min-max scaler to ensure that the input features are on the same scale for regularization. It accepts the same hyperparameters as the underlying `sklearn` method.\n",
        "- `BaselineRankMultiFeature` is a simple baseline method that ranks examples by one or more features, replicating a comonsense approach that could be taken without making use of machine learning. This method takes a single hyperparameter, `rules`, specified as a list of dictionaries with the keys `feature` and `low_value_high_score` to specify the directin of the ranking. Examples are sorted first by the first feature in this list, then the next, and so on.\n",
        "- `SimpleThresholder` is another basic baseline method, allowing you to specify a heuristic, rule-based approach to classifying examples. It uses two hyperparameters: a list of `rules` (e.g., `feature_1 > 3`) and a `logical_operator` (e.g., `and` or `or`) to specify how the rules are combined.\n",
        "\n",
        "To specify a model type in your grid config, you use the model's class path as a key and each hyperparameter as a key another level down. For example:\n",
        "```\n",
        "'module.submodule.ClassName':\n",
        "    param_1: [1,3,5,10,20]\n",
        "    param_2: [100, 500, 1000]\n",
        "```\n",
        "\n",
        "For our purposes here, we'll start with a very small grid that can run quickly in a colab notebook. Here's how that will look:\n",
        "```\n",
        "grid_config:\n",
        "    'sklearn.ensemble.RandomForestClassifier':\n",
        "        n_estimators: [150]\n",
        "        max_depth: [50]\n",
        "        min_samples_split: [25]\n",
        "    \n",
        "    'sklearn.tree.DecisionTreeClassifier':\n",
        "        max_depth: [3]\n",
        "        max_features: [null]\n",
        "        min_samples_split: [25]\n",
        "      \n",
        "    'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression':\n",
        "        C: [0.1]\n",
        "        penalty: ['l1']\n",
        "    \n",
        "    'triage.component.catwalk.baselines.rankers.BaselineRankMultiFeature':\n",
        "        rules:\n",
        "            - [{feature: 'project_features_entity_id_all_total_asking_price_sum', low_value_high_score: False}]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lOxjerFm79c"
      },
      "source": [
        "### Auditing Models for Bias\n",
        "\n",
        "The final section of the configuration file specifies how you want to evaluate your models for bias and fairness using the [aequitas](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/) toolkit. In order to do so, you need to tell `triage` what attributes are relavent for bias audits, a table or SQL query to specify these attributes (the `from_obj`), and the reference group to calculate disparities relative to (the value for this group will serve as the denominator for disparity calculations. Like the evaluation metrics described above, you'll also need to specify the set of thresholds against which you want to calculate fairness metrics. Note that `aequitas` will calculate the full range of confusion matrix-derived disparity metrics for all of your models, allowing you to explore how your models perform under different conceptualizations of fairness.\n",
        "\n",
        "To illustrate the use of a bias audit in our example project, we'll look at the `teacher_prefix` attribute as a proxy for the sex of the teacher, using `Mr.` as a reference group. Note that the `from_obj_table` will be joined using an `entity_id` and `as_of_date`, so you must specify a `knowledge_date_column` in the config, as some attributes (or your knowledge of them) might change over time. `aequitas` will use the most recent value of the attribute it finds for a given entity prior to the specified `as_of_date`. Here's how we turn that into a section in our configuration yaml:\n",
        "\n",
        "```\n",
        "bias_audit_config:\n",
        "    from_obj_table: 'data.projects'\n",
        "    attribute_columns:\n",
        "        - 'teacher_prefix'\n",
        "    knowledge_date_column: 'date_posted'\n",
        "    entity_id_column: 'entity_id'\n",
        "    ref_groups_method: 'predefined'\n",
        "    ref_groups:\n",
        "        'teacher_prefix': 'Mr.'\n",
        "    thresholds:\n",
        "        percentiles: [5, 10, 15, 20, 25, 50, 100]\n",
        "        top_n: [25, 50, 100]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZSSdXx-NcTE"
      },
      "source": [
        "## Running Triage\n",
        "\n",
        "Now that we've walked through the various aspects of configuring triage, we're ready to run our model grid! In order to do so, we need three pieces:\n",
        "- Our configuration file, pulling together the elements described above into a single yaml file we'll call `experiment_config.yaml` (in `triage`, an \"experiment\" is a run with a set of parameters and model types).\n",
        "- Credentials for connecting to your database, stored in a configuration file called `database.yaml` (alternatively, you can specify them through environment variables)\n",
        "- Code to run your `triage` experiment. This can be done via either a command line tool or python interface, the latter of which provides more flexibility so we'll focus on that approach here with a short python script called `run.py`.\n",
        "\n",
        "The following three sections provides the contents of each of these three files for our DonorsChoose project. In a real project, of course, these would be stored as separate files on your system, but here we include them inline. The `run.py` sets up logging, connects to the database, loads your configuration file, and creates and runs a `MultiCoreExperiment` object from `triage`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX-inX6o7QBE"
      },
      "source": [
        "### experiment_config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdDjQCovS1GG"
      },
      "outputs": [],
      "source": [
        "config_yaml = \"\"\"\n",
        "config_version: 'v8'\n",
        "\n",
        "model_comment: 'triage demo'\n",
        "\n",
        "random_seed: 1995\n",
        "\n",
        "temporal_config:\n",
        "\n",
        "    # first date our feature data is good\n",
        "    feature_start_time: '2000-01-01'\n",
        "    feature_end_time: '2013-06-01'\n",
        "\n",
        "    # first date our label data is good\n",
        "    # donorschoose: as far back as we have good donation data\n",
        "    label_start_time: '2011-09-02'\n",
        "    label_end_time: '2013-06-01'\n",
        "\n",
        "    model_update_frequency: '4month'\n",
        "\n",
        "    # length of time defining a test set\n",
        "    test_durations: ['3month']\n",
        "    # defines how far back a training set reaches\n",
        "    max_training_histories: ['1y']\n",
        "\n",
        "    # we sample every day, since new projects are posted\n",
        "    # every day\n",
        "    training_as_of_date_frequencies: ['1day']\n",
        "    test_as_of_date_frequencies: ['1day']\n",
        "    \n",
        "    # when posted project timeout\n",
        "    label_timespans: ['3month']\n",
        "\n",
        "\n",
        "label_config:\n",
        "  query: |\n",
        "    WITH cohort_query AS (\n",
        "      SELECT distinct(entity_id)\n",
        "      FROM data.projects\n",
        "      WHERE date_posted = '{as_of_date}'::date - interval '1day'\n",
        "    )\n",
        "    , cohort_donations AS (\n",
        "      SELECT \n",
        "        c.entity_id, \n",
        "        COALESCE(SUM(d.donation_to_project), 0) AS total_donation\n",
        "      FROM cohort_query c\n",
        "      LEFT JOIN data.donations d \n",
        "        ON c.entity_id = d.entity_id\n",
        "        AND d.donation_timestamp \n",
        "          BETWEEN '{as_of_date}'::date - interval '1day'\n",
        "          AND '{as_of_date}'::date + interval '{label_timespan}'\n",
        "      GROUP BY 1\n",
        "    )\n",
        "    SELECT c.entity_id,\n",
        "    CASE \n",
        "      WHEN COALESCE(d.total_donation, 0) >= p.total_asking_price THEN 0\n",
        "      ELSE 1\n",
        "    END AS outcome  \n",
        "    FROM cohort_query c\n",
        "    JOIN data.projects p USING(entity_id)\n",
        "    LEFT JOIN cohort_donations d using(entity_id)\n",
        "\n",
        "  name: 'fully_funded'\n",
        "\n",
        "\n",
        "feature_aggregations:\n",
        "  -\n",
        "    prefix: 'project_features'\n",
        "    from_obj: 'data.projects'\n",
        "    knowledge_date_column: 'date_posted'\n",
        "\n",
        "    aggregates_imputation:\n",
        "      all:\n",
        "        type: 'zero'\n",
        "\n",
        "    categoricals_imputation:\n",
        "      all:\n",
        "        type: 'null_category'          \n",
        "\n",
        "    categoricals:\n",
        "      -\n",
        "        column: 'resource_type'\n",
        "        metrics:\n",
        "          - 'max' \n",
        "        choice_query: 'select distinct resource_type from data.projects'\n",
        "    \n",
        "    aggregates:\n",
        "      -\n",
        "        quantity: 'total_asking_price'\n",
        "        metrics:\n",
        "          - 'sum'\n",
        "      \n",
        "    # Since our time-aggregate features are precomputed, feature interval is \n",
        "    # irrelvant. We keep 'all' as a default.\n",
        "    intervals: ['all']\n",
        "\n",
        "grid_config:\n",
        "    'sklearn.ensemble.RandomForestClassifier':\n",
        "        n_estimators: [150]\n",
        "        max_depth: [50]\n",
        "        min_samples_split: [25]\n",
        "    \n",
        "    'sklearn.tree.DecisionTreeClassifier':\n",
        "        max_depth: [3]\n",
        "        max_features: [null]\n",
        "        min_samples_split: [25]\n",
        "      \n",
        "    'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression':\n",
        "        C: [0.1]\n",
        "        penalty: ['l1']\n",
        "    \n",
        "    'triage.component.catwalk.baselines.rankers.BaselineRankMultiFeature':\n",
        "        rules:\n",
        "            - [{feature: 'project_features_entity_id_all_total_asking_price_sum', low_value_high_score: False}]\n",
        "\n",
        "\n",
        "scoring:\n",
        "    testing_metric_groups:\n",
        "        -\n",
        "          metrics: [precision@, recall@]\n",
        "          thresholds:\n",
        "              percentiles: [1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
        "                  10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
        "                  20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
        "                  30, 31, 32, 33, 34, 35, 36, 37, 38, 39, \n",
        "                  40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
        "                  50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
        "                  60, 61, 62, 63, 64, 65, 66, 67, 68, 69,\n",
        "                  70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
        "                  80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
        "                  90, 91, 92, 93, 94, 95, 96, 97, 98, 99,\n",
        "                  100]\n",
        "              top_n: [25, 50, 100]\n",
        "\n",
        "    training_metric_groups:\n",
        "        -\n",
        "          metrics: [precision@, recall@]\n",
        "          thresholds:\n",
        "              percentiles: [1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
        "                  10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
        "                  20, 21, 22, 23, 24, 25, 26, 27, 28, 29, \n",
        "                  30, 31, 32, 33, 34, 35, 36, 37, 38, 39, \n",
        "                  40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
        "                  50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
        "                  60, 61, 62, 63, 64, 65, 66, 67, 68, 69,\n",
        "                  70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
        "                  80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
        "                  90, 91, 92, 93, 94, 95, 96, 97, 98, 99,\n",
        "                  100]\n",
        "              top_n: [25, 50, 100]  \n",
        "          \n",
        "bias_audit_config:\n",
        "    from_obj_table: 'data.projects'\n",
        "    attribute_columns:\n",
        "        - 'teacher_prefix'\n",
        "    knowledge_date_column: 'date_posted'\n",
        "    entity_id_column: 'entity_id'\n",
        "    ref_groups_method: 'predefined'\n",
        "    ref_groups:\n",
        "        'teacher_prefix': 'Mr.'\n",
        "    thresholds:\n",
        "        percentiles: [5, 10, 15, 20, 25, 50, 100]\n",
        "        top_n: [25, 50, 100]\n",
        "        \n",
        "individual_importance:\n",
        "    methods: [] # empty list means don't calculate individual importances\n",
        "    n_ranks: 1 \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcqeRvUT7V1D"
      },
      "source": [
        "### database.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i88ZFQupfp6"
      },
      "outputs": [],
      "source": [
        "database_yaml = \"\"\"\n",
        "host: localhost\n",
        "user: postgres\n",
        "db: donors_choose\n",
        "pass: postgres\n",
        "port: 5432\n",
        "role: postgres\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHR9wnAw7d__"
      },
      "source": [
        "### run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYzBKFG3qDhQ"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import shutil\n",
        "import os\n",
        "import logging\n",
        "\n",
        "from sqlalchemy.engine.url import URL\n",
        "from sqlalchemy.event import listens_for\n",
        "from sqlalchemy.pool import Pool\n",
        "\n",
        "from triage.util.db import create_engine\n",
        "from triage.experiments import MultiCoreExperiment\n",
        "\n",
        "\n",
        "def run_triage():\n",
        "\n",
        "  # andrew_id = os.getenv('USER')\n",
        "  # user_path = os.path.join('/data/users/', andrew_id)\n",
        "  user_path = '/content'\n",
        "\n",
        "  # add logging to a file (it will also go to stdout via triage logging config)\n",
        "  log_filename = os.path.join(user_path, 'triage.log')\n",
        "  logger = logging.getLogger('')\n",
        "  hdlr = logging.FileHandler(log_filename)\n",
        "  hdlr.setLevel(15)   # verbose level\n",
        "  hdlr.setFormatter(logging.Formatter('%(name)-30s  %(asctime)s %(levelname)10s %(process)6d  %(filename)-24s  %(lineno)4d: %(message)s', '%d/%m/%Y %I:%M:%S %p'))\n",
        "  logger.addHandler(hdlr)\n",
        "\n",
        "  # creating database engine\n",
        "  # dbfile = os.path.join(user_path, 'database.yaml')\n",
        "\n",
        "  # with open(dbfile, 'r') as dbf:\n",
        "  #     dbconfig = yaml.safe_load(dbf)\n",
        "\n",
        "  dbconfig = yaml.safe_load(database_yaml)\n",
        "  print(dbconfig['role'])\n",
        "\n",
        "  # assume group role to ensure shared permissions\n",
        "  @listens_for(Pool, \"connect\")\n",
        "  def assume_role(dbapi_con, connection_record):\n",
        "      logging.debug(f\"setting role {dbconfig['role']};\")\n",
        "      dbapi_con.cursor().execute(f\"set role {dbconfig['role']};\")\n",
        "      # logging.debug(f\"setting role postres;\")\n",
        "      # dbapi_con.cursor().execute(f\"set role postgres;\")\n",
        "\n",
        "  db_url = URL(\n",
        "              'postgres',\n",
        "              host=dbconfig['host'],\n",
        "              username=dbconfig['user'],\n",
        "              database=dbconfig['db'],\n",
        "              password=dbconfig['pass'],\n",
        "              port=dbconfig['port'],\n",
        "          )\n",
        "\n",
        "  db_engine = create_engine(db_url)\n",
        "\n",
        "  triage_output_path = os.path.join(user_path, 'triage_output')\n",
        "  os.makedirs(triage_output_path, exist_ok=True)\n",
        "\n",
        "  # loading config file\n",
        "  # with open('%s_triage_config.yaml' % andrew_id, 'r') as fin:\n",
        "  #     config = yaml.safe_load(fin)\n",
        "\n",
        "  config = yaml.safe_load(config_yaml)\n",
        "\n",
        "  # creating experiment object\n",
        "  experiment = MultiCoreExperiment(\n",
        "      config = config,\n",
        "      db_engine = db_engine,\n",
        "      project_path = triage_output_path,\n",
        "      n_processes=2,\n",
        "      n_bigtrain_processes=1,\n",
        "      n_db_processes=2,\n",
        "      replace=True,\n",
        "      save_predictions=True\n",
        "      )\n",
        "\n",
        "  # experiment.validate()\n",
        "  experiment.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyqBcKHk7lTC"
      },
      "source": [
        "### Let's run triage!\n",
        "\n",
        "With these three files in place, we can simply run our model grid by calling `run_triage()`. Doing so will train and validate the four model specifications described above across three temporal validation splits. The run will output a log of its progress and store results into the postgres database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUAcMwv2qzLe"
      },
      "outputs": [],
      "source": [
        "run_triage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checking the results \n",
        "\n",
        "Running `triage` will generate three types of outputs: \n",
        "1. Summary report of the experiment\n",
        "2. Objects stored to disk\n",
        "3. Results stored in the database.\n",
        "\n",
        "#### 1. Summary report of the experiment \n",
        "\n",
        "After running an experiment in `triage` it will generate a summary report with important sanity checks to review before moving on to [Model Selection](#model-selection) and [Bias Audit](#bias-audit). Generally, this report would be generated by following this [guide](https://github.com/dssg/triage/tree/master/src/triage/component/postmodeling) using a notebook template. Since we are executing `triage` in Colab, we will call the methods of the summary report directly. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpfPG-nyq1Nk"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import pandas as pd\n",
        "\n",
        "from sqlalchemy.engine.url import URL\n",
        "from triage.util.db import create_engine\n",
        "from triage.component.postmodeling.experiment_summarizer import ExperimentReport\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "dbconfig = yaml.safe_load(database_yaml)\n",
        "db_url = URL(\n",
        "            'postgres',\n",
        "            host=dbconfig['host'],\n",
        "            username=dbconfig['user'],\n",
        "            database=dbconfig['db'],\n",
        "            password=dbconfig['pass'],\n",
        "            port=dbconfig['port'],\n",
        "        )\n",
        "\n",
        "db_engine = create_engine(db_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to get the hash code generated for this the experiment run. We can look for the hash on the database in table `triage_metadata-triage_runs` sorted by the latest date times (more details on the section [Results stored in the database](#3-results-stored-in-the-database))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = \"\"\" \n",
        "    select run_hash, start_time \n",
        "    from triage_metadata.triage_runs \n",
        "    order by start_time desc\n",
        "    limit 1\n",
        "\"\"\"\n",
        "\n",
        "pd.read_sql(q, db_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For generating the Experiment Summary Report, we need to define the performance metric, threshold, bias metric, and the priority groups we would like to look for. In our Donors Choose example, since we are interested in predicting the **10% of projects with the highest risk of not being fully funded within 4 months** we will do a first check on the performance of the models for the `precision` at the `10%`. Additionally, we want to analyze recall disparity accross different teacher prefixes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Triage created hash(es) of the experiment(s) you are interested in. \n",
        "# It has to be a list (even if single element)\n",
        "experiment_hashes = ['98112c011d842c43e841c415116ef179']\n",
        "\n",
        "# Model Performance metric and threshold\n",
        "# These default to 'recall@' and '1_pct'\n",
        "performance_metric = 'precision@'\n",
        "threshold = '10_pct'\n",
        "\n",
        "# Bias metric defaults to tpr_disparity and bias metric values for all groups generated (if bias audit specified in the experiment config)\n",
        "bias_metric = 'tpr_disparity'\n",
        "bias_priority_groups = {'teacher_prefix': ['Dr.', 'Mr.', 'Mrs.', 'Ms.']}\n",
        "\n",
        "# Create the Experient Rerport \n",
        "rep = ExperimentReport(\n",
        "    engine=db_engine,\n",
        "    experiment_hashes=experiment_hashes,\n",
        "    performance_priority_metric=performance_metric,\n",
        "    threshold=threshold,\n",
        "    bias_priority_metric=bias_metric,\n",
        "    bias_priority_groups=bias_priority_groups\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Summary report of the experiment run\n",
        "\n",
        "The summary gives a quick an short information of what happend in the experiment. \n",
        "* The number of temporal splits generated based on temporal configuration \n",
        "* The number of unique date times on those temporal splits\n",
        "* The average size of the cohorts and their baserates\n",
        "* The number of features generated and used in your models\n",
        "* The number of feature groups \n",
        "* The number of different type of models generated, e.g., Random Forest, Decision Tree, etc.\n",
        "* The number of models generated based on your grid configuration \n",
        "* The best average performance metric defined obtained and which model type generated it \n",
        "* A first glance of the disparity metric defined over the groups you have defined\n",
        "\n",
        "If the information looks correct based on your configuration of the experiment it means that the general sanity checks were fulfilled. If not, you need to come back to your experiment setup and look for inconsistences, e.g., the start and end dates for features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep.generate_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Visualize temporal splits  \n",
        "\n",
        "An interactive way to see the different temporal splits generated by `triage` based on the information of the experiment configuration. In our DonorsChoose example, we defined the labels started from **September 2, 2011** to **June 1, 2013**; and a label window of **3 months**. Based on that information, the first temporal split generated by `triage` for the validation matrix ends on the last available day of May 2013 -**May 20, 2013**- and begins three months earlier on the last available day of February -**February 28, 2013**-. The train matrix then has labels that end on **February 27, 2013** and begin on **December 1, 2012**. For a deeper discussion on this topic, you can refer to [longer \"dirty duck\" tutorial](https://dssg.github.io/triage/dirtyduck/triage_intro/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep.timesplits()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Cohorts generated\n",
        "\n",
        "A plot with the cohort sizes (in blue) at different `as_of_date` along with the baserate for each (in red). The dotted line indicates the mean. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cohort_summary = rep.cohorts(generate_plots=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A summary of cohort statistics is also generated. \n",
        "\n",
        "In our DonorsChoose example, the average baserate is near 40%, the maximum number of projects in a cohort is of 44 whilest the minimum is just 1 project, based on the definitions or our cohort and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cohort_summary[['cohort_size', 'baserate']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Cohort subsets**\n",
        "\n",
        "In case subsets were defined on the experiment configuration a plot for each subset will be generated. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep.subsets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Predictors\n",
        "\n",
        "This function will show the different features generated for the experiment with information related to the different metrics, time horizons and imputations made to each. \n",
        "\n",
        "In our DonorsChoose example, we have 2 group features: the total asking price, and the resource type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = rep.features()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Missing of features**\n",
        "\n",
        "In case the features generated have missing values, this table will indicated the average, max, and min percentages of missingness of each feature. If a feature has 100% percentage it means that all of its values are missing.\n",
        "In our DonorsChoose example, we don't have features with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep.feature_missingness()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Model groups built\n",
        "\n",
        "Summary of the models built based on the model type, hyperparameters and temporal configurations defined for the experiment. In our DonorsChoose example, we have 4 different model groups: Random Forest Classifiers, Decision Tree Classifiers, Baseline Rankers, and Scaled Logistic Regressions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep.model_groups()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**All models built**\n",
        "\n",
        "Each individual model built on the experiment (one per `as_of_date`, model type, hyperparameters, and temporal configuration). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep.models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Model Performance Overall Cohort\n",
        "\n",
        "Performance plot with the evaluation metric defined for each model group in all `as_of_date`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluations = rep.model_performance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Model Performance on Cohort Subsets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subset_evaluations = rep.model_performance_subsets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Model Performance vs Bias\n",
        "\n",
        "Plot with the peformance of different model groups and the bias metric selected in all groups defined and plot of group sizes vs the cohort size and the base rate for each `as_of_date`. \n",
        "\n",
        "In our DonorsChoose example, we can tell that the teacher prefix `Mr.` has been used a the reference group, and that the Random Forest Classfier has no disparity across the different groups: Mr., Mrs., and Ms. but has less performance that other models. A deeper analysis and discussion in section [Bias Audit](#bias-audit) on this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "equity_metrics = rep.efficiency_and_equity()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Initial model selection and further analysis on best models \n",
        "\n",
        "By default, the report will pick the best performing model from each model type based on average performance to generate additional outputs about the developed models. We would not assume the existence of predictions at this stage. Therefore, we will not do analysis such as list comparisons, crosstabs, score distribution type stuff. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep.get_best_hp_config_for_each_model_type()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPXmtJ667osT"
      },
      "source": [
        "#### 2. Objects stored in disk\n",
        "\n",
        "Two types of objects will be stored to disk in the `project_path` specified in creating the experiment object:\n",
        "- The matrices used for model training and validation, stored as CSV files and associated metadata in yaml format.\n",
        "- The trained model objects themselves, stored as `joblib` pickles, which can be loaded and applied to new data.\n",
        "\n",
        "#### 3. Results stored in the database\n",
        "\n",
        "In the database, `triage` will store results and metadata in several tables. Below is a very brief tour of the most important of these tables.\n",
        "\n",
        "In the **triage_metadata** schema, you'll find information about your run and the models that were created:\n",
        "- `triage_metadata.triage_runs`: metadata about every time `triage` is run, identified by a `run_id`\n",
        "- `triage_metadata.experiments`: configuration information for an experiment, identified by an `experiment_hash`. Note that a config file can be run multiple times, so a specific experiment might be associated with multiple `triage_runs` records. The `experiment_hash` can be linked to the `run_hash` in the `triage_runs` table where `run_type='experiment'`\n",
        "- `triage_metadata.model_groups`: in `triage` a `model_group` represents a full specification of a model type, set of hyperparameters, set of features, and training set parameters\n",
        "- `triage_metadata.models`: a `model` represents the application of a `model_group` to a given training set, yielding a set of trained parameters (such as the coefficients of a logistic regression, the splits of a decision tree, etc). The models are identified by both a `model_id` and `model_hash` and can be linked to their `model_group` via the `model_group_id`\n",
        "- `triage_metadata.experiment_models`: the association between models and experiments (linking an `experiment_hash` to a `model_hash`)\n",
        "\n",
        "In the **test_results** schema, you'll find information about the validation performance of the models:\n",
        "- `test_results.evaluations`: performance of each model on the metrics specified in the `scoring` section of your configuration file\n",
        "- `test_results.predictions`: individual entity-level predicted scores from each model\n",
        "- `test_results.prediction_metadata`: metadata associated with the predictions\n",
        "- `test_results.aequitas`: performance of each model on the fairness metrics using the parameters specified in your `bias_audit_config`\n",
        "\n",
        "In the **train_results** schema, you'll find model performance on the training set, as well as feature importances:\n",
        "- `train_results.evaluations`: similar to `test_results.evaluations` but for the training set (often may be overfit, but can be useful for debugging)\n",
        "- `train_results.predictions`: similar to `test_results.predictions` but for the training set\n",
        "- `train_results.prediction_metadata`: metadata associated with the predictions\n",
        "- `train_results.feature_importances`: overall feature importances from model training, usining the built-in method for the classifier (if one exists)\n",
        "\n",
        "Finally, a few intermediate tables can be particularly useful for debugging:\n",
        "- Tables containing your `cohort` and `label` will be generated in the `public` schema and identified by an associated hash that can be found in your logs.\n",
        "- The `features` schema contains two types of useful tables: tables containing calculated features for each feature group and \"matrix\" tables that provide the mapping from each training/validation matrix to `(entity_id, as_of_date)` pairs. Note, however, that these tables may be overwritten if a new run is performed with different feature logic, cohort, or underlying data and should not be assumed to be persistant across runs.\n",
        "\n",
        "Let's take a quick look at some of these outputs to confirm that our models ran as expected. First, we'll need a database connection..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajIxNg-S57e6"
      },
      "source": [
        "`triage_metadata.model_groups` should contain four records (for each model type/hyperparameter combination specified in our grid), while `triage_metadata.models` should have twelve (each model group trained on the three validation splits):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xjhe5S86V2H"
      },
      "outputs": [],
      "source": [
        "pd.read_sql('SELECT * FROM triage_metadata.model_groups;', db_engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZnlWPbMrFN-"
      },
      "outputs": [],
      "source": [
        "pd.read_sql('SELECT * FROM triage_metadata.models;', db_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyC6b5e07wBl"
      },
      "source": [
        "You can find the predictions from each model for each project in `test_results.predictions`. Let's take a quick look..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H_tiuoUrYzS"
      },
      "outputs": [],
      "source": [
        "pd.read_sql('SELECT * FROM test_results.predictions LIMIT 5;', db_engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6W6q6C98kPu"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"\"\"\n",
        "  SELECT \n",
        "    model_id, \n",
        "    COUNT(*) AS num_preds, \n",
        "    COUNT(DISTINCT entity_id) AS distinct_entities, \n",
        "    AVG(label_value) AS non_funded_rate\n",
        "  FROM test_results.predictions \n",
        "  GROUP BY 1\n",
        "  ORDER BY 1;\n",
        "  \"\"\", db_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6aGkcdb9Fmr"
      },
      "source": [
        "Note that the `label_value` column here is the actual label (e.g., if the project actually failed to be fully funded in four months).\n",
        "\n",
        "If we want to see how the model performed on your evaluation metrics, you can find these results calculated in `test_results.evaluations`. Let's look at how our models did in term of precision at the top 10% since that reflects our deployment setting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PqCVkEh-P_Q"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"\"\"\n",
        "  SELECT *\n",
        "  FROM test_results.evaluations \n",
        "  WHERE metric='precision@' AND parameter='10_pct'\n",
        "  ORDER BY model_id;\n",
        "  \"\"\", db_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_vwKbQo727Q"
      },
      "source": [
        "Finally, if you need to work with the training/validation matrices generated by triage or the model objects themselves, you can find them in your project path (here, `triage_output`). Let's take a quick look..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyx1QXJ3sAtQ"
      },
      "outputs": [],
      "source": [
        "!ls triage_output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnM6vADdvx-N"
      },
      "outputs": [],
      "source": [
        "!ls -la triage_output/matrices/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlJ017aVMhdS"
      },
      "outputs": [],
      "source": [
        "# clean up the database connection\n",
        "db_engine.dispose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFingR1B32rm"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "`triage` includes a component called `audition` that can help you visualize your model results over time and narrow down your best-performing models. Here we'll provide a quick introduction, but you can find more depth in the [audition tutorial](https://github.com/dssg/triage/blob/master/src/triage/component/audition/Audition_Tutorial.ipynb) as well as the [audition documentation](https://dssg.github.io/triage/audition/audition_intro/) and [model selection concepts overview](https://dssg.github.io/triage/audition/model_selection/). In general, `audition` is best run using a notebook to iteratively explore and narrow down your models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csg-HDeiPDma"
      },
      "source": [
        "### Audition Parameters\n",
        "\n",
        "To run `audition`, you'll need to specify a few parameters:\n",
        "\n",
        "`metric` and `parameter` together specify the evaluation metric of interest for your project. Note that these need to be calculated as part of the scoring section in your `triage` config and should match the values in the columns of the same name in `test_results.evaluations`\n",
        "\n",
        "The `run_hash` is an identifier for the run with your complete model grid that you want to evaluate -- the easiest way to find this is from the `triage_metadata.triage_runs` table. This will likely be the `run_hash` associated with the most recent record in that table, but you should be able to figure out which run you want to use from there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW1vSrhQP4ww"
      },
      "outputs": [],
      "source": [
        "from triage.component.audition import Auditioner\n",
        "from triage.component.audition.pre_audition import PreAudition\n",
        "from triage.component.audition.rules_maker import SimpleRuleMaker, RandomGroupRuleMaker, create_selection_grid\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "import yaml\n",
        "from sqlalchemy.engine.url import URL\n",
        "from triage.util.db import create_engine\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision', 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwg62Co837C4"
      },
      "outputs": [],
      "source": [
        "metric = 'precision@'\n",
        "parameter = '10_pct'\n",
        "run_hash = '98112c011d842c43e841c415116ef179'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyeFeaOEPLzk"
      },
      "outputs": [],
      "source": [
        "dbconfig = yaml.safe_load(database_yaml)\n",
        "db_url = URL(\n",
        "            'postgres',\n",
        "            host=dbconfig['host'],\n",
        "            username=dbconfig['user'],\n",
        "            database=dbconfig['db'],\n",
        "            password=dbconfig['pass'],\n",
        "            port=dbconfig['port'],\n",
        "        )\n",
        "\n",
        "conn = create_engine(db_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq9qO4-_QPJl"
      },
      "outputs": [],
      "source": [
        "# table where audition results will be stored\n",
        "best_dist_table = 'audition_best_dist'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVRftIpfQZxs"
      },
      "source": [
        "### Pre-Audition: Models and Temporal Splits\n",
        "\n",
        "Because you may have run several experiments as you iterate, explore, and debug, `audition` needs to know which set of model groups and temporal validation splits to focus on for model selection. While you can specify these directly, `triage` also provides some `pre-audition` tools to help define these.\n",
        "\n",
        "For example, `get_model_groups_from_experiment()` and `get_train_end_times()` (note that this will return the `train_end_times` associated with the set of model groups returned by one of the `get_model_groups` methods, so those should be run first). Note that the `baseline_model_types` parameter in the constructor is optional and can be used to identify model groups as baselines rather than candidates for model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NmsPZT7Rx_J"
      },
      "outputs": [],
      "source": [
        "pre_aud = PreAudition(\n",
        "    conn, \n",
        "    baseline_model_types=[\n",
        "        'sklearn.dummy.DummyClassifier',\n",
        "        'triage.component.catwalk.baselines.rankers.BaselineRankMultiFeature',\n",
        "        'triage.component.catwalk.baselines.thresholders.SimpleThresholder'\n",
        "    ]\n",
        ")\n",
        "\n",
        "# select model groups by experiment hash id\n",
        "model_groups = pre_aud.get_model_groups_from_experiment(run_hash)\n",
        "\n",
        "# Note that this will find train_end_times associated with the model groups defined above\n",
        "end_times = pre_aud.get_train_end_times(after='1900-01-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLpWTrG_SM88"
      },
      "source": [
        "`get_model_groups_from_experiment()` returns a dictionary with keys `model_groups` and `baseline_model_groups`.\n",
        "\n",
        "How many of each did we get?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFqr_IEBSbui"
      },
      "outputs": [],
      "source": [
        "# Number of non-baseline model groups:\n",
        "print(len(model_groups['model_groups']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLuZIZr9SfiY"
      },
      "outputs": [],
      "source": [
        "# Number of baseline model groups:\n",
        "print(len(model_groups['baseline_model_groups']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOYgNVIGSkl2"
      },
      "source": [
        "`get_train_end_times()` returns a list of `train_end_times`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSVYkQNzSqJR"
      },
      "outputs": [],
      "source": [
        "end_times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHeaYey0S3mr"
      },
      "source": [
        "### Setting Up Your Auditioner\n",
        "\n",
        "`Auditioner` is the main API to do the rules selection and model groups selection. It filters model groups using a two-step process.\n",
        "\n",
        "- Broad thresholds to filter out truly bad models\n",
        "- A selection rule grid to find the best model groups over time for each of a variety of methods\n",
        "\n",
        "Note that model groups that don't have a full set of `train_end_time` splits associated with them will be excluded from the analysis, so **it's important to ensure that all model groups have been completed across all train/test splits**\n",
        "\n",
        "When we set up our auditioner object, we need to give it a database connection, the model groups to consider (and optionally baseline model groups), train_end_times, and tell it how we're going to filter the models. Note that the `initial_metric_filters` parameter specified below tells `Auditioner` what metric and parameter we'll be using and starts off without any initial filtering constraints (which is what you'll typically want):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRjTq_cITqv1"
      },
      "outputs": [],
      "source": [
        "aud = Auditioner(\n",
        "    db_engine = conn,\n",
        "    model_group_ids = model_groups['model_groups'],\n",
        "    train_end_times = end_times,\n",
        "    initial_metric_filters = [{'metric': metric, 'parameter': parameter, 'max_from_best': 1.0, 'threshold_value': 0.0}],\n",
        "    distance_table = best_dist_table,\n",
        "    baseline_model_group_ids = model_groups['baseline_model_groups'] # optional\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnYukR-2WJYI"
      },
      "source": [
        "### Using Audition for Model Selection\n",
        "\n",
        "We can use the `plot_model_groups` method to visualize the performance of our model groups over temporal split (note that the plot may take a few minutes to generate). When this method is called, it applies the metric filters specificied to get rid of really bad model groups with respect the metric of interest. A model group is discarded if:\n",
        "\n",
        "- Itâ€™s never close to the â€œbestâ€ model (based on the `max_from_best` filter) or\n",
        "- If itâ€™s metric is below a certain number (based on the `threshold_value` filter) at least once\n",
        "\n",
        "As a starting point, we don't filter out any models, but can iteratively narrow our grid by refining these filters. Let's take a look at our models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-FZ-8JVYbvl"
      },
      "outputs": [],
      "source": [
        "aud.plot_model_groups()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0-YE3VYcVH"
      },
      "source": [
        "With our first default setting, we don't filter out models because `max_from_best=1.0` and `threshold_value=0.0` are the loosest criteria.\n",
        "\n",
        "- The first graph shows us the fraction of models worse than the best model by distance with respect to the metric of interest.\n",
        "- The second graph shows us the performance of a model group over time. The dashed line is the best case at that time period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQMTtcJ5ZORw"
      },
      "outputs": [],
      "source": [
        "# model groups that conform to the current filters\n",
        "aud.thresholded_model_group_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vfJKljfZWZS"
      },
      "source": [
        "We won't delve further into the model selection process here, but see the resources noted above for a deeper look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPm3NKoz4BLo"
      },
      "source": [
        "## Bias Audit\n",
        "\n",
        "Finally, we turn to the results of the bias audit using the toolkit [aequitas](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/), which can be found in the table `test_results.aequitas`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH5CgdzO4GX3"
      },
      "outputs": [],
      "source": [
        "pd.read_sql(\"\"\"SELECT * FROM test_results.aequitas LIMIT 5\"\"\", conn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJPLLBF4d-4h"
      },
      "source": [
        "For each model, the table contains the full set of confusion matrix values for the thresholds and each attribute value specified in your `bias_audit_config`, as well as disparities relative to reference group identified in that configuration file. Note that these disparities are calculated as ratios with the reference group as the denominator.\n",
        "\n",
        "Audition provides some tools for visualizing these results. If we assume the appropriate fairness metric for this context is true positive rate (TPR) disparity, let's take a look at how model 12 performed at the 10% threshold:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-X0aJWBiZIV"
      },
      "outputs": [],
      "source": [
        "import triage.component.postmodeling.fairness.aequitas_utils as au\n",
        "import aequitas.plot as ap\n",
        "\n",
        "# Load Data\n",
        "bdf = au.get_aequitas_results(conn, parameter = \"0.1_pct\", model_id = 12)\n",
        "\n",
        "# attribute of interest\n",
        "attribute = 'teacher_prefix'\n",
        "\n",
        "# fairness metric we care about\n",
        "metrics = ['tpr']\n",
        "\n",
        "# tolerance for disparities\n",
        "disparity_tolerance = 1.30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6XHYI28i84I"
      },
      "source": [
        "The `disparity` plot shows disparities for each group on the selected metric relative to the reference group:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSvZaJGIjPpO"
      },
      "outputs": [],
      "source": [
        "ap.disparity(bdf, metrics, attribute, fairness_threshold = disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyUpTI_RjeJK"
      },
      "source": [
        "The `absolute` plot will show the value of the true positive rate itself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q13a7KUjjryc"
      },
      "outputs": [],
      "source": [
        "ap.absolute(bdf, metrics, attribute, fairness_threshold = disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrl9izw7jxRw"
      },
      "source": [
        "We might also want to look at the disparities across models and how they relate to our metric for evaluating model accuracy to consider trade-offs between these goals, which we can do by joining the aequitas table to the evaluations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USxfuZrvlW9c"
      },
      "outputs": [],
      "source": [
        "acc_metric = 'precision@'\n",
        "disp_metric = 'tpr_disparity'\n",
        "acc_thresh = '10_pct'\n",
        "disp_thresh = '0.1_pct'\n",
        "attribute_name = 'teacher_prefix'\n",
        "attribute_value = 'Mrs.'\n",
        "\n",
        "df = pd.read_sql(f\"\"\"\n",
        "  WITH last_end_time AS (\n",
        "    SELECT max(train_end_time) AS train_end_time\n",
        "    FROM triage_metadata.models\n",
        "  )\n",
        "  , last_models AS (\n",
        "    SELECT model_id\n",
        "    FROM triage_metadata.models\n",
        "    JOIN last_end_time USING(train_end_time)\n",
        "  )\n",
        "  , eval_metrics AS (\n",
        "    SELECT e.model_id, e.stochastic_value\n",
        "    FROM test_results.evaluations e\n",
        "    JOIN last_models m USING(model_id)\n",
        "    WHERE e.metric = '{acc_metric}' AND e.parameter = '{acc_thresh}'\n",
        "  )\n",
        "  , fair_metrics AS (\n",
        "    SELECT a.model_id, a.{disp_metric}\n",
        "    FROM test_results.aequitas a\n",
        "    JOIN last_models m USING(model_id)\n",
        "    WHERE a.parameter = '{disp_thresh}'\n",
        "      AND a.attribute_name = '{attribute_name}'\n",
        "      AND a.attribute_value = '{attribute_value}'\n",
        "      AND tie_breaker = 'worst'\n",
        "  )\n",
        "  SELECT * \n",
        "  FROM eval_metrics e\n",
        "  JOIN fair_metrics f USING(model_id)\n",
        "  ORDER BY e.stochastic_value DESC\n",
        "  \"\"\", conn)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JiXPz-2obK1"
      },
      "outputs": [],
      "source": [
        "df.plot('stochastic_value', disp_metric, kind='scatter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFLsJwjSojeM"
      },
      "source": [
        "This final plot helps us visualize the fairness-accuracy trade-offs across the grid of models we ran, with the disparity metric on the y-axis (values closer to 1 don't favor either group) and the accuracy metric on the x-axis (higher values being more accurate).\n",
        "\n",
        "Although you might choose a model from this set based on these trade-offs, you might also want to consider some of the many fairness-enhancing methods that have been developed for machine learning recently. For an introduction, you may want to explore the materials from our [hands-on fairness tutorial](https://dssg.github.io/fairness_tutorial/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqYZSRXn6ni9"
      },
      "source": [
        "## Where Do I Go From Here?\n",
        "\n",
        "You've reached the end of this online `triage` tutorial, but have a few potential options for where to go next:\n",
        "- Feel free to tinker with the parameters, features, and outputs of the DonorsChoose example here for a working sandbox to start from (keeping in mind the resource constraints of colab, of course)\n",
        "- For a more comprehensive tutorial that explores `triage` concepts and functionality in much more depth, see our [Dirty Duck Tutorial](https://dssg.github.io/triage/dirtyduck/)\n",
        "- If you're ready to get started with using `triage` for your own project, check out our [Quickstart Guide](https://dssg.github.io/triage/quickstart/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "colab_triage.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "colab_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
