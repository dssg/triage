{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Triage","text":""},{"location":"#what-is-triage","title":"What is Triage?","text":"<p>Triage is an open source machine learning toolkit to help data scientists, machine learning developers, and analysts quickly prototype, build and evaluate end-to-end predictive risk modeling systems for public policy and social good problems.</p> <p>While many tools (sklearn, keras, pytorch, etc.) exist to build ML models, an end-to-end project requires a lot more than just building models. Developing AI/ML/data science systems requires making many design decisions that need to match how the system is going to be deployed and used. These choices then get turned into modeling choices and code. Triage lets you focus on the problem you\u2019re solving and guides you through design choices you need to make at each step of the machine learning pipeline.</p>"},{"location":"#how-to-get-started-with-triage","title":"How to get started with Triage?","text":""},{"location":"#go-through-a-quick-online-tutorial-with-sample-data-no-setup-required","title":"Go through a quick online tutorial with sample data (no setup required)","text":""},{"location":"#go-through-a-more-in-depth-tutorial-with-sample-data","title":"Go through a more in-depth tutorial with sample data","text":""},{"location":"#get-started-with-your-own-project-and-data","title":"Get started with your own project and data","text":""},{"location":"#background","title":"Background","text":"<p>Triage was initially developed at the University of Chicago's Center For Data Science and Public Policy and is now being maintained and extended at Carnegie Mellon University.</p>"},{"location":"db/","title":"Triage database provisioner","text":"<p>This document explains the purpose and behavior of the Triage database provisioner, accessed from the Triage CLI. It is optional and only intended for use if you don't have an existing Postgres database to use for Triage.</p> <p>The Triage database provisioner is just a single command:</p> <p><code>triage db up</code></p> <p>This command attempts to use docker to spawn a new Postgres 12 database. If successful, it will prompt you for a password to use for a user, and populate the connection information in <code>database.yaml</code> in the directory where you ran it from. The next time you run <code>triage db up</code>, it will look for the existing container and reuse it. </p> <p>At this point, you can use the database either from Triage or anything else that can connect to Postgres (eg. psql or dbeaver, using the credentials in the autogenerated <code>database.yaml</code>.</p>"},{"location":"db/#troubleshooting","title":"Troubleshooting","text":""},{"location":"db/#no-docker","title":"No docker","text":"<p>The command does require some version of Docker. We recommend getting it from the official Docker downloads page.</p>"},{"location":"db/#cant-log-in","title":"Can't log in","text":"<p>Because of the way Docker volumes work, if you manually remove the Docker container created by <code>triage db up</code>, the volume will still be around. This is usually fine, but the superuser credential information will persist as well, which means the next time you spawn the database, the Postgres server will not take the new credential information into account. Under normal usage (simply calling <code>triage db up</code> and never removing the container), you will never run into this situation. But if you do, and you would like to use a new username/password, you will have to remove the volume before recreating. This can be done with <code>docker volume rm triage-db-data</code>. This will also remove all of the stored data in Postgres, so beware!</p>"},{"location":"quickstart/","title":"Quickstart guide to using Triage","text":""},{"location":"quickstart/#1-install-triage","title":"1. Install Triage","text":"<p>Triage can be installed using pip or through python setup.py. It requires Python 3.8+ and access to a postgresql database (for now). If you're interested in helping port the queries to support other databases, let us know. Ideally you have full access to a database so triage can create additional schemas inside that it needs to store metadata, predictions, and evaluation metrics.</p> <p>To install Triage locally, you need: - Ubuntu/RedHat - Python 3.8+ - A PostgreSQL 9.6+ database with your source data (events,   geographical data, etc) loaded.   - NOTE: If your database is PostgreSQL 11+ you will get some     speed improvements. We recommend updating to a recent     version of PostgreSQL. - Ample space on an available disk, (or for example in Amazon Web   Services's S3), to store the matrices and models that will be created for your   experiments</p> <p>We also recommend installing triage inside a python virtual environment for your project so you don't have any conflicts with other packages installed on the machine. You can use virutalenv or pyenv to do that.</p> <p>If you use pyenv (be sure your default python is 3.8+): <pre><code>$ pyenv virtualenv triage-env\n$ pyenv activate triage-env\n(triage-env) $ pip install triage\n</code></pre></p> <p>If you use virtualenv (be sure your default python is 3+): <pre><code>$ virtualenv triage-env\n$ . triage-env/bin/activate\n(triage-env) $ pip install triage\n</code></pre> If you get an error related to pg_config executable, run the following command (make sure you have sudo access): <pre><code>(triage-env) $ sudo apt-get install libpq-dev python3.9-dev\n</code></pre> Then rerun pip install triage <pre><code>(triage-env) $ pip install triage\n</code></pre> To test if triage was installed correctly, type: <pre><code>(triage-env) $ triage -h\n</code></pre></p> <p></p>"},{"location":"quickstart/#2-make-sure-you-have-access-to-a-postgres-database","title":"2. Make sure you have access to a Postgres database","text":"<p>You'll need to have the servername, databasename, username, and password and put it in a credentials file in Step 5 below.</p>"},{"location":"quickstart/#3-structure-your-data","title":"3. Structure your data","text":"<p>The simplest way to start is to structure your data as a series of events that are connected to your entity of interest (person, organization, business, etc.) that take place at a certain time. Each row of the data will be an event. Each event can have some <code>event_id</code>, and an <code>entity_id</code> to link it to the entity it happened to, a date field, as well as additional attributes about the event (<code>type</code>, for example) and the entity (<code>age</code>, <code>gender</code>, <code>race</code>, etc.). A sample row might look like:</p> <p><pre><code>event_id, entity_id, date, event_attribute (type), entity_attribute (age), entity_attribute (gender), ...\n121, 19334, 1/1/2013, Placement, 12, Male, ...\n</code></pre> Triage needs a field named <code>entity_id</code> (that needs to be of type integer) to refer to the primary entities of interest in our project. It also needs a date field to specify the date each event occurred in order to use it appropriately in building and validating models.</p>"},{"location":"quickstart/#examples","title":"Examples","text":"<ol> <li> <p>Healthcare: Typical data from EHR systems will have a table about the demographics of each patient. The <code>entity_id</code> will be the patient id (typically MRN) here. Then there are tables that have a row for each <code>encounter</code>, or <code>diagnosis</code>, or <code>procedure</code> with a patient identified (through the entity_id column), a timestamp, and additional information/columns about that <code>encounter</code>, or <code>diagnosis</code>, or <code>procedure</code>. All of these tables are provided as input to triage in a postgresql database.</p> </li> <li> <p>Education: the entity_id will typically be the student_id and the events include things like a grade in a class in a given year, a test score n a test at a given time, graduation, etc.</p> </li> </ol>"},{"location":"quickstart/#4-set-up-triage-configuration-files","title":"4. Set up Triage configuration files","text":"<p>The Triage configuration file sets up the modeling process to match the deployment scenario the models will be used in. This involves defining the cohort to train/predict on, when the prediction is taking place, the outcome we're predicting, how far out we're predicting, how often will the model be updated, how often will the predicted list be used for interventions, the quantity of resources available to intervene to define the evaluation metric, etc.</p> <p>A lot of details about each section of the configuration file can be found here, but for the moment we'll start with the much simpler configuration file below:</p> <pre><code>config_version: 'v8'\n\nmodel_comment: 'quickstart_test_run'\nrandom_seed: 1234\n\ntemporal_config:\n    label_timespans: ['&lt;&lt; YOUR_VALUE_HERE &gt;&gt;']\n\nlabel_config:\n  filepath: '&lt;&lt; YOUR_VALUE_HERE &gt;&gt;'\n  name: 'quickstart_label'\n\nfeature_aggregations:\n  -\n    prefix: 'qstest'\n    from_obj: '&lt;&lt; YOUR_VALUE_HERE &gt;&gt;'\n    knowledge_date_column: '&lt;&lt; YOUR_VALUE_HERE &gt;&gt;'\n\n    aggregates_imputation:\n      count:\n        type: 'zero_noflag'\n\n    aggregates:\n      -\n        quantity:\n          total: \"*\"\n        metrics:\n          - 'count'\n\n    intervals: ['all']\n\nmodel_grid_preset:  'quickstart'\n\nscoring:\n    testing_metric_groups:\n        -\n          metrics: [precision@]\n          thresholds:\n            percentiles: [1]\n\n\n    training_metric_groups:\n      -\n          metrics: [precision@]\n          thresholds:\n            percentiles: [1]\n</code></pre> <p>Copy that code block into your text editor of choice and save it as something like <code>quickstart-config.yaml</code> in your working directory for your project. You must fill out the sections marked <code>&lt;&lt; YOUR_VALUE_HERE &gt;&gt;</code> with values appropriate to your project.</p> <p>The configuration file has a lot of sections. As a first pass, we will infer a lot of the parameters that are needed in there and use defaults for others. The primary parameters to specify (for now) are:</p> <ol> <li> <p>TIMECHOP config: This sets up temporal parameters for training and    testing models. The key things to set up here are your prediction    horizon/timespan (how  far out in the future do you want to    predict?). For example, if you want to predict an outcome within    one year, you would set <code>label_timespans = '12month'</code>. See    our guide to Temporal    Validation</p> </li> <li> <p>LABEL config: This is a <code>sql</code> query that defines the outcome of    interest. </p> </li> </ol> <p>The query should return a relation containing the columns    - <code>entity_id</code>: each <code>entity_id</code> affected by an event within the amount of time specified by <code>label_timespan</code> after a given <code>as_of_date</code>    - <code>outcome</code>: a binary variable representing the events that happened to each entity, within the period specified by that <code>as_of_date</code> and <code>label_timespan</code></p> <p>The query is parameterized over <code>as_of_date</code>, and <code>label_timespan</code>. These parameters are passed to your query as named keywords using the Python's <code>str.format()</code> method. You can use them in your query by surrounding their keywords with curly braces (as in the example below).</p> <p>See our    guide to Labels for a more in-depth discussion of this topic.</p> <p>Example Query </p> <p>Given a source table called <code>semantic.events</code>, with the following structure:</p> entity_id event_date label 135 2014-06-04 1 246 2013-11-05 0 135 2013-04-19 0 <p>Assuming an early-warning problem, where a client wants to predict the likelihood that each entity experiences at least one positive event (such as a failed inspection) within some period of time, we could use the following label query:</p> <pre><code>select entity_id, max(label) as outcome\nfrom semantic.events\nwhere '{as_of_date}'::timestamp &lt;= event_date\n      and event_date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\n</code></pre> <p>For each <code>as_of_date</code>, this query returns:    - all <code>entity_ids</code> that experienced at least one event (such as an inspection) within the amount of time specified by <code>label_timespan</code>    - a binary variable that equals 1 if an entity experienced at least one positive event (failed inspection), or 0 if all events experienced by the entity had negative results.</p> <ol> <li> <p>FEATURE config: This is where we define different aggregate    features/attributes/variables to be created and used in our machine    learning models. We need at least one feature specified here. For the purposes of the quickstart, let's just take the count of all events before the modeling date. In the template, you can simply fill in <code>from_obj</code> with the <code>schema.table_name</code> where your data can be found (but this can also be a more complex query in general) and <code>knowledge_date_column</code> with that table's date column.</p> </li> <li> <p>MODEL_GRID_PRESET config: Which models and hyperparameters we want to try in    this run. We can start with <code>quickstart</code> that will run a quick    model grid to test if everything works.</p> </li> </ol> <p>Additionally, we will need a database credential file that contains the name of the database, server, username, and password to use to connect to it:</p> <pre><code># Connecting to the database requires a configuration file like this one but\n# named database.yaml\n\nhost: address.of.database.server\nuser: user_name\ndb: database_name\npass: user_password\nport: connection_port (often 5432)\n</code></pre> <p>Copy this into a separate text file, fill in your values and save it as <code>database.yaml</code> in the working directory where you'll be running triage. Note, however, that if you have a <code>DATABASE_URL</code> environment variable set, triage will use this by default as well.</p>"},{"location":"quickstart/#5-run-triage","title":"5. Run Triage","text":"<p>An overview of different steps that take place when you run Triage is here</p> <p>For this quickstart, you shouldn't need much free disk space, but note that in general your project path will contain both data matrices and trained model objects, so will need to have ample free space (you can also specify a location in S3 if you don't want to store the files locally).</p> <p>If you want a bit more detail or documentation, a good overview of running an experiment in triage is here</p>"},{"location":"quickstart/#using-triage-cli","title":"Using Triage CLI:","text":"<ol> <li> <p>Validate the configuration files by running: <pre><code>triage experiment config.yaml --project-path '/project_directory' --validate-only\n</code></pre></p> </li> <li> <p>Run Triage <pre><code>triage experiment config.yaml --project-path '/project_directory'\n</code></pre></p> </li> </ol>"},{"location":"quickstart/#using-the-triage-python-interface","title":"Using the Triage python interface:","text":"<ol> <li> <p>Import packages and load config files: <pre><code>import yaml\nfrom triage.experiments import SingleThreadedExperiment\nfrom sqlalchemy.engine.url import URL\nfrom triage.util.db import create_engine\n\nwith open('config.yaml', 'r') as fin:\n    experiment_config = yaml.load(fin)\n\nwith open('database.yaml', 'r') as fin:\n    db_config = yaml.load(fin)\n</code></pre></p> </li> <li> <p>Create a database engine and Triage experiment  <pre><code># create postgres database url\ndb_url = URL(\n            'postgres',\n            host=db_config['host'],\n            username=db_config['user'],\n            database=db_config['db'],\n            password=db_config['pass'],\n            port=db_config['port'],\n        )\n\nexperiment = SingleThreadedExperiment(\n    config=experiment_config\n    db_engine=create_engine(db_url)\n    project_path='/path/to/directory/to/save/data'\n)\n</code></pre></p> </li> <li> <p>Validate your config</p> </li> </ol> <pre><code>experiment.validate()\n</code></pre> <ol> <li>Run Triage <pre><code>experiment.run()\n</code></pre></li> </ol>"},{"location":"quickstart/#6-look-at-results-generated-by-triage","title":"6. Look at results generated by Triage","text":"<p>Once the feature/cohort/label/matrix building is done and the experiment has moved onto modeling, check out the <code>triage_metadata.models</code> and <code>test_results.evaluations</code> tables as data starts to come in.</p> <p>Here are a couple of quick queries to help get you started:</p> <p>Tables in the <code>triage_metadata</code> schema have some general information about experiments that you've run and the models they created. The <code>quickstart</code> model grid preset should have built 3 models. You can check that with:</p> <pre><code>select \n  model_id, model_group_id, model_type \n  from \n      triage_metadata.models;\n</code></pre> <p>This should give you a result that looks something like:</p> model_id model_group_id model_type 1 1 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression 2 2 sklearn.tree.DecisionTreeClassifier 3 3 sklearn.dummy.DummyClassifier <p>If you want to see predictions for individual entities, you can check out <code>test_results.predictions</code>, for instance:</p> <pre><code>select \n  model_id, entity_id, as_of_date, score, label_value\n  from\n      test_results.predictions\n  limit 5;\n</code></pre> <p>This will give you something like:</p> model_id entity_id as_of_date score label_value 1 15596 2017-09-29 00:00:00 0.21884 0 2 15596 2017-09-29 00:00:00 0.22831 0 3 15596 2017-09-29 00:00:00 0.25195 0 <p>Finally, <code>test_results.evaluations</code> holds some aggregate information on model performance:</p> <pre><code>select \n  model_id, metric, parameter, stochastic_value\n  from\n      test_results.evaluations\n  order by model_id, metric, parameter;\n</code></pre> <p>Feel free to explore some of the other tables in these schemas (note that there's also a <code>train_results</code> schema with performance on the training set as well as feature importances, where defined).</p> <p>In a more complete modeling run, you could <code>audition</code> the mdoels created using jupyter notebooks to help you select the best-performing model specifications from a wide variety of options (see the overview of model selection and tutorial audition notebook) and <code>postmodeling</code> to delve deeper into understanding these models (see the README and tutorial postmodeling notebook).</p> <p>You can also look at the directory you specified in the triage run as the project-path (--project-path '/project_directory') for matrices (stored as gzipped csvs) and model objects (stored as pickles).</p>"},{"location":"quickstart/#7-iterate-and-explore","title":"7. Iterate and Explore","text":"<p>Now that you have triage running, continue onto the suggested project workflow for some tips about how to iterate and tune the pipeline for your project.</p> <p>Alternatively, if you'd like more of a guided tour with sample data, check out our [colab tutorial](https://colab.research.google.com/github/dssg/triage/blob/master/example/colab/colab_triage.ipynb) that is hosted so you don't need to install anything or the dirty duck tutorial.</p>"},{"location":"triage.experiments.base/","title":"Triage.experiments.base","text":"<p>Source: triage/experiments/base.py#L0</p>"},{"location":"triage.experiments.base/#global-variables","title":"Global Variables","text":"<ul> <li>CONFIG_VERSION</li> </ul>"},{"location":"triage.experiments.base/#dt_from_str","title":"dt_from_str","text":"<pre><code>dt_from_str(dt_str)\n</code></pre>"},{"location":"triage.experiments.base/#experimentbase","title":"ExperimentBase","text":"<p>The Base class for all Experiments.</p>"},{"location":"triage.experiments.base/#experimentbaseall_as_of_times","title":"ExperimentBase.all_as_of_times","text":"<p>All 'as of times' in experiment config</p> <p>Used for label and feature generation.</p> <p>Returns: (list) of datetimes</p>"},{"location":"triage.experiments.base/#experimentbaseall_label_windows","title":"ExperimentBase.all_label_windows","text":"<p>All train and test label windows</p> <p>Returns: (list) label windows, in string form as they appeared in the experiment config</p>"},{"location":"triage.experiments.base/#experimentbasecollate_aggregations","title":"ExperimentBase.collate_aggregations","text":"<p>collate Aggregation objects used by this experiment.</p> <p>Returns: (list) of collate.Aggregation objects</p>"},{"location":"triage.experiments.base/#experimentbasefeature_dicts","title":"ExperimentBase.feature_dicts","text":"<p>Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups.</p> <p>Returns: (list) of dicts, keys being feature table names and values being lists of feature names</p>"},{"location":"triage.experiments.base/#experimentbasefeature_table_tasks","title":"ExperimentBase.feature_table_tasks","text":"<p>All feature table query tasks specified by this Experiment</p> <p>Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands</p>"},{"location":"triage.experiments.base/#experimentbasefull_matrix_definitions","title":"ExperimentBase.full_matrix_definitions","text":"<p>Full matrix definitions</p> <p>Returns: (list) temporal and feature information for each matrix</p>"},{"location":"triage.experiments.base/#experimentbasemaster_feature_dictionary","title":"ExperimentBase.master_feature_dictionary","text":"<p>All possible features found in the database. Not all features will necessarily end up in matrices</p> <p>Returns: (list) of dicts, keys being feature table names and values being lists of feature names</p>"},{"location":"triage.experiments.base/#experimentbasematrix_build_tasks","title":"ExperimentBase.matrix_build_tasks","text":"<p>Tasks for all matrices that need to be built as a part of this Experiment.</p> <p>Each task contains arguments understood by Architect.build_matrix</p> <p>Returns: (list) of dicts</p>"},{"location":"triage.experiments.base/#experimentbasesplit_definitions","title":"ExperimentBase.split_definitions","text":"<p>Temporal splits based on the experiment's configuration</p> <p>Returns: (dict) temporal splits</p> <p>Example:</p> <pre><code>{\n  'beginning_of_time': {datetime},\n  'modeling_start_time': {datetime},\n  'modeling_end_time': {datetime},\n  'train_matrix': {\n  'matrix_start_time': {datetime},\n  'matrix_end_time': {datetime},\n  'as_of_times': [list of {datetime}s]\n  },\n  'test_matrices': [list of matrix defs similar to train_matrix]\n}\n</code></pre>"},{"location":"triage.experiments.base/#experimentbase__init__","title":"ExperimentBase.<code>__init__</code>","text":"<pre><code>__init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"triage.experiments.base/#experimentbasebuild_matrices","title":"ExperimentBase.build_matrices","text":"<pre><code>build_matrices(self)\n</code></pre> <p>Generate labels, features, and matrices</p>"},{"location":"triage.experiments.base/#experimentbasecatwalk","title":"ExperimentBase.catwalk","text":"<pre><code>catwalk(self)\n</code></pre> <p>Train, test, and evaluate models</p>"},{"location":"triage.experiments.base/#experimentbasegenerate_labels","title":"ExperimentBase.generate_labels","text":"<pre><code>generate_labels(self)\n</code></pre> <p>Generate labels based on experiment configuration</p> <p>Results are stored in the database, not returned</p>"},{"location":"triage.experiments.base/#experimentbasegenerate_sparse_states","title":"ExperimentBase.generate_sparse_states","text":"<pre><code>generate_sparse_states(self)\n</code></pre>"},{"location":"triage.experiments.base/#experimentbaseinitialize_components","title":"ExperimentBase.initialize_components","text":"<pre><code>initialize_components(self)\n</code></pre>"},{"location":"triage.experiments.base/#experimentbaseinitialize_factories","title":"ExperimentBase.initialize_factories","text":"<pre><code>initialize_factories(self)\n</code></pre>"},{"location":"triage.experiments.base/#experimentbaselog_split","title":"ExperimentBase.log_split","text":"<pre><code>log_split(self, split_num, split)\n</code></pre>"},{"location":"triage.experiments.base/#experimentbasematrix_store","title":"ExperimentBase.matrix_store","text":"<pre><code>matrix_store(self, matrix_uuid)\n</code></pre> <p>Construct a matrix store for a given matrix uuid, using the Experiment's #matrix_store_class</p> <p>Args:</p> <p>matrix_uuid (string) A uuid for a matrix</p>"},{"location":"triage.experiments.base/#experimentbaserun","title":"ExperimentBase.run","text":"<pre><code>run(self)\n</code></pre>"},{"location":"triage.experiments.base/#experimentbaseupdate_split_definitions","title":"ExperimentBase.update_split_definitions","text":"<pre><code>update_split_definitions(self, new_split_definitions)\n</code></pre> <p>Update split definitions</p> <p>Args: (dict) split definitions (should have matrix uuids)</p>"},{"location":"triage.experiments.multicore/","title":"Triage.experiments.multicore","text":"<p>Source: triage/experiments/multicore.py#L0</p>"},{"location":"triage.experiments.multicore/#insert_into_table","title":"insert_into_table","text":"<pre><code>insert_into_table(insert_statements, feature_generator_factory, db_connection_string)\n</code></pre>"},{"location":"triage.experiments.multicore/#build_matrix","title":"build_matrix","text":"<pre><code>build_matrix(build_tasks, planner_factory, db_connection_string)\n</code></pre>"},{"location":"triage.experiments.multicore/#train_model","title":"train_model","text":"<pre><code>train_model(train_tasks, trainer_factory, db_connection_string)\n</code></pre>"},{"location":"triage.experiments.multicore/#test_and_evaluate","title":"test_and_evaluate","text":"<pre><code>test_and_evaluate(model_ids, predictor_factory, evaluator_factory, indiv_importance_factory, \\\n    test_store, db_connection_string, split_def, train_matrix_columns, config)\n</code></pre>"},{"location":"triage.experiments.multicore/#multicoreexperiment","title":"MultiCoreExperiment","text":"<p>The Base class for all Experiments.</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentall_as_of_times","title":"MultiCoreExperiment.all_as_of_times","text":"<p>All 'as of times' in experiment config</p> <p>Used for label and feature generation.</p> <p>Returns: (list) of datetimes</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentall_label_windows","title":"MultiCoreExperiment.all_label_windows","text":"<p>All train and test label windows</p> <p>Returns: (list) label windows, in string form as they appeared in the experiment config</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentcollate_aggregations","title":"MultiCoreExperiment.collate_aggregations","text":"<p>collate Aggregation objects used by this experiment.</p> <p>Returns: (list) of collate.Aggregation objects</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentfeature_dicts","title":"MultiCoreExperiment.feature_dicts","text":"<p>Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups.</p> <p>Returns: (list) of dicts, keys being feature table names and values being lists of feature names</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentfeature_table_tasks","title":"MultiCoreExperiment.feature_table_tasks","text":"<p>All feature table query tasks specified by this Experiment</p> <p>Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentfull_matrix_definitions","title":"MultiCoreExperiment.full_matrix_definitions","text":"<p>Full matrix definitions</p> <p>Returns: (list) temporal and feature information for each matrix</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentmaster_feature_dictionary","title":"MultiCoreExperiment.master_feature_dictionary","text":"<p>All possible features found in the database. Not all features will necessarily end up in matrices</p> <p>Returns: (list) of dicts, keys being feature table names and values being lists of feature names</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentmatrix_build_tasks","title":"MultiCoreExperiment.matrix_build_tasks","text":"<p>Tasks for all matrices that need to be built as a part of this Experiment.</p> <p>Each task contains arguments understood by Architect.build_matrix</p> <p>Returns: (list) of dicts</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentsplit_definitions","title":"MultiCoreExperiment.split_definitions","text":"<p>Temporal splits based on the experiment's configuration</p> <p>Returns: (dict) temporal splits</p> <p>Example:</p> <pre><code>{\n  'beginning_of_time': {datetime},\n  'modeling_start_time': {datetime},\n  'modeling_end_time': {datetime},\n  'train_matrix': {\n  'matrix_start_time': {datetime},\n  'matrix_end_time': {datetime},\n  'as_of_times': [list of {datetime}s]\n  },\n  'test_matrices': [list of matrix defs similar to train_matrix]\n}\n</code></pre>"},{"location":"triage.experiments.multicore/#multicoreexperiment__init__","title":"MultiCoreExperiment.<code>__init__</code>","text":"<pre><code>__init__(self, n_processes=1, n_db_processes=1, *args, **kwargs)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentbuild_matrices","title":"MultiCoreExperiment.build_matrices","text":"<pre><code>build_matrices(self)\n</code></pre> <p>Generate labels, features, and matrices</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentcatwalk","title":"MultiCoreExperiment.catwalk","text":"<pre><code>catwalk(self)\n</code></pre> <p>Train, test, and evaluate models</p>"},{"location":"triage.experiments.multicore/#multicoreexperimentparallelize","title":"MultiCoreExperiment.parallelize","text":"<pre><code>parallelize(self, partially_bound_function, tasks, n_processes, chunksize=1)\n</code></pre>"},{"location":"triage.experiments.multicore/#multicoreexperimentparallelize_with_success_count","title":"MultiCoreExperiment.parallelize_with_success_count","text":"<pre><code>parallelize_with_success_count(self, partially_bound_function, tasks, n_processes, chunksize=1)\n</code></pre>"},{"location":"triage.experiments.singlethreaded/","title":"Triage.experiments.singlethreaded","text":"<p>Source: triage/experiments/singlethreaded.py#L0</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperiment","title":"SingleThreadedExperiment","text":"<p>The Base class for all Experiments.</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentall_as_of_times","title":"SingleThreadedExperiment.all_as_of_times","text":"<p>All 'as of times' in experiment config</p> <p>Used for label and feature generation.</p> <p>Returns: (list) of datetimes</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentall_label_windows","title":"SingleThreadedExperiment.all_label_windows","text":"<p>All train and test label windows</p> <p>Returns: (list) label windows, in string form as they appeared in the experiment config</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentcollate_aggregations","title":"SingleThreadedExperiment.collate_aggregations","text":"<p>collate Aggregation objects used by this experiment.</p> <p>Returns: (list) of collate.Aggregation objects</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfeature_dicts","title":"SingleThreadedExperiment.feature_dicts","text":"<p>Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups.</p> <p>Returns: (list) of dicts, keys being feature table names and values being lists of feature names</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfeature_table_tasks","title":"SingleThreadedExperiment.feature_table_tasks","text":"<p>All feature table query tasks specified by this Experiment</p> <p>Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfull_matrix_definitions","title":"SingleThreadedExperiment.full_matrix_definitions","text":"<p>Full matrix definitions</p> <p>Returns: (list) temporal and feature information for each matrix</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentmaster_feature_dictionary","title":"SingleThreadedExperiment.master_feature_dictionary","text":"<p>All possible features found in the database. Not all features will necessarily end up in matrices</p> <p>Returns: (list) of dicts, keys being feature table names and values being lists of feature names</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentmatrix_build_tasks","title":"SingleThreadedExperiment.matrix_build_tasks","text":"<p>Tasks for all matrices that need to be built as a part of this Experiment.</p> <p>Each task contains arguments understood by Architect.build_matrix</p> <p>Returns: (list) of dicts</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentsplit_definitions","title":"SingleThreadedExperiment.split_definitions","text":"<p>Temporal splits based on the experiment's configuration</p> <p>Returns: (dict) temporal splits</p> <p>Example:</p> <pre><code>{\n  'beginning_of_time': {datetime},\n  'modeling_start_time': {datetime},\n  'modeling_end_time': {datetime},\n  'train_matrix': {\n  'matrix_start_time': {datetime},\n  'matrix_end_time': {datetime},\n  'as_of_times': [list of {datetime}s]\n  },\n  'test_matrices': [list of matrix defs similar to train_matrix]\n}\n</code></pre>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperiment__init__","title":"SingleThreadedExperiment.<code>__init__</code>","text":"<pre><code>__init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True)\n</code></pre> <p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentbuild_matrices","title":"SingleThreadedExperiment.build_matrices","text":"<pre><code>build_matrices(self)\n</code></pre> <p>Generate labels, features, and matrices</p>"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentcatwalk","title":"SingleThreadedExperiment.catwalk","text":"<pre><code>catwalk(self)\n</code></pre> <p>Train, test, and evaluate models</p>"},{"location":"triage_project_workflow/","title":"Using <code>triage</code> for a Project: Workflow Tips","text":"<p>Getting Started...</p> <p>The setup and first iteration here closely follow the QuickStart Guide, so that may be a good place to start if you're new to <code>triage</code>.</p> <p>If you've already completed the QuickStart and have a working environment, you may want to jump ahead to Iteration 2</p>"},{"location":"triage_project_workflow/#step-1-get-your-data-set-up","title":"Step 1: Get your data set up","text":"<p>Triage needs data in a <code>Postgresql</code> database, with at least one table that contains <code>events</code> (one per row) and <code>entities</code> of interest (people, place, organization, etc.; identified by an integer-valued <code>entity_id</code>), a <code>timestamp</code> (specifying when the event occurred), and additional attributes of interest about the event and/or entity (demographics for example).</p> <p>We will need database credentials either in a config file or as an environment variable called <code>DATABASE_URL</code>  that contains the name  of the database, server, username, and password to use to connect to it.</p>"},{"location":"triage_project_workflow/#iteration-1-quick-check","title":"Iteration 1: Quick Check","text":"<p>This set up will run a quick sanity check to make sure everything is set up correctly and that triage runs with your data and set up.</p>"},{"location":"triage_project_workflow/#configuration","title":"Configuration","text":"<p>The full <code>triage</code> configuration file has a lot of sections allowing for extensive customization. In the first iteration, we'll set up the minimal parameters necessary to get started and make sure you have a working <code>triage</code> setup, but will describe how to better customize this configuration to your project in subsequent iterations. The starting point here is the QuickStart <code>triage</code> config found here.</p> <ol> <li>Define <code>outcome/label</code> of interest: This is a <code>SQL</code> query that defines outcome we want to predict and needs to return The query must return two columns: <code>entity_id</code> and <code>outcome</code>, based on a given <code>as_of_date</code> and <code>label_timespan</code>. For more detail, see our guide to Labels.</li> <li>Define <code>label_timespan</code> ( = '12month' for example for predicting one year out)</li> <li>Specify <code>features_aggregations</code> (at least one is neded to run <code>triage</code> -- for the QuickStart config, you need only specify an events table and we'll start with a simple count)</li> <li>Specify <code>scoring</code> (i.e. evaluation metrics) to calculate (at least one is needed)</li> <li>Specify <code>model_grid_preset</code> (i.e. model grid) for <code>triage</code> to run models with different hyperparameters (set to <code>'quickstart'</code> at this time)</li> </ol>"},{"location":"triage_project_workflow/#run-triage","title":"Run <code>triage</code>","text":"<ol> <li> <p>Check triage config <pre><code>triage experiment config.yaml --project-path '/project_directory' --validate-only\n</code></pre></p> </li> <li> <p>Run the pipeline <pre><code>triage experiment config.yaml --project-path '/project_directory'\n</code></pre></p> </li> </ol> <p>Alternatively, you can also import <code>triage</code> as a package in your python scrips and run it that way. Learn more about that option here.</p>"},{"location":"triage_project_workflow/#check-for-results","title":"Check for Results","text":"<p>For this quick check, we're only running a handful of models for a single time window, so <code>triage</code>'s tools for model selection and postmodeling analysis won't be instructive, but you can confirm that by checking out the <code>triage</code>-created tables in the <code>triage_metadata</code>, <code>test_results</code>, and <code>train-results</code> schemas in your database. In particular, you should find records for all your expected models in <code>triage_metadata.models</code>, predictions from every model for every entity in <code>test_results.predictions</code>, and aggregate performance metrics in <code>test_results.evaluations</code> for every model.</p> <p>If that all looks good, it's time to get started with customizing your modeling configuration to your project...</p>"},{"location":"triage_project_workflow/#iteration-2-refine-the-cohort-and-temporal-setup","title":"Iteration 2: Refine the cohort and temporal setup","text":"<p>In this next iteration, we'll stick with a simplified feature configuration, but start to refine the parameters that control your modeling universe and cross-validation. We'll also introduce <code>audition</code>, <code>triage</code>'s component for model selection. However, with the limited feature set, we'll stick with the <code>quickstart</code> model grid for the time being.</p>"},{"location":"triage_project_workflow/#define-your-cohort","title":"Define your cohort","text":"<p>For <code>triage</code>, the <code>cohort</code> represents the universe of relevant entities for a model at a given point in time. In the first iteration, we omitted the <code>cohort_config</code> section of our experiment configuration, in which case <code>triage</code> simply includes every entity it can find in your feature configuration. However, in most cases, you'll likely want to focus on a smaller set of entities, for instance:</p> <ul> <li> <p>In a housing inspection project, you might want to include only houses that were built before a given modeling date and occupied at the modeling date</p> </li> <li> <p>In a project to help allocate housing subsidies, only certain individuals in your data might be eligible for the intervention at a given point in time</p> </li> <li> <p>In a recidivism prediction project, you might want to exclude individuals who are incarcerated as of the modeling date</p> </li> </ul> <p>You can specify a cohort in your config file with a filepath to a SQL query that returns a set of integer <code>entity_id</code>s for a given modeling date (parameterized as <code>{as_of_date}</code> in your query). For instance:</p> <pre><code>cohort_config:\n    filepath: 'cohorts/past_events.sql'\n    name: 'past_events'\n</code></pre> <pre><code>select entity_id\nfrom events\nwhere outcome_date &lt; '{as_of_date}'\n</code></pre>"},{"location":"triage_project_workflow/#configure-your-temporal-settings","title":"Configure your temporal settings","text":"<p>In most real-world machine learning applications, you're interested in training on past data and predicting forward into the future. <code>triage</code> is built with this common use-case in mind, relying on temporal cross-validation to evaluate your models' performance in a manner that best reflects how it will be used in practice.</p> <p>There is a lot of nuance to the temporal configuration and it can take a bit of effort to get right. If you're new to <code>triage</code> (or want a refresher), we highly reccomend you check out the temporal crossvalidation deep dive.</p> <p>In previous iteration, we used a highly simplified temporal config, with just one parameter: <code>label_timespans</code>, yielding a single time split to get us started. However, these default values are generally not particularly meaningful in most cases and you'll need to fill out a more detailed <code>temporal_config</code>. Here's what that might look like:</p> <pre><code>temporal_config:\n    feature_start_time: '1995-01-01' # earliest date included in features\n    feature_end_time: '2015-01-01'   # latest date included in features\n    label_start_time: '2012-01-01' # earliest date for which labels are avialable\n    label_end_time: '2015-01-01' # day AFTER last label date (all dates in any model are &lt; this date)\n    model_update_frequency: '6month' # how frequently to retrain models\n    training_as_of_date_frequencies: '1day' # time between as of dates for same entity in train matrix\n    test_as_of_date_frequencies: '3month' # time between as of dates for same entity in test matrix\n    max_training_histories: ['6month', '3month'] # length of time included in a train matrix\n    test_durations: ['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end)\n    training_label_timespans: ['1month'] # time period across which outcomes are labeled in train matrices\n    test_label_timespans: ['7day'] # time period across which outcomes are labeled in test matrices\n</code></pre> <p>For more detailed guidance on how to think about each of these parameters and set them for your context, see the deep dive, but here are a couple of quick notes:</p> <ul> <li> <p>The <code>feature_start_time</code> should reflect the earliest time available for your features, and will often be considerably earlier than the <code>label_start_time</code>.</p> </li> <li> <p>All of your train/test splits will be between the <code>label_start_time</code> and <code>label_end_time</code>, with splits starting from the last date and working backwards. Note that the <code>label_end_time</code> should be 1 day AFTER the last label date.</p> </li> <li> <p>If you're using the same label timespan in both training and testing, you can still use the single <code>label_timespans</code> parameter (as we did in the QuickStart config). If you need different values, you can separately configure <code>test_label_timespans</code> and <code>training_label_timespans</code> (but note in this case, you should omit <code>label_timespans</code>).</p> </li> <li> <p>The parameters with plural names (e.g., <code>test_durations</code>) can be given as lists, in which case, <code>triage</code> will run models using all possible combinations of these values. This can get complicated fast, so you're generally best off starting with a single value for each parameter, for instance:</p> </li> </ul> <pre><code>temporal_config:\n    feature_start_time: '1980-01-01'\n    feature_end_time: '2019-05-01'\n    label_start_time: '2012-01-01'\n    label_end_time: '2019-05-01'\n    model_update_frequency: '1month'\n    label_timespans: ['1y']\n    max_training_histories: ['0d']\n    training_as_of_date_frequencies: ['1y']\n    test_as_of_date_frequencies: ['1y']\n    test_durations: ['0d']\n</code></pre> <p>As you figure out your temporal parameters, you can use the <code>triage</code> CLI's <code>--show-timechop</code> parameter to visualize the resulting time splits:</p> <pre><code>triage experiment config.yaml --project-path '/project_directory' --show-timechop\n</code></pre>"},{"location":"triage_project_workflow/#set-a-random_seed","title":"Set a <code>random_seed</code>","text":"<p>You may want to set an integer-valued <code>random_seed</code> for python to use in your configuration file in order to ensure reproducibility of your results across <code>triage</code> runs.</p>"},{"location":"triage_project_workflow/#run-triage_1","title":"Run <code>triage</code>","text":"<ol> <li> <p>Check triage config <pre><code>triage experiment config.yaml --project-path '/project_directory' --validate-only\n</code></pre></p> </li> <li> <p>Run the pipeline <pre><code>triage experiment config.yaml --project-path '/project_directory'\n</code></pre></p> </li> </ol> <p>Alternatively, you can also import <code>triage</code> as a package in your python scrips and run it that way. Learn more about that option here.</p>"},{"location":"triage_project_workflow/#check-for-results_1","title":"Check for Results","text":""},{"location":"triage_project_workflow/#check-the-database","title":"Check the database","text":"<p>As above, you should check the <code>triage</code>-created tables in your database to ensure the run with your new config has trained and tested all of the expected models. A couple of things to look out for:</p> <ul> <li> <p>In <code>triage</code>, a specification of a model algorithm, related hyperparameters, and set of features is referred to as a <code>model_group</code> while an instantiation of these parameters on particular set of data at a specific point in time is referred to as a <code>model</code>. As such, with the <code>quickstart</code> preset model grid, you should still have the same 3 records in <code>triage_metadata.model_groups</code> while you should have several new records in <code>triage_metadata.models</code> with different <code>train_end_time</code>s implied by your temporal config.</p> </li> <li> <p>Likewise, in <code>test_results.predictions</code> and <code>test_results.evaluations</code>, you will find an <code>as_of_date</code> column. In many cases, you will likely have a single <code>as_of_date</code> per model that lines up with the model's <code>train_end_time</code>, but in some situations, you may want to evaluate at several <code>as_of_dates</code> for each model. See the temporal crossvalidation deep dive for more details.</p> </li> </ul>"},{"location":"triage_project_workflow/#a-first-look-at-model-selection-with-audition","title":"A first look at model selection with <code>audition</code>","text":"<p>Now that we have models trained across several time periods, we can use <code>audition</code> to take a look at each <code>model_group</code>'s performance over time. While the <code>quickstart</code> models are quite simple and there isn't much meaningful model selection to do at this point, we can start to explore how model selection works in <code>triage</code>. A good place to begin is with the model selection primer.</p> <p>We generally recommend using <code>audition</code> interactively with as a <code>jupyter notebook</code>. If you don't already have <code>jupyter</code> installed, you can learn more about it here. Once you have a notebook server running, you can modify the <code>audition</code> tutorial notebook to take a look at the data from your current experiment. The <code>audition README</code> is also a good resource for options available with the tool.</p>"},{"location":"triage_project_workflow/#iteration-3-add-more-datafeatures-models-and-hyperparameters-and-evaluation-metrics-of-interest","title":"Iteration 3: Add more data/features, models and hyperparameters, and evaluation metrics of interest","text":"<p>After completing iteration 2, you should now have your cohort, label, and temporal configuration well-defined for your problem and you're ready to focus on features and model specifications.</p> <p>We've labeled this section <code>Iteration 3</code>, but in practice it's probably more like <code>Iterations 3-n</code> as you will likely want to do a bit of intermediate testing while adding new features and refine your model grid as you learn more about what does and doesn't seem to work well.</p>"},{"location":"triage_project_workflow/#define-some-additional-features","title":"Define some additional features","text":"<p>Generally speaking, the biggest determinant of the performance of many models is the quality of the underlying features, so you'll likely spend a considerable amount of time at this stage of the process. Here, you'll likely want to add additional features based on the data you've already prepared, but likely will discover that you want to structure or collect additional raw data as well where possible.</p> <p>The experiment configuration file provides a decent amount of flexibility for defining features, so we'll walk through some of the details here, however you may also want to refer to the relevant sections of the config README and example config file for more details.</p> <p>Features in <code>triage</code> are temporal aggregations</p> <p>Just as <code>triage</code> is built with temporal cross-validation in mind, features in <code>triage</code> reflect this inherent temporal nature as well. As such, all feature definitions need to be specified with an associated date reflecting when the information was known (which may or may not be the same as when an event actually happened) and a time frame before the modeling date over which to aggregate.</p> <p>This has two consequences which may feel unintuitive at first: - Even static features are handled in this way, so in practice, we tend to specify them as a <code>max</code> (or <code>min</code>) taken over identical values over all time.</p> <ul> <li> <p>Categorical features are also aggregated over time in this way, so in practice these are split into separate features for each value the categorical can take, each of which is expressed as a numerical value (either binary or real-valued, like a mean over time). As a result, these values will not necessarily be mutually exclusive --- that is, a given entity can have non-zero values for more than one feature derived from the same underlying categorical depending on their history of values for that feature.</p> </li> <li> <p>Unfortunately, <code>triage</code> has not yet implemented functionality for \"first value\" or \"most recent value\" feature aggregates, so you'll need to pre-calculate any features you want with this logic (though we do hope to add this ability).</p> </li> </ul> <p>Feature definitions are specified in the <code>feature_aggregations</code> section of the config file, under which you should provide a list of sets of related features, and each element in this list must contain several keys (see the example config file for a detailed example of what this looks like in practice):</p> <ul> <li><code>prefix</code>: a simple name to identify this set of related features - all of the features defined by this <code>feature_aggregations</code> entry will start with this prefix.</li> <li><code>from_obj</code>: this may be a table name or a SQL expression that provides the source data for these features.</li> <li><code>knowledge_date_column</code>: the date column specifying when information in the source data was known (e.g., available to be used for predictive modeling), which may differ from when the event ocurred.</li> <li><code>aggregates</code> and/or <code>categoricals</code>: lists used to define the specific features (you must specify at least one, but may include both). See below for more detail on each.</li> <li><code>intervals</code>: The time intervals (as a SQL interval, such a <code>'5 year'</code>, <code>'6 month'</code>, or <code>all</code> for all time) over which to aggregate features.<ul> <li>For instance, if you specified a count of the number of events under <code>aggregates</code> and <code>['5 year', '10 year', 'all']</code> as <code>intervals</code>, <code>triage</code> would create features for the number of events related to an entity in the last 5 years, 10 years, and since the <code>feature_start_time</code> (that is, three separate features)</li> </ul> </li> <li>You also need to provide rules for how to handle missing data, which can be provided either overall under <code>feature_aggregations</code> to apply to all features or on a feature-by-feature basis. It's worth reading through the Feature Generation README to learn about the available options here, including options for when missingness is meaningful (e.g., in a count) or there should be no missing data.</li> </ul> <p>When defining features derived from numerical data, you list them under the <code>aggregates</code> key in your feature config, and these should include keys for: - <code>quantity</code>: A column or SQL expression from the <code>from_obj</code> yielding a number that can be aggregated - <code>metrics</code>: What types of aggregation to do. Namely, these are postgres aggregation functions, such as <code>count</code>, <code>avg</code>, <code>sum</code>, <code>stddev</code>, <code>min</code>, <code>max</code>, etc. - (optional) <code>coltype</code>: can be used to control the type of column used in the generated features table, but generally is not necessary to define. - As noted above, imputation rules can be specified at this level as well.</p> <p>When defining features derived from categorical data, you list them under the <code>categoricals</code> key in your feature config, and these should include keys for: - <code>column</code>: The column containing the categorical information in the <code>from_obj</code> (note that this must be a column, not a SQL expression). May be any type of data, but the choice values specified must be compatible for equality comparisson in SQL. - <code>choices</code> or <code>choice_query</code>: Either a hand-coded list of <code>choices</code> (that is, categorical values) or a <code>choice_query</code> that returns these distinct values from the data.     - For categoricals with a very large number of possible unique values, you may want to limit the set of choices to a set of most frequently observed values.     - Values in the data but not in this set of choice values will simply yield <code>0</code>s for all of these choice-set values. - <code>metrics</code>: As above, the postgres aggregation functions used to aggregate values across the time intervals for the feature.     - If categorical values associated with an entity do not change over time, using <code>max</code> would give you a simple one-hot encoded categorical.     - If they are changing over time, <code>max</code> would give you something similar to a one-hot encoding, but note that the values would no longer be mutually-exclusive. - As noted above, imputation rules can be specified at this level as well.</p> <p>Much more detail about defining your features can be found in the example config file and associated README.</p>"},{"location":"triage_project_workflow/#expand-then-refine-your-model-grid","title":"Expand, then refine, your model grid","text":"<p>As you develop more features, you'll want to build out your modeling grid as well. Above, we've used the very sparse <code>quickstart</code> grid preset, but <code>triage</code> offers additional <code>model_grid_preset</code> options of varying size: - The <code>small</code> model grid includes a reasonably extensive set of logistic regression and decision tree classifiers as well as a single random forest specification. This grid can be a good option as you build and refine your features, but you'll likely want to try something more extensive once you have the rest of your config set. - The <code>medium</code> model grid is a good starting point for general modeling, including fairly extensive logistic regressions, decision trees, and random forest grids as well as a few ada boost and extra trees specification. - The <code>large</code> grid adds additional specifications for these modeling types, including some very large (10,000-estimator) random forest and extra trees classifiers, so can take a bit more time and computational resources to run.</p> <p>These preset grids should really serve as a starting point, and as you learn what seems to be working well in your use-case, you'll likely want to explore other specifications, which you can do by specifying your own <code>grid_config</code> in the triage config, which looks like:</p> <pre><code>grid_config:\n    'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression':\n        C: [0.00001,0.0001, 0.001, 0.01, 0.1, 10]\n        penalty: ['l2']\n\n    'sklearn.tree.DecisionTreeClassifier':\n        criterion: ['entropy']\n        max_depth: [null,2,5,10,50,100]\n        min_samples_split: [2, 10, 50]\n        min_samples_leaf: [0.01,0.05,0.10]\n</code></pre> <p>Here, each top-level key is the modeling package (this needs to be a classification algorithm with a <code>scikit-learn</code>-style interface, but need not come from <code>scikit-learn</code> specifically), and the keys listed under it are hyperparameters of the algorithm with a list of values to test. <code>triage</code> will run the grid of all possible combinations of these hyperparameter values. Note that if you specify both a <code>model_grid_preset</code> and <code>grid_config</code> at the same time, triage will combine the unique model specifications across the two sets (generally, this is most useful when adding project-specific commonsense baselines to the preset grid).</p> <p>Check out the example config file for more details on specifying your grid.</p>"},{"location":"triage_project_workflow/#specify-evaluation-metrics-you-care-about","title":"Specify evaluation metrics you care about","text":"<p>In the initial iterations, we simply used precision in the top 1% as the evaluation metric for our models, but this is likely not what you care about for you project! Under the <code>scoring</code> section of your config file, you should specify the metrics of interest:</p> <pre><code>scoring:\n    testing_metric_groups:\n        -\n          metrics: [precision@, recall@]\n          thresholds:\n            percentiles: [1,5,10]\n            top_n: [100, 250, 500]\n        -\n          metrics: [accuracy, roc_auc]\n\n\n    training_metric_groups:\n      -\n          metrics: [fpr@]\n          thresholds:\n            top_n: [100, 250, 500]\n</code></pre> <p>You can specify any number of evaluation metrics to be calculated for your models on either the training or test sets (the set of available metrics can be found here). For metrics that need to be calculated relative to a specific threshold in the score (e.g. precision), you must specify either <code>percentiles</code> or <code>top_n</code> (and can optionally provide both) at which to do the calculations.</p> <p>Additionally, you can have <code>triage</code> pre-calculate statistics about bias and disparities in your modeling results by specifying a <code>bias_audit_config</code> section, which should give details about the attributes of interest (e.g., race, age, sex) and thresholds at which to do the calculations. See the example config file and associated README for more details on setting it up.</p>"},{"location":"triage_project_workflow/#run-triage_2","title":"Run <code>triage</code>","text":"<ol> <li> <p>Check triage config <pre><code>triage experiment config.yaml --project-path '/project_directory' --validate-only\n</code></pre></p> </li> <li> <p>Run the pipeline <pre><code>triage experiment config.yaml --project-path '/project_directory'\n</code></pre></p> </li> </ol> <p>Alternatively, you can also import <code>triage</code> as a package in your python scrips and run it that way. Learn more about that option here.</p>"},{"location":"triage_project_workflow/#check-for-results-and-select-models","title":"Check for Results and Select Models","text":""},{"location":"triage_project_workflow/#check-the-database_1","title":"Check the database","text":"<p>As before, the first place to look to check on the results of your modeling run is in the database: - Even while the modeling is still running, you can check out <code>test_results.evaluations</code> to keep an eye on the progress of the run (join it to <code>triage_metadata.models</code> using <code>model_id</code> if you want to see information about the model specifications). - Once the run has finished, you should see many more models in <code>test_results.evaluations</code> reflecting the full model grid evaluated on each of the metrics you specified above. - Information on feature importances can be found in <code>train_results.feature_importances</code> (note the schema is <code>train_results</code> since these are calculated based on the training data).</p>"},{"location":"triage_project_workflow/#run-audition","title":"Run <code>audition</code>","text":"<p>Once you have a more comprehensive model run with a variety of features and modeling grid, <code>audition</code> can help you understand the performance of different specifications and further refine your models for future iterations. In a typical project, you'll likely run through the <code>audition</code> flow several times as you progressively improve your modeling configuration.</p> <p>When you finally settle on a configuration you're happy with, <code>audition</code> will also help you narrow your models down to a smaller set of well-performing options for futher analysis. Often, this might involve something like specifying a few different \"selection rules\" (e.g., best mean performance, recency-weighted performance, etc.) and exploring one or two of the best performing under each rule using <code>postmodeling</code>.</p> <p>More about using <code>audition</code>: - model selection primer. - <code>audition</code> tutorial notebook - <code>audition</code> README</p>"},{"location":"triage_project_workflow/#a-first-look-at-postmodeling","title":"A first look at <code>postmodeling</code>","text":"<p>Now that you've narrowed your grid down to a handful of model specification for a closer look, the <code>postmodeling</code> methods provided in <code>triage</code> will help you answer three avenues of investigation:</p> <ul> <li> <p>Dive deeper into what\u2019s going on with each of these models, such as:</p> <ul> <li>score and feature distributions</li> <li>feature importances</li> <li>performance characteristics, such as stack ranking, ROC curves, and precision-recall curves</li> </ul> </li> <li> <p>Debug and improve future models</p> <ul> <li>look for potential leakage of future information into your training set</li> <li>explore patterns in the model's errors</li> <li>identify hyperparameter values and features to focus on in subsequent iterations</li> </ul> </li> <li> <p>Decide how to proceed with deployment</p> <ul> <li>compare lists and important features across models</li> <li>help decide on either a single \"best\" model to deploy or a strategy that combines models</li> </ul> </li> </ul> <p>Like <code>audition</code>, our <code>postmodeling</code> tools are currently best used interactively with a <code>jupyter notebook</code>. You can read more about these tools in the <code>postmodeling</code> README and modify the example <code>postmodeling</code> notebook for your project.</p>"},{"location":"triage_project_workflow/#iteration-4-explore-additional-labelsoutcomes-feature-group-strategies-and-calculation-evaluation-metrics-on-subsets-of-entities-that-may-be-of-special-interest","title":"Iteration 4: Explore additional labels/outcomes, feature group strategies, and calculation evaluation metrics on subsets of entities that may be of special interest","text":"<p>Finally, in <code>Iteration 4</code>, you should consider exploring additional labels, <code>triage</code>'s tools for understanding feature contributions, and potentially look at evaluating your models on subsets of interest in your cohort.</p>"},{"location":"triage_project_workflow/#additional-labels","title":"Additional labels","text":"<p>In many projects, how you choose to define your outcome label can have a dramatic impact on which entities your models bring to the top, as well as disparities across protected groups. As such, we generally recommend exploring a number of options for your label definition in the course of a given project. For instance:</p> <ul> <li> <p>In a project to target health and safety inspections of apartment buildings, you might consider labels that look at the presence of any violation, the presence of at least X violations, violations of a certain type or severity, violations in a certain fraction of units, etc.</p> </li> <li> <p>In a recidivism prediction project, you might consider labels that focus on subsequent arrests for any reason or only related to new criminal activity; based on either arrests, bookings, or convictions; or related to certain types or severity of offense.</p> </li> <li> <p>In a health care project, you might consider re-hospitalizations over differ time frames, certain types of complications or severity of outcomes or prognoses.</p> </li> </ul> <p>Be sure to change the <code>name</code> parameter in your <code>label_config</code> with each version to ensure that <code>triage</code> recognizes that models built with different labels are distinct.</p>"},{"location":"triage_project_workflow/#feature-group-strategies","title":"Feature group strategies","text":"<p>If you want to get a better sense for the most important types of features in your models, you can specify a <code>feature_group_strategies</code> key in your configuration file, allowing you to run models that include subsets of your features (note that these are taken over your feature groups --- often the different <code>prefix</code> values you specified --- not the individual features).</p> <p>The strategies you can use are: <code>all</code>, <code>leave-one-out</code>, <code>leave-one-in</code>, <code>all-combinations</code>. You can specify a list of multiple strategies, for instance:</p> <pre><code>feature_group_strategies: ['all', 'leave-one-out']\n</code></pre> <p>If you had five feature groups, this would run a total of six strategies (one including all your feature groups, and five including all but one of them) for each specification in your model grid.</p> <p>Before using feature group stragies...</p> <p>Note that model runs specifying <code>feature_group_strategies</code> can become quite time and resource-intensive, especially using the <code>all-combinations</code> option.</p> <p>Before making use of this functionality, it's generally smart to narrow your modeling grid considerably to at most a handful of well-performing models and do some back-of-the-envelope calculations of how many variations <code>triage</code> will have to run.</p> <p>Learn more about feature groups and strategies in the config README.</p>"},{"location":"triage_project_workflow/#subsets","title":"Subsets","text":"<p>In some cases, you may be interested in your models' performance on subsets of the full cohort on which it is trained, such as certain demographics or individuals who meet a specific criteria of interest to your program (for instance, a certain level or history of need).</p> <p>Subsets are defined in the <code>scoring</code> section of the configuration file as a list of dictionaries specifying a <code>name</code> and <code>query</code> that identify the set of entities for each subset of interest using <code>{as_of_date}</code> as a placeholder for the modeling date.</p> <p>Here's a quick example:</p> <pre><code>scoring:\n\n    ...\n\n    subsets:\n        -\n            name: women\n            query: |\n                select distinct entity_id\n                from demographics\n                where d.gender = 'woman'\n                and demographic_date &lt; '{as_of_date}'::date\n        -\n            name: youts\n            query: |\n                select distinct entity_id\n                from demographics\n                where extract('year' from age({as_of_date}, d.birth_date)) &lt;= 18\n                and demographic_date &lt; '{as_of_date}'::date\n</code></pre> <p>When specify subsets, all of the model evaluation metrics will be calculated for each subset you define here, as well as the cohort overall. In the <code>test_results.evaluations</code> table, the <code>subset_hash</code> column will identify the subset for the evaluation (<code>NULL</code> values indicate evaluations on the entire cohort), and can be joined to <code>triage_metadata.subsets</code> to obtain the name and definition of the subset.</p> <p>Note that subsets are only used for the purposes of evaluation, while the model will still be trained and scored on the entire cohort described above.</p>"},{"location":"triage_project_workflow/#run-triage_3","title":"Run <code>triage</code>","text":"<ol> <li> <p>Check triage config <pre><code>triage experiment config.yaml --project-path '/project_directory' --validate-only\n</code></pre></p> </li> <li> <p>Run the pipeline <pre><code>triage experiment config.yaml --project-path '/project_directory'\n</code></pre></p> </li> </ol> <p>Alternatively, you can also import <code>triage</code> as a package in your python scrips and run it that way. Learn more about that option here.</p>"},{"location":"triage_project_workflow/#check-for-results-and-select-models_1","title":"Check for Results and Select Models","text":"<p>As described above, once your modeling run has completed, you can explore the results in the <code>triage</code>-generated tables in your database, perform model selection with <code>audition</code>, and dig deeper into your results with <code>postmodeling</code>:</p>"},{"location":"triage_project_workflow/#check-the-database_2","title":"Check the database","text":"<p>Look for results and associated information in: - <code>triage_metadata</code> - <code>train_results</code> - <code>test_results</code></p>"},{"location":"triage_project_workflow/#run-audition_1","title":"Run <code>audition</code>","text":"<p>More about using <code>audition</code>:</p> <ul> <li>model selection primer.</li> <li><code>audition</code> tutorial notebook</li> <li><code>audition</code> README</li> </ul>"},{"location":"triage_project_workflow/#run-postmodeling","title":"Run <code>postmodeling</code>","text":"<p>More about <code>postmodeling</code>:</p> <ul> <li><code>postmodeling</code> README</li> <li>example <code>postmodeling</code> notebook</li> </ul>"},{"location":"api/audition/","title":"API Reference","text":""},{"location":"api/audition/#audition-reference","title":"Audition Reference","text":"<p>Audition is the Triage model selection module. It simplifies the process of comparing multiple model_groups trained across time.</p> <p>Find user-focused documentation for Audition here</p> Page Auditioner The Auditioner class is the main entry point for Audition. Selection Rules The Audition selection rules implement a range of criteria for identifying best-performing models. Audition Config Users of the Triage CLI can specify settings for Audition in an Audition config file. Database Dependencies The database schema from which Audition reads model training results."},{"location":"api/audition/audition-config/","title":"Audition Configuration","text":""},{"location":"api/audition/audition-config/#audition-configuration","title":"Audition Configuration","text":"<p><code>audition.yaml</code> controls Audition, Triage's model selection module. An example <code>audition.yaml</code> can be found here</p>"},{"location":"api/audition/audition-config/#syntax","title":"Syntax","text":"<pre><code>model_groups:\n    query: 'string'\n\ntime_stamps:\n    query: 'string'\n\nfilter:\n    metric: 'string'\n    parameter: 'string'\n    max_from_best: 1.0\n    threshold_value: 0.0\n    distance_table: 'string'\n    models_table: 'models'\n\nrules:\n    - shared_parameters:\n        - \n            metric: 'string'\n            parameter': 'string'\n      selection_rules:\n        - \n            name: 'string'\n            n: 1\n</code></pre>"},{"location":"api/audition/audition-config/#parameters","title":"Parameters","text":"<ul> <li><code>model_groups</code> (dict):<ul> <li><code>query</code> (string): A sql query returning a list of model groups to be included in the first round of model selection</li> </ul> </li> <li><code>time_stamps</code> (dict):<ul> <li><code>query</code> (string): A sql query specifying the list of train end times over which the specified model groups are evaluated</li> </ul> </li> <li><code>filter</code> (dict):<ul> <li><code>metric</code> (string): Metric of interest, e.g. <code>precision@</code></li> <li><code>parameter</code> (string): Parameter to metric of interest, e.g. <code>50_abs</code></li> <li><code>max_from_best</code> (float): Model groups that perform this much worse than the best-performing model in at least one train period will be pruned.</li> <li><code>threshold_value</code> (float): Model groups that perform worse than this value in at least one train/test period will be pruned.</li> <li><code>distance_table</code> (string): Name of the table that will store model distance results (created by Triage if not found). </li> <li><code>models_table</code> (string): Name of the table that stores trained model groups to be evaluated.</li> <li><code>agg_type</code> (string): optional for aggregating metric values across multiple models for a given <code>model_group_id</code> and <code>train_end_time</code> combination (e.g., from different random seeds) -- <code>mean</code>, <code>best</code>, or <code>worst</code> (the default)</li> </ul> </li> <li><code>rules</code> (list): A list of selection rule groups<ul> <li>(dict):<ul> <li><code>shared_parameters</code> (list): A list of parameters shared by the selection rules in a group<ul> <li>(dict): A bundle of shared parameters<ul> <li><code>metric</code> (string): Metric of interest, e.g. <code>precision@</code>. Should match specification in <code>filter</code>.</li> <li><code>parameter</code> (string): Parameter to metric of interest, e.g. <code>50_abs</code>. Should match specification in <code>filter</code>.</li> </ul> </li> </ul> </li> <li><code>selection_rules</code> (list): A list of selection rules to be applied<ul> <li>(dict): Specification of a selection rule function, including its name and parameters<ul> <li><code>name</code> (string): The name of a selection rule function</li> <li>Pass other arguments required by the function here as key:value pairs. Optionally, pass multiple arguments in a list.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"api/audition/auditioner/","title":"Auditioner","text":"<p>The <code>Auditioner</code> class is the main entry point for the Audition module. Users pass its constructor a database connection, information about the model groups to be evaluated, and a specification for a filter to prune the worst-performing models.</p> <p>Other methods allow users to define more complex selection rules, list selected models, or plot results from the selection process.</p>"},{"location":"api/audition/auditioner/#triage.component.audition-attributes","title":"Attributes","text":""},{"location":"api/audition/auditioner/#triage.component.audition.logger","title":"<code>logger = verboselogs.VerboseLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition-classes","title":"Classes","text":""},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner","title":"<code>AuditionRunner</code>","text":"Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>class AuditionRunner:\n    def __init__(self, config_dict, db_engine, directory=None):\n        self.dir = directory\n        self.config = config_dict\n        self.db_engine = db_engine\n\n    def run(self):\n        pre_aud = PreAudition(self.db_engine)\n        model_group_ids = pre_aud.get_model_groups(self.config[\"model_groups\"][\"query\"])\n        query_end_times = self.config[\"time_stamps\"][\"query\"].format(\n            \", \".join(map(str, model_group_ids))\n        )\n        end_times = pre_aud.get_train_end_times(query=query_end_times)\n\n        aud = Auditioner(\n            db_engine=self.db_engine,\n            model_group_ids=model_group_ids,\n            train_end_times=end_times,\n            initial_metric_filters=[\n                {\n                    \"metric\": self.config[\"filter\"][\"metric\"],\n                    \"parameter\": self.config[\"filter\"][\"parameter\"],\n                    \"max_from_best\": self.config[\"filter\"][\"max_from_best\"],\n                    \"threshold_value\": self.config[\"filter\"][\"threshold_value\"],\n                }\n            ],\n            models_table=self.config[\"filter\"][\"models_table\"],\n            distance_table=self.config[\"filter\"][\"distance_table\"],\n            directory=self.dir,\n            agg_type=self.config[\"filter\"].get(\"agg_type\") or 'worst',\n        )\n\n        aud.plot_model_groups()\n        aud.register_selection_rule_grid(rule_grid=self.config[\"rules\"], plot=True)\n        aud.save_result_model_group_ids()\n\n        logger.debug(f\"Audition ran! Results are stored in {self.dir}.\")\n\n    def validate(self):\n        try:\n            logger.debug(\"Validate!\")\n        except Exception as err:\n            raise err\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner-attributes","title":"Attributes","text":""},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner.config","title":"<code>config = config_dict</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner.db_engine","title":"<code>db_engine = db_engine</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner.dir","title":"<code>dir = directory</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner-functions","title":"Functions","text":""},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner.__init__","title":"<code>__init__(config_dict, db_engine, directory=None)</code>","text":"Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def __init__(self, config_dict, db_engine, directory=None):\n    self.dir = directory\n    self.config = config_dict\n    self.db_engine = db_engine\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner.run","title":"<code>run()</code>","text":"Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def run(self):\n    pre_aud = PreAudition(self.db_engine)\n    model_group_ids = pre_aud.get_model_groups(self.config[\"model_groups\"][\"query\"])\n    query_end_times = self.config[\"time_stamps\"][\"query\"].format(\n        \", \".join(map(str, model_group_ids))\n    )\n    end_times = pre_aud.get_train_end_times(query=query_end_times)\n\n    aud = Auditioner(\n        db_engine=self.db_engine,\n        model_group_ids=model_group_ids,\n        train_end_times=end_times,\n        initial_metric_filters=[\n            {\n                \"metric\": self.config[\"filter\"][\"metric\"],\n                \"parameter\": self.config[\"filter\"][\"parameter\"],\n                \"max_from_best\": self.config[\"filter\"][\"max_from_best\"],\n                \"threshold_value\": self.config[\"filter\"][\"threshold_value\"],\n            }\n        ],\n        models_table=self.config[\"filter\"][\"models_table\"],\n        distance_table=self.config[\"filter\"][\"distance_table\"],\n        directory=self.dir,\n        agg_type=self.config[\"filter\"].get(\"agg_type\") or 'worst',\n    )\n\n    aud.plot_model_groups()\n    aud.register_selection_rule_grid(rule_grid=self.config[\"rules\"], plot=True)\n    aud.save_result_model_group_ids()\n\n    logger.debug(f\"Audition ran! Results are stored in {self.dir}.\")\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.AuditionRunner.validate","title":"<code>validate()</code>","text":"Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def validate(self):\n    try:\n        logger.debug(\"Validate!\")\n    except Exception as err:\n        raise err\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner","title":"<code>Auditioner</code>","text":"Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>class Auditioner:\n    def __init__(\n        self,\n        db_engine,\n        model_group_ids,\n        train_end_times,\n        initial_metric_filters,\n        models_table=None,\n        distance_table=None,\n        directory=None,\n        agg_type='worst',\n        baseline_model_group_ids=None,\n    ):\n        \"\"\"Filter model groups using a two-step process:\n\n        1. Broad thresholds to filter out truly bad models\n        2. A selection rule grid to find the best model groups over time\n            for each of a variety of methods\n\n        This is achieved by creating a 'best distance' table, which functions like a\n        denormalized 'model group/model/evaluations', storing for each\n        model group/train end time/metric/parameter:\n            1. the raw evaluation value,\n            2. the distance of that evaluation metric from the best model group at that train time,\n            3. and the distance of the metric from the best model group the *next* train time\n\n        Each of the steps is computed based on the data in this table, and an iterative process of\n            sending thresholding/selection configuration and viewing the results.\n\n        For step 1, the initial configuration is sent in this constructor\n            (as 'initial_metric_filters', format detailed below), future iterations of this\n            configuration are sent to 'update_metric_filters'.\n\n        For step 2, all configuration is sent to the object via 'register_selection_rule_grid',\n            and its format is detailed in that method's docstring\n\n        Args:\n            db_engine (sqlalchemy.engine): A database engine with access to a\n                results schema of a completed modeling run\n            model_group_ids (list): A large list of model groups to audition. No effort should\n                be needed to pick 'good' model groups, but they should all be groups that could\n                be used if they are found to perform well. They should also each have evaluations\n                for any train end times you wish to include in analysis\n            train_end_times (list): A list of train end times that all of the given model groups\n                contain evaluations for and that you want to be deemed important in the analysis\n            initial_metric_filters (list): A list of metrics to filter model\n                groups on, and how to filter them. Each entry should be a dict\n                of the format:\n\n                    {\n                        'metric': 'string',\n                        'parameter': 'string',\n                        'max_below_best': .5,\n                        'threshold_value': .5\n                     }\n\n                    metric (string): model evaluation metric, such as 'precision@'\n                    parameter (string): model evaluation metric parameter,\n                        such as '300_abs'\n                    max_below_best (float): The maximum value that the given metric\n                        can be below the best for a given train end time\n                    threshold_value (float): The minimum value that the given metric can be\n            models_table (string, optional): The name of the results schema\n                models table that you want to use. Will default to 'models',\n                which is also the default in triage.\n            distance_table (string, optional): The name of the 'best distance' table to use.\n                Will default to 'best_distance', but this can be sent if you want to avoid\n                clobbering the results from a prior analysis.\n            agg_type (string) Method for aggregating metric values (for instance, if there\n                are multiple models at a given train_end_time with different random seeds).\n                Can be: 'mean', 'best', or 'worst' (the default)\n            baseline_model_group_ids (list): An optional list of model groups for baseline \n                models which will be included on all plots without being subject to filtering \n                or included as candidate models from the selection process.\n        \"\"\"\n        self.metric_filters = initial_metric_filters\n        # sort the train end times so we can reliably pick off the last time later\n        self.train_end_times = sorted(train_end_times)\n        self.directory = directory\n        models_table = models_table or \"models\"\n        distance_table = distance_table or \"best_distance\"\n        self.distance_from_best_table = DistanceFromBestTable(\n            db_engine=db_engine,\n            models_table=models_table,\n            distance_table=distance_table,\n            agg_type=agg_type\n        )\n        self.best_distance_plotter = BestDistancePlotter(\n            self.distance_from_best_table, self.directory\n        )\n\n        if baseline_model_group_ids:\n            self.baseline_model_groups = model_groups_filter(\n                train_end_times=train_end_times,\n                initial_model_group_ids=baseline_model_group_ids,\n                models_table=models_table,\n                db_engine=db_engine,\n            )\n        else:\n            self.baseline_model_groups = set([])\n\n        self.first_pass_model_groups = model_groups_filter(\n            train_end_times=train_end_times,\n            initial_model_group_ids=model_group_ids,\n            models_table=models_table,\n            db_engine=db_engine,\n        )\n\n        self.model_group_thresholder = ModelGroupThresholder(\n            distance_from_best_table=self.distance_from_best_table,\n            train_end_times=train_end_times,\n            initial_model_group_ids=self.first_pass_model_groups,\n            initial_metric_filters=initial_metric_filters,\n        )\n        self.model_group_performance_plotter = ModelGroupPerformancePlotter(\n            self.distance_from_best_table, self.directory\n        )\n\n        self.selection_rule_picker = SelectionRulePicker(self.distance_from_best_table)\n        self.selection_rule_plotter = SelectionRulePlotter(\n            self.selection_rule_picker, self.directory\n        )\n        self.selection_rule_performance_plotter = SelectionRulePerformancePlotter(\n            self.selection_rule_picker, directory\n        )\n\n        # note we populate the distance from best table using both the\n        # baseline and candidate model groups\n        self.distance_from_best_table.create_and_populate(\n            self.first_pass_model_groups | self.baseline_model_groups, \n            self.train_end_times, \n            self.metrics\n        )\n        self.results_for_rule = {}\n\n    @property\n    def metrics(self):\n        return [\n            {\"metric\": f[\"metric\"], \"parameter\": f[\"parameter\"]}\n            for f in self.metric_filters\n        ]\n\n    @property\n    def thresholded_model_group_ids(self) -&gt; list:\n        \"\"\"The model group thresholder will have a varying list of model group ids\n        depending on its current thresholding rules, this is a reference to whatever\n        that current list is.\n\n        Returns:\n            list of model group ids allowed by the current metric threshold rules\n        \"\"\"\n        return self.model_group_thresholder.model_group_ids\n\n    @property\n    def average_regret_for_rules(self) -&gt; dict:\n        \"\"\"\n        Returns the average regret for each selection rule, over the specified list of train/test periods.\n\n        Returns:\n            A dict with a key-value pair for each selection rule and the average regret for that rule. Structure:\n\n                {'descriptive rule_name': .5}\n        \"\"\"\n        result = dict()\n        for k in self.results_for_rule.keys():\n            result[k] = (\n                self.results_for_rule[k]\n                .groupby(\"selection_rule\")[\"regret\"]\n                .mean()\n                .to_dict()\n            )\n        return result\n\n    @property\n    def selection_rule_model_group_ids(self) -&gt; dict:\n        \"\"\"\n        Calculate the current winners for each selection rule and the most recent date\n\n        Returns:\n            A dict with a key-value pair for each selection rule and the list of n\n            model_group_ids that it selected. Structure:\n\n                {'descriptive rule_name':[1,2,3]}\n        \"\"\"\n        logger.debug(\"Calculating selection rule picks for all rules\")\n        model_group_ids = dict()\n        thresholded_ids = self.thresholded_model_group_ids\n        for selection_rule in self.selection_rules:\n            logger.debug(\"Calculating selection rule picks for %s\", selection_rule)\n            model_group_ids[\n                selection_rule.descriptive_name\n            ] = self.selection_rule_picker.model_group_from_rule(\n                bound_selection_rule=selection_rule,\n                model_group_ids=thresholded_ids,\n                # evaluate the selection rules for the most recent\n                # time period and use those as candidate model groups\n                train_end_time=self.train_end_times[-1],\n            )\n            logger.debug(\n                \"For rule %s, model group %s was picked\",\n                selection_rule,\n                model_group_ids[selection_rule.descriptive_name],\n            )\n        return model_group_ids\n\n    def save_result_model_group_ids(self, fname=\"results_model_group_ids.json\"):\n        with open(os.path.join(self.directory, fname), \"w\") as f:\n            f.write(json.dumps(self.selection_rule_model_group_ids))\n\n    def plot_model_groups(self):\n        \"\"\"Display model group plots, one of the below for each configured metric.\n\n        1. A cumulative plot showing the effect of different worse-than-best\n        thresholds for the given metric across the thresholded model groups.\n\n        2. A performance-over-time plot showing the value for the given\n        metric over time for each thresholded model group\n        \"\"\"\n        logger.debug(\"Showing best distance plots for all metrics\")\n        thresholded_model_group_ids = self.thresholded_model_group_ids\n        if len(thresholded_model_group_ids) == 0:\n            logger.warning(\n                \"Zero model group ids found that passed configured thresholds. \"\n                \"Nothing to plot\"\n            )\n            return\n        self.best_distance_plotter.plot_all_best_dist(\n            self.metrics, \n            thresholded_model_group_ids | self.baseline_model_groups, \n            self.train_end_times\n        )\n        logger.debug(\"Showing model group performance plots for all metrics\")\n        self.model_group_performance_plotter.plot_all(\n            metric_filters=self.metric_filters,\n            model_group_ids=thresholded_model_group_ids | self.baseline_model_groups,\n            train_end_times=self.train_end_times,\n        )\n\n    def set_one_metric_filter(\n        self,\n        metric=\"precision@\",\n        parameter=\"100_abs\",\n        max_from_best=0.05,\n        threshold_value=0.1,\n    ):\n        \"\"\"Set one thresholding metric filter\n        If one wnats to update multiple filters, one should use `update_metric_filters()` instead.\n\n        Args:\n            metric (string): model evaluation metric such as 'precision@'\n            parameter (string): model evaluation parameter such as '100_abs'\n            max_from_best (string): The maximum value that the given metric can be below the best\n                for a given train end time\n            threshold_value (string): The thresold value that the given metric can be\n            plot (boolean, default True): Whether or not to also plot model group performance\n                and thresholding details at this time.\n        \"\"\"\n        new_filters = [\n            {\n                \"metric\": metric,\n                \"parameter\": parameter,\n                \"max_from_best\": max_from_best,\n                \"threshold_value\": threshold_value,\n            }\n        ]\n        self.update_metric_filters(new_filters)\n\n    def update_metric_filters(self, new_filters=None, plot=True):\n        \"\"\"Update the thresholding metric filters\n\n        Args:\n            new_filters (list): A list of metrics to filter model\n                groups on, and how to filter them. This is an identical format to\n                the list given to 'initial_metric_filters' in the constructor.\n                Each entry should be a dict with the keys:\ninitial_metric_filters\n                    metric (string) -- model evaluation metric, such as 'precision@'\n                    parameter (string) -- model evaluation metric parameter,\n                        such as '300_abs'\n                    max_below_best (float) The maximum value that the given metric\n                        can be below the best for a given train end time\n                    threshold_value (float) The threshold value that the given metric can be\n            plot (boolean, default True): Whether or not to also plot model group performance\n                and thresholding details at this time.\n        \"\"\"\n        logger.debug(\"Updating metric filters with new config %s\", new_filters)\n        self.model_group_thresholder.update_filters(new_filters)\n        if plot:\n            logger.debug(\"After config update, plotting model groups\")\n            self.plot_model_groups()\n\n    def plot_selection_rules(self):\n        \"\"\"Plot data about the configured selection rules. The three plots outlined below\n        are plotted for each metric.\n\n        We base a lot of this on the concept of the 'regret'.\n        The regret refers to the difference in performance between a model group\n        and the best model group for the next testing window if a selection rule is followed.\n\n        1. A distance-next-time plot, showing the fraction of models worse then a succession of\n            regret thresholds for each selection rule\n        2. A regret-over-time plot for each selection rule\n        3. A metric-over-time plot for each selection rule\n        \"\"\"\n        for metric_definition in self.metrics:\n            common_kwargs = dict(\n                bound_selection_rules=self.selection_rules,\n                regret_metric=metric_definition[\"metric\"],\n                regret_parameter=metric_definition[\"parameter\"],\n                model_group_ids=self.thresholded_model_group_ids,\n                train_end_times=self.train_end_times[:-1],\n                # We can't calculate regrets for the most recent train end time,\n                # so don't send that in. Assumes that the train_end_times\n                # are sorted in the constructor\n            )\n            self.selection_rule_plotter.plot_all_selection_rules(**common_kwargs)\n\n            df = self.selection_rule_performance_plotter.generate_plot_data(\n                **common_kwargs\n            )\n            self.selection_rule_performance_plotter.regret_plot_from_dataframe(\n                metric=metric_definition[\"metric\"],\n                parameter=metric_definition[\"parameter\"],\n                df=df,\n            )\n            self.selection_rule_performance_plotter.raw_next_time_plot_from_dataframe(\n                metric=metric_definition[\"metric\"],\n                parameter=metric_definition[\"parameter\"],\n                df=df,\n            )\n\n            key = metric_definition[\"metric\"] + metric_definition[\"parameter\"]\n            self.results_for_rule[key] = df\n\n    def register_selection_rule_grid(self, rule_grid, plot=True):\n        \"\"\"Register a grid of selection rules\n\n        Args:\n            rule_grid (list): Groups of selection rules that share parameters. See documentation below for schema.\n            plot: (boolean, defaults to True) Whether or not to plot the selection\n                rules at this time.\n\n        `rules_grid` is a list of dicts. Each dict, which defines a group, has two required keys:\n        `shared_parameters` and `selection_rules`.\n\n        `shared_parameters`: A list of dicts, each with a set of parameters that are taken\n        by all selection rules in this group.\n\n        For each of these shared parameter sets, the grid will create selection rules\n        combining the set with all possible selection rule/parameter combinations.\n\n        This can be used to quickly combine, say, a variety of rules that\n        all are concerned with precision at top 100 entities.\n\n        `selection_rules`: A list of dicts, each with:\n\n        - A 'name' attribute that matches a selection rule in audition.selection_rules\n        - Parameters and values taken by that selection rule. Values in list form are\n        all added to the grid. If the selection rule has no parameters, or the parameters are all covered\n        by the shared parameters in this group, none are needed here.\n\n        Each selection rule in the group must have all of its required parameters\n        covered by the shared parameters in its group and the parameters given to it.\n\n        Refer to [Selection Rules](../selection_rules/#selection-rules) for available selection rules\n        and their parameters.\n        The exceptions are the first two arguments to each selection rule,\n        'df' and 'train_end_time'.\n        These are contextual and thus provided internally by Audition.\n\n        Example:\n        ```\n        [{\n            'shared_parameters': [\n                    {'metric': 'precision@', 'parameter': '100_abs'},\n                    {'metric': 'recall@', 'parameter': '100_abs'},\n                ],\n                'selection_rules': [\n                    {'name': 'most_frequent_best_dist',\n                        'dist_from_best_case': [0.1, 0.2, 0.3]},\n                    {'name': 'best_current_value'}\n                ]\n        }]\n        ```\n        \"\"\"\n        self.selection_rules = make_selection_rule_grid(rule_grid)\n        if plot:\n            self.plot_selection_rules()\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner-attributes","title":"Attributes","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.average_regret_for_rules","title":"<code>average_regret_for_rules</code>  <code>property</code>","text":"<p>Returns the average regret for each selection rule, over the specified list of train/test periods.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with a key-value pair for each selection rule and the average regret for that rule. Structure:</p> <p>{'descriptive rule_name': .5}</p>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.baseline_model_groups","title":"<code>baseline_model_groups = model_groups_filter(train_end_times=train_end_times, initial_model_group_ids=baseline_model_group_ids, models_table=models_table, db_engine=db_engine)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.best_distance_plotter","title":"<code>best_distance_plotter = BestDistancePlotter(self.distance_from_best_table, self.directory)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.directory","title":"<code>directory = directory</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.distance_from_best_table","title":"<code>distance_from_best_table = DistanceFromBestTable(db_engine=db_engine, models_table=models_table, distance_table=distance_table, agg_type=agg_type)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.first_pass_model_groups","title":"<code>first_pass_model_groups = model_groups_filter(train_end_times=train_end_times, initial_model_group_ids=model_group_ids, models_table=models_table, db_engine=db_engine)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.metric_filters","title":"<code>metric_filters = initial_metric_filters</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.metrics","title":"<code>metrics</code>  <code>property</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.model_group_performance_plotter","title":"<code>model_group_performance_plotter = ModelGroupPerformancePlotter(self.distance_from_best_table, self.directory)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.model_group_thresholder","title":"<code>model_group_thresholder = ModelGroupThresholder(distance_from_best_table=self.distance_from_best_table, train_end_times=train_end_times, initial_model_group_ids=self.first_pass_model_groups, initial_metric_filters=initial_metric_filters)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.results_for_rule","title":"<code>results_for_rule = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.selection_rule_model_group_ids","title":"<code>selection_rule_model_group_ids</code>  <code>property</code>","text":"<p>Calculate the current winners for each selection rule and the most recent date</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with a key-value pair for each selection rule and the list of n</p> <code>dict</code> <p>model_group_ids that it selected. Structure:</p> <p>{'descriptive rule_name':[1,2,3]}</p>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.selection_rule_performance_plotter","title":"<code>selection_rule_performance_plotter = SelectionRulePerformancePlotter(self.selection_rule_picker, directory)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.selection_rule_picker","title":"<code>selection_rule_picker = SelectionRulePicker(self.distance_from_best_table)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.selection_rule_plotter","title":"<code>selection_rule_plotter = SelectionRulePlotter(self.selection_rule_picker, self.directory)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.thresholded_model_group_ids","title":"<code>thresholded_model_group_ids</code>  <code>property</code>","text":"<p>The model group thresholder will have a varying list of model group ids depending on its current thresholding rules, this is a reference to whatever that current list is.</p> <p>Returns:</p> Type Description <code>list</code> <p>list of model group ids allowed by the current metric threshold rules</p>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.train_end_times","title":"<code>train_end_times = sorted(train_end_times)</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner-functions","title":"Functions","text":""},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.__init__","title":"<code>__init__(db_engine, model_group_ids, train_end_times, initial_metric_filters, models_table=None, distance_table=None, directory=None, agg_type='worst', baseline_model_group_ids=None)</code>","text":"<p>Filter model groups using a two-step process:</p> <ol> <li>Broad thresholds to filter out truly bad models</li> <li>A selection rule grid to find the best model groups over time     for each of a variety of methods</li> </ol> <p>This is achieved by creating a 'best distance' table, which functions like a denormalized 'model group/model/evaluations', storing for each model group/train end time/metric/parameter:     1. the raw evaluation value,     2. the distance of that evaluation metric from the best model group at that train time,     3. and the distance of the metric from the best model group the next train time</p> <p>Each of the steps is computed based on the data in this table, and an iterative process of     sending thresholding/selection configuration and viewing the results.</p> <p>For step 1, the initial configuration is sent in this constructor     (as 'initial_metric_filters', format detailed below), future iterations of this     configuration are sent to 'update_metric_filters'.</p> <p>For step 2, all configuration is sent to the object via 'register_selection_rule_grid',     and its format is detailed in that method's docstring</p> <p>Parameters:</p> Name Type Description Default <code>db_engine</code> <code>engine</code> <p>A database engine with access to a results schema of a completed modeling run</p> required <code>model_group_ids</code> <code>list</code> <p>A large list of model groups to audition. No effort should be needed to pick 'good' model groups, but they should all be groups that could be used if they are found to perform well. They should also each have evaluations for any train end times you wish to include in analysis</p> required <code>train_end_times</code> <code>list</code> <p>A list of train end times that all of the given model groups contain evaluations for and that you want to be deemed important in the analysis</p> required <code>initial_metric_filters</code> <code>list</code> <p>A list of metrics to filter model groups on, and how to filter them. Each entry should be a dict of the format:</p> <pre><code>{\n    'metric': 'string',\n    'parameter': 'string',\n    'max_below_best': .5,\n    'threshold_value': .5\n }\n\nmetric (string): model evaluation metric, such as 'precision@'\nparameter (string): model evaluation metric parameter,\n    such as '300_abs'\nmax_below_best (float): The maximum value that the given metric\n    can be below the best for a given train end time\nthreshold_value (float): The minimum value that the given metric can be\n</code></pre> required <code>models_table</code> <code>string</code> <p>The name of the results schema models table that you want to use. Will default to 'models', which is also the default in triage.</p> <code>None</code> <code>distance_table</code> <code>string</code> <p>The name of the 'best distance' table to use. Will default to 'best_distance', but this can be sent if you want to avoid clobbering the results from a prior analysis.</p> <code>None</code> <code>baseline_model_group_ids</code> <code>list</code> <p>An optional list of model groups for baseline  models which will be included on all plots without being subject to filtering  or included as candidate models from the selection process.</p> <code>None</code> Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def __init__(\n    self,\n    db_engine,\n    model_group_ids,\n    train_end_times,\n    initial_metric_filters,\n    models_table=None,\n    distance_table=None,\n    directory=None,\n    agg_type='worst',\n    baseline_model_group_ids=None,\n):\n    \"\"\"Filter model groups using a two-step process:\n\n    1. Broad thresholds to filter out truly bad models\n    2. A selection rule grid to find the best model groups over time\n        for each of a variety of methods\n\n    This is achieved by creating a 'best distance' table, which functions like a\n    denormalized 'model group/model/evaluations', storing for each\n    model group/train end time/metric/parameter:\n        1. the raw evaluation value,\n        2. the distance of that evaluation metric from the best model group at that train time,\n        3. and the distance of the metric from the best model group the *next* train time\n\n    Each of the steps is computed based on the data in this table, and an iterative process of\n        sending thresholding/selection configuration and viewing the results.\n\n    For step 1, the initial configuration is sent in this constructor\n        (as 'initial_metric_filters', format detailed below), future iterations of this\n        configuration are sent to 'update_metric_filters'.\n\n    For step 2, all configuration is sent to the object via 'register_selection_rule_grid',\n        and its format is detailed in that method's docstring\n\n    Args:\n        db_engine (sqlalchemy.engine): A database engine with access to a\n            results schema of a completed modeling run\n        model_group_ids (list): A large list of model groups to audition. No effort should\n            be needed to pick 'good' model groups, but they should all be groups that could\n            be used if they are found to perform well. They should also each have evaluations\n            for any train end times you wish to include in analysis\n        train_end_times (list): A list of train end times that all of the given model groups\n            contain evaluations for and that you want to be deemed important in the analysis\n        initial_metric_filters (list): A list of metrics to filter model\n            groups on, and how to filter them. Each entry should be a dict\n            of the format:\n\n                {\n                    'metric': 'string',\n                    'parameter': 'string',\n                    'max_below_best': .5,\n                    'threshold_value': .5\n                 }\n\n                metric (string): model evaluation metric, such as 'precision@'\n                parameter (string): model evaluation metric parameter,\n                    such as '300_abs'\n                max_below_best (float): The maximum value that the given metric\n                    can be below the best for a given train end time\n                threshold_value (float): The minimum value that the given metric can be\n        models_table (string, optional): The name of the results schema\n            models table that you want to use. Will default to 'models',\n            which is also the default in triage.\n        distance_table (string, optional): The name of the 'best distance' table to use.\n            Will default to 'best_distance', but this can be sent if you want to avoid\n            clobbering the results from a prior analysis.\n        agg_type (string) Method for aggregating metric values (for instance, if there\n            are multiple models at a given train_end_time with different random seeds).\n            Can be: 'mean', 'best', or 'worst' (the default)\n        baseline_model_group_ids (list): An optional list of model groups for baseline \n            models which will be included on all plots without being subject to filtering \n            or included as candidate models from the selection process.\n    \"\"\"\n    self.metric_filters = initial_metric_filters\n    # sort the train end times so we can reliably pick off the last time later\n    self.train_end_times = sorted(train_end_times)\n    self.directory = directory\n    models_table = models_table or \"models\"\n    distance_table = distance_table or \"best_distance\"\n    self.distance_from_best_table = DistanceFromBestTable(\n        db_engine=db_engine,\n        models_table=models_table,\n        distance_table=distance_table,\n        agg_type=agg_type\n    )\n    self.best_distance_plotter = BestDistancePlotter(\n        self.distance_from_best_table, self.directory\n    )\n\n    if baseline_model_group_ids:\n        self.baseline_model_groups = model_groups_filter(\n            train_end_times=train_end_times,\n            initial_model_group_ids=baseline_model_group_ids,\n            models_table=models_table,\n            db_engine=db_engine,\n        )\n    else:\n        self.baseline_model_groups = set([])\n\n    self.first_pass_model_groups = model_groups_filter(\n        train_end_times=train_end_times,\n        initial_model_group_ids=model_group_ids,\n        models_table=models_table,\n        db_engine=db_engine,\n    )\n\n    self.model_group_thresholder = ModelGroupThresholder(\n        distance_from_best_table=self.distance_from_best_table,\n        train_end_times=train_end_times,\n        initial_model_group_ids=self.first_pass_model_groups,\n        initial_metric_filters=initial_metric_filters,\n    )\n    self.model_group_performance_plotter = ModelGroupPerformancePlotter(\n        self.distance_from_best_table, self.directory\n    )\n\n    self.selection_rule_picker = SelectionRulePicker(self.distance_from_best_table)\n    self.selection_rule_plotter = SelectionRulePlotter(\n        self.selection_rule_picker, self.directory\n    )\n    self.selection_rule_performance_plotter = SelectionRulePerformancePlotter(\n        self.selection_rule_picker, directory\n    )\n\n    # note we populate the distance from best table using both the\n    # baseline and candidate model groups\n    self.distance_from_best_table.create_and_populate(\n        self.first_pass_model_groups | self.baseline_model_groups, \n        self.train_end_times, \n        self.metrics\n    )\n    self.results_for_rule = {}\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.plot_model_groups","title":"<code>plot_model_groups()</code>","text":"<p>Display model group plots, one of the below for each configured metric.</p> <ol> <li> <p>A cumulative plot showing the effect of different worse-than-best thresholds for the given metric across the thresholded model groups.</p> </li> <li> <p>A performance-over-time plot showing the value for the given metric over time for each thresholded model group</p> </li> </ol> Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def plot_model_groups(self):\n    \"\"\"Display model group plots, one of the below for each configured metric.\n\n    1. A cumulative plot showing the effect of different worse-than-best\n    thresholds for the given metric across the thresholded model groups.\n\n    2. A performance-over-time plot showing the value for the given\n    metric over time for each thresholded model group\n    \"\"\"\n    logger.debug(\"Showing best distance plots for all metrics\")\n    thresholded_model_group_ids = self.thresholded_model_group_ids\n    if len(thresholded_model_group_ids) == 0:\n        logger.warning(\n            \"Zero model group ids found that passed configured thresholds. \"\n            \"Nothing to plot\"\n        )\n        return\n    self.best_distance_plotter.plot_all_best_dist(\n        self.metrics, \n        thresholded_model_group_ids | self.baseline_model_groups, \n        self.train_end_times\n    )\n    logger.debug(\"Showing model group performance plots for all metrics\")\n    self.model_group_performance_plotter.plot_all(\n        metric_filters=self.metric_filters,\n        model_group_ids=thresholded_model_group_ids | self.baseline_model_groups,\n        train_end_times=self.train_end_times,\n    )\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.plot_selection_rules","title":"<code>plot_selection_rules()</code>","text":"<p>Plot data about the configured selection rules. The three plots outlined below are plotted for each metric.</p> <p>We base a lot of this on the concept of the 'regret'. The regret refers to the difference in performance between a model group and the best model group for the next testing window if a selection rule is followed.</p> <ol> <li>A distance-next-time plot, showing the fraction of models worse then a succession of     regret thresholds for each selection rule</li> <li>A regret-over-time plot for each selection rule</li> <li>A metric-over-time plot for each selection rule</li> </ol> Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def plot_selection_rules(self):\n    \"\"\"Plot data about the configured selection rules. The three plots outlined below\n    are plotted for each metric.\n\n    We base a lot of this on the concept of the 'regret'.\n    The regret refers to the difference in performance between a model group\n    and the best model group for the next testing window if a selection rule is followed.\n\n    1. A distance-next-time plot, showing the fraction of models worse then a succession of\n        regret thresholds for each selection rule\n    2. A regret-over-time plot for each selection rule\n    3. A metric-over-time plot for each selection rule\n    \"\"\"\n    for metric_definition in self.metrics:\n        common_kwargs = dict(\n            bound_selection_rules=self.selection_rules,\n            regret_metric=metric_definition[\"metric\"],\n            regret_parameter=metric_definition[\"parameter\"],\n            model_group_ids=self.thresholded_model_group_ids,\n            train_end_times=self.train_end_times[:-1],\n            # We can't calculate regrets for the most recent train end time,\n            # so don't send that in. Assumes that the train_end_times\n            # are sorted in the constructor\n        )\n        self.selection_rule_plotter.plot_all_selection_rules(**common_kwargs)\n\n        df = self.selection_rule_performance_plotter.generate_plot_data(\n            **common_kwargs\n        )\n        self.selection_rule_performance_plotter.regret_plot_from_dataframe(\n            metric=metric_definition[\"metric\"],\n            parameter=metric_definition[\"parameter\"],\n            df=df,\n        )\n        self.selection_rule_performance_plotter.raw_next_time_plot_from_dataframe(\n            metric=metric_definition[\"metric\"],\n            parameter=metric_definition[\"parameter\"],\n            df=df,\n        )\n\n        key = metric_definition[\"metric\"] + metric_definition[\"parameter\"]\n        self.results_for_rule[key] = df\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.register_selection_rule_grid","title":"<code>register_selection_rule_grid(rule_grid, plot=True)</code>","text":"<p>Register a grid of selection rules</p> <p>Parameters:</p> Name Type Description Default <code>rule_grid</code> <code>list</code> <p>Groups of selection rules that share parameters. See documentation below for schema.</p> required <code>plot</code> <p>(boolean, defaults to True) Whether or not to plot the selection rules at this time.</p> <code>True</code> <p><code>rules_grid</code> is a list of dicts. Each dict, which defines a group, has two required keys: <code>shared_parameters</code> and <code>selection_rules</code>.</p> <p><code>shared_parameters</code>: A list of dicts, each with a set of parameters that are taken by all selection rules in this group.</p> <p>For each of these shared parameter sets, the grid will create selection rules combining the set with all possible selection rule/parameter combinations.</p> <p>This can be used to quickly combine, say, a variety of rules that all are concerned with precision at top 100 entities.</p> <p><code>selection_rules</code>: A list of dicts, each with:</p> <ul> <li>A 'name' attribute that matches a selection rule in audition.selection_rules</li> <li>Parameters and values taken by that selection rule. Values in list form are all added to the grid. If the selection rule has no parameters, or the parameters are all covered by the shared parameters in this group, none are needed here.</li> </ul> <p>Each selection rule in the group must have all of its required parameters covered by the shared parameters in its group and the parameters given to it.</p> <p>Refer to Selection Rules for available selection rules and their parameters. The exceptions are the first two arguments to each selection rule, 'df' and 'train_end_time'. These are contextual and thus provided internally by Audition.</p> <p>Example: <pre><code>[{\n    'shared_parameters': [\n            {'metric': 'precision@', 'parameter': '100_abs'},\n            {'metric': 'recall@', 'parameter': '100_abs'},\n        ],\n        'selection_rules': [\n            {'name': 'most_frequent_best_dist',\n                'dist_from_best_case': [0.1, 0.2, 0.3]},\n            {'name': 'best_current_value'}\n        ]\n}]\n</code></pre></p> Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def register_selection_rule_grid(self, rule_grid, plot=True):\n    \"\"\"Register a grid of selection rules\n\n    Args:\n        rule_grid (list): Groups of selection rules that share parameters. See documentation below for schema.\n        plot: (boolean, defaults to True) Whether or not to plot the selection\n            rules at this time.\n\n    `rules_grid` is a list of dicts. Each dict, which defines a group, has two required keys:\n    `shared_parameters` and `selection_rules`.\n\n    `shared_parameters`: A list of dicts, each with a set of parameters that are taken\n    by all selection rules in this group.\n\n    For each of these shared parameter sets, the grid will create selection rules\n    combining the set with all possible selection rule/parameter combinations.\n\n    This can be used to quickly combine, say, a variety of rules that\n    all are concerned with precision at top 100 entities.\n\n    `selection_rules`: A list of dicts, each with:\n\n    - A 'name' attribute that matches a selection rule in audition.selection_rules\n    - Parameters and values taken by that selection rule. Values in list form are\n    all added to the grid. If the selection rule has no parameters, or the parameters are all covered\n    by the shared parameters in this group, none are needed here.\n\n    Each selection rule in the group must have all of its required parameters\n    covered by the shared parameters in its group and the parameters given to it.\n\n    Refer to [Selection Rules](../selection_rules/#selection-rules) for available selection rules\n    and their parameters.\n    The exceptions are the first two arguments to each selection rule,\n    'df' and 'train_end_time'.\n    These are contextual and thus provided internally by Audition.\n\n    Example:\n    ```\n    [{\n        'shared_parameters': [\n                {'metric': 'precision@', 'parameter': '100_abs'},\n                {'metric': 'recall@', 'parameter': '100_abs'},\n            ],\n            'selection_rules': [\n                {'name': 'most_frequent_best_dist',\n                    'dist_from_best_case': [0.1, 0.2, 0.3]},\n                {'name': 'best_current_value'}\n            ]\n    }]\n    ```\n    \"\"\"\n    self.selection_rules = make_selection_rule_grid(rule_grid)\n    if plot:\n        self.plot_selection_rules()\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.save_result_model_group_ids","title":"<code>save_result_model_group_ids(fname='results_model_group_ids.json')</code>","text":"Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def save_result_model_group_ids(self, fname=\"results_model_group_ids.json\"):\n    with open(os.path.join(self.directory, fname), \"w\") as f:\n        f.write(json.dumps(self.selection_rule_model_group_ids))\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.set_one_metric_filter","title":"<code>set_one_metric_filter(metric='precision@', parameter='100_abs', max_from_best=0.05, threshold_value=0.1)</code>","text":"<p>Set one thresholding metric filter If one wnats to update multiple filters, one should use <code>update_metric_filters()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>string</code> <p>model evaluation metric such as 'precision@'</p> <code>'precision@'</code> <code>parameter</code> <code>string</code> <p>model evaluation parameter such as '100_abs'</p> <code>'100_abs'</code> <code>max_from_best</code> <code>string</code> <p>The maximum value that the given metric can be below the best for a given train end time</p> <code>0.05</code> <code>threshold_value</code> <code>string</code> <p>The thresold value that the given metric can be</p> <code>0.1</code> <code>plot</code> <code>boolean, default True</code> <p>Whether or not to also plot model group performance and thresholding details at this time.</p> required Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>def set_one_metric_filter(\n    self,\n    metric=\"precision@\",\n    parameter=\"100_abs\",\n    max_from_best=0.05,\n    threshold_value=0.1,\n):\n    \"\"\"Set one thresholding metric filter\n    If one wnats to update multiple filters, one should use `update_metric_filters()` instead.\n\n    Args:\n        metric (string): model evaluation metric such as 'precision@'\n        parameter (string): model evaluation parameter such as '100_abs'\n        max_from_best (string): The maximum value that the given metric can be below the best\n            for a given train end time\n        threshold_value (string): The thresold value that the given metric can be\n        plot (boolean, default True): Whether or not to also plot model group performance\n            and thresholding details at this time.\n    \"\"\"\n    new_filters = [\n        {\n            \"metric\": metric,\n            \"parameter\": parameter,\n            \"max_from_best\": max_from_best,\n            \"threshold_value\": threshold_value,\n        }\n    ]\n    self.update_metric_filters(new_filters)\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition.Auditioner.update_metric_filters","title":"<code>update_metric_filters(new_filters=None, plot=True)</code>","text":"<p>Update the thresholding metric filters</p> <pre><code>    Args:\n        new_filters (list): A list of metrics to filter model\n            groups on, and how to filter them. This is an identical format to\n            the list given to 'initial_metric_filters' in the constructor.\n            Each entry should be a dict with the keys:\n</code></pre> <p>initial_metric_filters                     metric (string) -- model evaluation metric, such as 'precision@'                     parameter (string) -- model evaluation metric parameter,                         such as '300_abs'                     max_below_best (float) The maximum value that the given metric                         can be below the best for a given train end time                     threshold_value (float) The threshold value that the given metric can be             plot (boolean, default True): Whether or not to also plot model group performance                 and thresholding details at this time.</p> Source code in <code>src/triage/component/audition/__init__.py</code> <pre><code>    def update_metric_filters(self, new_filters=None, plot=True):\n        \"\"\"Update the thresholding metric filters\n\n        Args:\n            new_filters (list): A list of metrics to filter model\n                groups on, and how to filter them. This is an identical format to\n                the list given to 'initial_metric_filters' in the constructor.\n                Each entry should be a dict with the keys:\ninitial_metric_filters\n                    metric (string) -- model evaluation metric, such as 'precision@'\n                    parameter (string) -- model evaluation metric parameter,\n                        such as '300_abs'\n                    max_below_best (float) The maximum value that the given metric\n                        can be below the best for a given train end time\n                    threshold_value (float) The threshold value that the given metric can be\n            plot (boolean, default True): Whether or not to also plot model group performance\n                and thresholding details at this time.\n        \"\"\"\n        logger.debug(\"Updating metric filters with new config %s\", new_filters)\n        self.model_group_thresholder.update_filters(new_filters)\n        if plot:\n            logger.debug(\"After config update, plotting model groups\")\n            self.plot_model_groups()\n</code></pre>"},{"location":"api/audition/auditioner/#triage.component.audition-functions","title":"Functions","text":""},{"location":"api/audition/database-dependencies/","title":"Database Dependencies","text":""},{"location":"api/audition/database-dependencies/#preparing-experiment-results","title":"Preparing Experiment Results","text":"<p>Note: If you are familiar with the DSaPP 'results schema', you can skip this section and head to the Using the Auditioner section.</p> <p>Audition expects to be able to read experiment metadata from a relational database. This includes information about models, what we call 'model groups', and evaluations. </p> <p>The full experiment schema used by DSaPP post-modeling tools such as Audition is defined in the results-schema repository, and is automatically populated after a triage.Experiment. However, even without using those tools, you can populate these tables for other experiments. </p> <p>Here's an overview of Audition's direct dependencies:</p> <ul> <li><code>triage_metadata.model_groups</code> - Everything unique about a classifier model, except for its train date. We define this as:<ul> <li><code>model_group_id</code> - An autogenerated integer surrogate key for the model group, used as a foreign key in other tables</li> <li><code>model_type</code> - The name of the class, e.g 'sklearn.ensemble.RandomForestClassifier'</li> <li><code>hyperparameters</code> - This is a dictionary (stored in the database as JSON) that describes how the class is configured (e.g. {'criterion': 'gini', 'min_samples_split': 2})</li> <li><code>feature_list</code> - A list of feature names (stored in the database as an array).</li> <li><code>model_config</code> - A catch-all for anything else you want to uniquely define as a model group, for instance different pieces of time config. Use of this column is not strictly necessary.</li> </ul> </li> <li><code>triage_metadata.models</code> - An instantiation of a model group at a specific training time. In production, our version of this table has quite a few column, but the only columns used by Audition are below. Note that the name of this table can be customized with the <code>models</code> parameter in the audition config or as an argument to <code>Auditioner</code>. If you reproduce this table, these three should be sufficient:<ul> <li><code>model_id</code> - An autogenerated integer surrogate key for the model, used as a foreign key in the 'evaluations' table.</li> <li><code>model_group_id</code> - A foreign key to the model_groups table.</li> <li><code>train_end_time</code> - A timestamp that signifies when this model was trained.</li> </ul> </li> <li><code>test_results.evaluations</code> - This is where metrics (such as precision, recall) and their values for specific models get stored.<ul> <li><code>model_id</code> - A foreign key to the models table</li> <li><code>evaluation_start_time</code> - The start of the time period from which testing data was taken from.</li> <li><code>evaluation_end_time</code> - The end of the time period from which testing data was taken from.</li> <li><code>metric</code> - The name of a metric, e.g. 'precision@' for thresholded precision. Audition needs some knowledge about what direction indicates 'better' results for this metric, so it wishes that the metric is one of the options in catwalk.ModelEvaluator.available_metrics. If you use a metric that is not available here, it will assume that greater is better.</li> <li><code>parameter</code> - A string that indicates any parameters for the metric. For instance, <code>100_abs</code> indicates top-100 entities, while <code>5_pct</code> indicates top 5 percentile of entities. These are commonly used for metrics like precision and recall to prioritize the evaluation of models to how they affect the actions likely to be taken as a result of the model.</li> <li><code>value</code> - A float that represents the value of the metric and parameters applied to the model and evaluation time.</li> </ul> </li> </ul>"},{"location":"api/audition/selection_rules/","title":"Selection Rules","text":""},{"location":"api/audition/selection_rules/#selection-rules","title":"Selection Rules","text":"<p>The Triage uses selection rules to compare the performance of trained model groups over time, and select a model group for future predictions. A selection rule tries to predict the best-performing model group in some train/test period, based on the historical performance of each model group on some metric.</p> <p>For example, a simple selection rule might predict that the best-performing model group during one train/test period will perform best in the following period.</p> <p>A selection rule can be evaluated by calculating its regret, or the difference between the performance of its selected model group and the best-performing model group in some period.</p> <p>Triage supports 8 model selection rules. Each is represented internally by one of the following functions:</p>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.BoundSelectionRule","title":"<code>BoundSelectionRule</code>","text":"<p>A selection rule bound with a set of arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict</code> <p>A set of keyword arguments, that should be sufficient to call the function when a dataframe and train_end_time is added</p> required <code>function_name</code> <code>string</code> <p>The name of a function in SELECTION_RULES</p> <code>None</code> <code>descriptive_name</code> <code>string</code> <p>A descriptive name, used in charts If none is given it will be automatically constructed</p> <code>None</code> <code>function</code> <code>function</code> <p>A function</p> <code>None</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>class BoundSelectionRule:\n    \"\"\"A selection rule bound with a set of arguments\n\n    Args:\n        args (dict): A set of keyword arguments, that should be sufficient\n            to call the function when a dataframe and train_end_time is added\n        function_name (string, optional): The name of a function in SELECTION_RULES\n        descriptive_name (string, optional): A descriptive name, used in charts\n            If none is given it will be automatically constructed\n        function (function, optional): A function\n    \"\"\"\n\n    def __init__(self, args, function_name=None, descriptive_name=None, function=None):\n        if not function_name and not function:\n            raise ValueError(\"Need either function_name or function\")\n\n        if not descriptive_name and not function_name:\n            raise ValueError(\"Need either descriptive_name or function_name\")\n\n        self.args = args\n        self.function_name = function_name\n        self._function = function\n        self._descriptive_name = descriptive_name\n\n    @property\n    def function(self):\n        if not self._function:\n            self._function = SELECTION_RULES[self.function_name]\n        return self._function\n\n    @property\n    def descriptive_name(self):\n        if not self._descriptive_name:\n            self._descriptive_name = self._build_descriptive_name()\n\n        return self._descriptive_name\n\n    def __str__(self):\n        return self.descriptive_name\n\n    def _build_descriptive_name(self):\n        \"\"\"Build a descriptive name for the bound selection rule\n\n        Constructed using the function name and arguments.\n        \"\"\"\n        argspec = inspect.getfullargspec(self.function)\n        args = [arg for arg in argspec.args if arg not in [\"df\", \"train_end_time\", \"n\"]]\n        return \"_\".join([self.function_name] + [str(self.args[key]) for key in args])\n\n    def pick(self, dataframe, train_end_time):\n        \"\"\"Run the selection rule for a given time on a dataframe\n\n        Args:\n            dataframe (pandas.DataFrame)\n            train_end_time (timestamp) Current train end time\n\n        Returns: (int) a model group id\n        \"\"\"\n        return self.function(dataframe, train_end_time, **(self.args))\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.BoundSelectionRule.pick","title":"<code>pick(dataframe, train_end_time)</code>","text":"<p>Run the selection rule for a given time on a dataframe</p> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def pick(self, dataframe, train_end_time):\n    \"\"\"Run the selection rule for a given time on a dataframe\n\n    Args:\n        dataframe (pandas.DataFrame)\n        train_end_time (timestamp) Current train end time\n\n    Returns: (int) a model group id\n    \"\"\"\n    return self.function(dataframe, train_end_time, **(self.args))\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.best_average_two_metrics","title":"<code>best_average_two_metrics(df, train_end_time, metric1, parameter1, metric2, parameter2, metric1_weight=0.5, n=1)</code>","text":"<p>Pick the model with the highest average combined value to date of two metrics weighted together using <code>metric1_weight</code></p> <p>Parameters:</p> Name Type Description Default <code>metric1_weight</code> <code>float</code> <p>relative weight of metric1, between 0 and 1</p> <code>0.5</code> <code>metric1</code> <code>string</code> <p>model evaluation metric, such as 'precision@'</p> required <code>parameter1</code> <code>string</code> <p>model evaluation metric parameter, such as '300_abs'</p> required <code>metric2</code> <code>string</code> <p>model evaluation metric, such as 'precision@'</p> required <code>parameter2</code> <code>string</code> <p>model evaluation metric parameter, such as '300_abs'</p> required <code>train_end_time</code> <code>Timestamp</code> <p>current train end time</p> required <code>df</code> <code>DataFrame</code> <p>dataframe containing the columns     model_group_id,     train_end_time,     metric,     parameter,     raw_value,     below_best</p> required <code>n</code> <code>int</code> <p>the number of model group ids to return</p> <code>1</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def best_average_two_metrics(\n    df,\n    train_end_time,\n    metric1,\n    parameter1,\n    metric2,\n    parameter2,\n    metric1_weight=0.5,\n    n=1,\n):\n    \"\"\"Pick the model with the highest average combined value to date\n    of two metrics weighted together using `metric1_weight`\n\n    Arguments:\n        metric1_weight (float): relative weight of metric1, between 0 and 1\n        metric1 (string): model evaluation metric, such as 'precision@'\n        parameter1 (string): model evaluation metric parameter,\n            such as '300_abs'\n        metric2 (string): model evaluation metric, such as 'precision@'\n        parameter2 (string): model evaluation metric parameter,\n            such as '300_abs'\n        train_end_time (Timestamp): current train end time\n        df (pandas.DataFrame): dataframe containing the columns\n                model_group_id,\n                train_end_time,\n                metric,\n                parameter,\n                raw_value,\n                below_best\n        n (int): the number of model group ids to return\n    Returns: (int) the model group id to select, with highest mean raw metric value\n    \"\"\"\n\n    if metric1_weight &lt; 0 or metric1_weight &gt; 1:\n        raise ValueError(\"Metric weight must be between 0 and 1\")\n\n    metric1_dir = greater_is_better(metric1)\n    metric2_dir = greater_is_better(metric2)\n    if metric1_dir != metric2_dir:\n        raise ValueError(\"Metric directionalities must be the same\")\n\n    met_df = df.loc[\n        ((df[\"metric\"] == metric1) &amp; (df[\"parameter\"] == parameter1))\n        | ((df[\"metric\"] == metric2) &amp; (df[\"parameter\"] == parameter2))\n    ]\n\n    met_df.loc[\n        (met_df[\"metric\"] == metric1) &amp; (met_df[\"parameter\"] == parameter1),\n        \"weighted_raw\",\n    ] = (\n        met_df.loc[\n            (met_df[\"metric\"] == metric1) &amp; (met_df[\"parameter\"] == parameter1),\n            \"raw_value\",\n        ]\n        * metric1_weight\n    )\n\n    met_df.loc[\n        (met_df[\"metric\"] == metric2) &amp; (met_df[\"parameter\"] == parameter2),\n        \"weighted_raw\",\n    ] = met_df.loc[\n        (met_df[\"metric\"] == metric2) &amp; (met_df[\"parameter\"] == parameter2), \"raw_value\"\n    ] * (\n        1.0 - metric1_weight\n    )\n\n    met_df_wt = met_df.groupby(\n        [\"model_group_id\", \"train_end_time\"], as_index=False\n    ).sum()\n\n    # sample(frac=1) to shuffle rows so we don't accidentally introduce bias in breaking ties\n    return _mg_best_avg_by(met_df_wt, \"weighted_raw\", metric1, n)\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.best_average_value","title":"<code>best_average_value(df, train_end_time, metric, parameter, n=1)</code>","text":"<p>Pick the model with the highest average metric value so far</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>string</code> <p>model evaluation metric, such as 'precision@'</p> required <code>parameter</code> <code>string</code> <p>model evaluation metric parameter, such as '300_abs'</p> required <code>train_end_time</code> <code>Timestamp</code> <p>current train end time</p> required <code>df</code> <code>DataFrame</code> <p>dataframe containing the columns     model_group_id,     train_end_time,     metric,     parameter,     raw_value,     dist_from_best_case</p> required <code>n</code> <code>int</code> <p>the number of model group ids to return</p> <code>1</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def best_average_value(df, train_end_time, metric, parameter, n=1):\n    \"\"\"Pick the model with the highest average metric value so far\n\n    Arguments:\n        metric (string): model evaluation metric, such as 'precision@'\n        parameter (string): model evaluation metric parameter,\n            such as '300_abs'\n        train_end_time (Timestamp): current train end time\n        df (pandas.DataFrame): dataframe containing the columns\n                model_group_id,\n                train_end_time,\n                metric,\n                parameter,\n                raw_value,\n                dist_from_best_case\n        n (int): the number of model group ids to return\n    Returns: (int) the model group id to select, with highest mean raw metric value\n    \"\"\"\n    met_df = df.loc[(df[\"metric\"] == metric) &amp; (df[\"parameter\"] == parameter)]\n    return _mg_best_avg_by(met_df, \"raw_value\", metric, n)\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.best_avg_recency_weight","title":"<code>best_avg_recency_weight(df, train_end_time, metric, parameter, curr_weight, decay_type, n=1)</code>","text":"<p>Pick the model with the highest average metric value so far, penalized for relative variance as:     avg_value - (stdev_penalty) * (stdev - min_stdev) where min_stdev is the minimum standard deviation of the metric across all model groups</p> <p>Parameters:</p> Name Type Description Default <code>decay_type</code> <code>string</code> <p>either 'linear' or 'exponential'; the shape of how the weights fall off between the current and first point</p> required <code>curr_weight</code> <code>float</code> <p>amount of weight to put on the most recent point, relative to the first point (e.g., a value of 5.0 would mean the current data is weighted 5 times as much as the first one)</p> required <code>metric</code> <code>string</code> <p>model evaluation metric, such as 'precision@'</p> required <code>parameter</code> <code>string</code> <p>model evaluation metric parameter, such as '300_abs'</p> required <code>train_end_time</code> <code>Timestamp</code> <p>current train end time</p> required <code>df</code> <code>DataFrame</code> <p>dataframe containing the columns     model_group_id,     train_end_time,     metric,     parameter,     raw_value,     below_best</p> required <code>n</code> <code>int</code> <p>the number of model group ids to return</p> <code>1</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def best_avg_recency_weight(\n    df, train_end_time, metric, parameter, curr_weight, decay_type, n=1\n):\n    \"\"\"Pick the model with the highest average metric value so far, penalized\n    for relative variance as:\n        avg_value - (stdev_penalty) * (stdev - min_stdev)\n    where min_stdev is the minimum standard deviation of the metric across all\n    model groups\n\n    Arguments:\n        decay_type (string): either 'linear' or 'exponential'; the shape of\n            how the weights fall off between the current and first point\n        curr_weight (float): amount of weight to put on the most recent point,\n            relative to the first point (e.g., a value of 5.0 would mean the\n            current data is weighted 5 times as much as the first one)\n        metric (string): model evaluation metric, such as 'precision@'\n        parameter (string): model evaluation metric parameter,\n            such as '300_abs'\n        train_end_time (Timestamp): current train end time\n        df (pandas.DataFrame): dataframe containing the columns\n                model_group_id,\n                train_end_time,\n                metric,\n                parameter,\n                raw_value,\n                below_best\n        n (int): the number of model group ids to return\n    Returns: (int) the model group id to select, with highest mean raw metric value\n    \"\"\"\n\n    # curr_weight is amount of weight to put on current point, relative to the first point\n    # (e.g., if the first point has a weight of 1.0)\n    # decay type is linear or exponetial\n\n    first_date = df[\"train_end_time\"].min()\n    df[\"days_out\"] = (df[\"train_end_time\"] - first_date).apply(lambda x: float(x.days))\n    tmax = df[\"days_out\"].max()\n\n    if tmax == 0:\n        # only one date (must be on first time point), so everything gets a weight of 1\n        df[\"weight\"] = 1.0\n    elif decay_type == \"linear\":\n        # weight = (curr_weight - 1.0) * (t/tmax) + 1.0\n        df[\"weight\"] = (curr_weight - 1.0) * (df[\"days_out\"] / tmax) + 1.0\n    elif decay_type == \"exponential\":\n        # weight = exp(ln(curr_weight)*t/tmax)\n        df[\"weight\"] = exp(log(curr_weight) * df[\"days_out\"] / tmax)\n    else:\n        raise ValueError(\"Must specify linear or exponential decay type\")\n\n    def wm(x):\n        return average(x, weights=df.loc[x.index, \"weight\"])\n\n    met_df = df.loc[(df[\"metric\"] == metric) &amp; (df[\"parameter\"] == parameter)]\n    if n == 1:\n        # sample(frac=1) to shuffle rows so we don't accidentally introduce bias in breaking ties\n        result = getattr(\n            met_df.groupby([\"model_group_id\"])\n            .aggregate({\"raw_value\": wm})\n            .sample(frac=1),\n            idxbest(metric),\n        )()\n        return result.tolist()\n    else:\n        met_df_grp = met_df.groupby([\"model_group_id\"]).aggregate({\"raw_value\": wm})\n        if greater_is_better(metric):\n\n            return met_df_grp[\"raw_value\"].nlargest(n).index.tolist()\n        else:\n            return met_df_grp[\"raw_value\"].nsmallest(n).index.tolist()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.best_avg_var_penalized","title":"<code>best_avg_var_penalized(df, train_end_time, metric, parameter, stdev_penalty, n=1)</code>","text":"<p>Pick the model with the highest  average metric value so far, placing less weight in older  results. You need to specify two parameters: the shape of how the  weight affects points (decay_type, linear or exponential) and the relative  weight of the most recent point (curr_weight).</p> <p>Parameters:</p> Name Type Description Default <code>stdev_penalty</code> <code>float</code> <p>penalty for instability</p> required <code>metric</code> <code>string</code> <p>model evaluation metric, such as 'precision@'</p> required <code>parameter</code> <code>string</code> <p>model evaluation metric parameter, such as '300_abs'</p> required <code>train_end_time</code> <code>Timestamp</code> <p>current train end time</p> required <code>df</code> <code>DataFrame</code> <p>dataframe containing the columns     model_group_id,     train_end_time,     metric,     parameter,     raw_value,     below_best</p> required <code>n</code> <code>int</code> <p>the number of model group ids to return</p> <code>1</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def best_avg_var_penalized(df, train_end_time, metric, parameter, stdev_penalty, n=1):\n    \"\"\"Pick the model with the highest\n     average metric value so far, placing less weight in older\n     results. You need to specify two parameters: the shape of how the\n     weight affects points (decay_type, linear or exponential) and the relative\n     weight of the most recent point (curr_weight).\n\n    Arguments:\n        stdev_penalty (float): penalty for instability\n        metric (string): model evaluation metric, such as 'precision@'\n        parameter (string): model evaluation metric parameter,\n            such as '300_abs'\n        train_end_time (Timestamp): current train end time\n        df (pandas.DataFrame): dataframe containing the columns\n                model_group_id,\n                train_end_time,\n                metric,\n                parameter,\n                raw_value,\n                below_best\n        n (int): the number of model group ids to return\n    Returns: (int) the model group id to select, with highest mean raw metric value\n    \"\"\"\n\n    # for metrics where smaller values are better, the penalty for instability should\n    # add to the mean, so introduce a factor of -1\n    stdev_penalty = stdev_penalty if greater_is_better(metric) else -1.0 * stdev_penalty\n\n    met_df = df.loc[(df[\"metric\"] == metric) &amp; (df[\"parameter\"] == parameter)]\n    met_df_grp = met_df.groupby([\"model_group_id\"]).aggregate(\n        {\"raw_value\": [\"mean\", \"std\"]}\n    )\n    met_df_grp.columns = met_df_grp.columns.droplevel(0)\n    met_df_grp.columns = [\"raw_avg\", \"raw_stdev\"]\n    if met_df_grp[\"raw_stdev\"].isnull().sum() == met_df_grp.shape[0]:\n        # variance will be undefined in first time window since we only have one obseravtion\n        # per model group\n        logger.debug(\n            \"Null metric variances for %s %s at %s; just using mean\",\n            metric,\n            parameter,\n            train_end_time,\n        )\n        return [getattr(met_df_grp[\"raw_avg\"].sample(frac=1), idxbest(metric))()]\n    elif met_df_grp[\"raw_stdev\"].isnull().sum() &gt; 0:\n        # the variances should be all null or no nulls, a mix shouldn't be possible\n        # since we should have the same number of observations for every model group\n        raise ValueError(\n            \"Mix of null and non-null metric variances for or {} {} at {}\".format(\n                metric, parameter, train_end_time\n            )\n        )\n\n    min_stdev = met_df_grp[\"raw_stdev\"].min()\n    met_df_grp[\"penalized_avg\"] = met_df_grp[\"raw_avg\"] - stdev_penalty * (\n        met_df_grp[\"raw_stdev\"] - min_stdev\n    )\n\n    if n == 1:\n        # sample(frac=1) to shuffle rows so we don't accidentally introduce bias in breaking ties\n        return [getattr(met_df_grp[\"penalized_avg\"].sample(frac=1), idxbest(metric))()]\n    else:\n        if greater_is_better(metric):\n            return met_df_grp[\"penalized_avg\"].nlargest(n).index.tolist()\n        else:\n            return met_df_grp[\"penalized_avg\"].nsmallest(n).index.tolist()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.best_current_value","title":"<code>best_current_value(df, train_end_time, metric, parameter, n=1)</code>","text":"<p>Pick the model group with the best current metric value</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>string</code> <p>model evaluation metric, such as 'precision@'</p> required <code>parameter</code> <code>string</code> <p>model evaluation metric parameter, such as '300_abs'</p> required <code>train_end_time</code> <code>Timestamp</code> <p>current train end time</p> required <code>df</code> <code>DataFrame</code> <p>dataframe containing the columns:     model_group_id,     train_end_time,     metric,     parameter,     raw_value,     dist_from_best_case</p> required <code>n</code> <code>int</code> <p>the number of model group ids to return</p> <code>1</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def best_current_value(df, train_end_time, metric, parameter, n=1):\n    \"\"\"Pick the model group with the best current metric value\n\n    Arguments:\n        metric (string): model evaluation metric, such as 'precision@'\n        parameter (string): model evaluation metric parameter,\n            such as '300_abs'\n        train_end_time (Timestamp): current train end time\n        df (pandas.DataFrame): dataframe containing the columns:\n                model_group_id,\n                train_end_time,\n                metric,\n                parameter,\n                raw_value,\n                dist_from_best_case\n        n (int): the number of model group ids to return\n    Returns: (int) the model group id to select, with highest current raw metric value\n    \"\"\"\n    curr_df = df.loc[\n        (df[\"train_end_time\"] == train_end_time)\n        &amp; (df[\"metric\"] == metric)\n        &amp; (df[\"parameter\"] == parameter)\n    ]\n    # sample(frac=1) to shuffle rows so we don't accidentally introduce bias in breaking ties\n    best_raw_value = getattr(curr_df[\"raw_value\"], best_in_series(metric))()\n    if n &lt;= 1:\n        return (\n            curr_df.loc[curr_df[\"raw_value\"] == best_raw_value, \"model_group_id\"]\n            .sample(frac=1)\n            .tolist()\n        )\n    else:\n        if greater_is_better(metric):\n            result = curr_df.nlargest(n, \"raw_value\")[\"model_group_id\"].tolist()\n            return result\n        else:\n            result = curr_df.nsmallest(n, \"raw_value\")[\"model_group_id\"].tolist()\n            return result\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.lowest_metric_variance","title":"<code>lowest_metric_variance(df, train_end_time, metric, parameter, n=1)</code>","text":"<p>Pick the model with the lowest metric variance so far</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>string</code> <p>model evaluation metric, such as 'precision@'</p> required <code>parameter</code> <code>string</code> <p>model evaluation metric parameter, such as '300_abs'</p> required <code>train_end_time</code> <code>Timestamp</code> <p>current train end time</p> required <code>df</code> <code>DataFrame</code> <p>dataframe containing the columns     model_group_id,     train_end_time,     metric,     parameter,     raw_value,     below_best</p> required <code>n</code> <code>int</code> <p>the number of model group ids to return</p> <code>1</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def lowest_metric_variance(df, train_end_time, metric, parameter, n=1):\n    \"\"\"Pick the model with the lowest metric variance so far\n\n    Arguments:\n        metric (string): model evaluation metric, such as 'precision@'\n        parameter (string): model evaluation metric parameter,\n            such as '300_abs'\n        train_end_time (Timestamp): current train end time\n        df (pandas.DataFrame): dataframe containing the columns\n                model_group_id,\n                train_end_time,\n                metric,\n                parameter,\n                raw_value,\n                below_best\n        n (int): the number of model group ids to return\n    Returns: (int) the model group id to select, with highest mean raw metric value\n    \"\"\"\n\n    met_df = (\n        df.loc[(df[\"metric\"] == metric) &amp; (df[\"parameter\"] == parameter)]\n        .groupby([\"model_group_id\"])[\"raw_value\"]\n        .std()\n    )\n\n    if met_df.isnull().sum() == met_df.shape[0]:\n        # variance will be undefined in first time window since we only have one obseravtion\n        # per model group\n        logger.debug(\n            \"Null metric variances for %s %s at %s; picking at random\",\n            metric,\n            parameter,\n            train_end_time,\n        )\n        return df[\"model_group_id\"].drop_duplicates().sample(n=n).tolist()\n    elif met_df.isnull().sum() &gt; 0:\n        # the variances should be all null or no nulls, a mix shouldn't be possible\n        # since we should have the same number of observations for every model group\n        raise ValueError(\n            \"Mix of null and non-null metric variances for or {} {} at {}\".format(\n                metric, parameter, train_end_time\n            )\n        )\n    if n == 1:\n        # sample(frac=1) to shuffle rows so we don't accidentally introduce bias in breaking ties\n        return [met_df.sample(frac=1).idxmin()]\n    else:\n        return met_df.nsmallest(n).index.tolist()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.most_frequent_best_dist","title":"<code>most_frequent_best_dist(df, train_end_time, metric, parameter, dist_from_best_case, n=1)</code>","text":"<p>Pick the model that is most frequently within <code>dist_from_best_case</code> from the best-performing model group across test sets so far</p> <p>Parameters:</p> Name Type Description Default <code>dist_from_best_case</code> <code>float</code> <p>distance from the best performing model</p> required <code>metric</code> <code>string</code> <p>model evaluation metric, such as 'precision@'</p> required <code>parameter</code> <code>string</code> <p>model evaluation metric parameter, such as '300_abs'</p> required <code>train_end_time</code> <code>Timestamp</code> <p>current train end time</p> required <code>df</code> <code>DataFrame</code> <p>dataframe containing the columns     model_group_id,     train_end_time,     metric,     parameter,     raw_value,     below_best</p> required <code>n</code> <code>int</code> <p>the number of model group ids to return</p> <code>1</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def most_frequent_best_dist(\n    df, train_end_time, metric, parameter, dist_from_best_case, n=1\n):\n    \"\"\"Pick the model that is most frequently within `dist_from_best_case` from the\n    best-performing model group across test sets so far\n\n    Arguments:\n        dist_from_best_case (float): distance from the best performing model\n        metric (string): model evaluation metric, such as 'precision@'\n        parameter (string): model evaluation metric parameter,\n            such as '300_abs'\n        train_end_time (Timestamp): current train end time\n        df (pandas.DataFrame): dataframe containing the columns\n                model_group_id,\n                train_end_time,\n                metric,\n                parameter,\n                raw_value,\n                below_best\n        n (int): the number of model group ids to return\n    Returns: (int) the model group id to select, with highest mean raw metric value\n    \"\"\"\n\n    met_df = df.loc[(df[\"metric\"] == metric) &amp; (df[\"parameter\"] == parameter)]\n    met_df[\"within_dist\"] = (df[\"dist_from_best_case\"] &lt;= dist_from_best_case).astype(\n        \"int\"\n    )\n    if n == 1:\n        # sample(frac=1) to shuffle rows so we don't accidentally introduce bias in breaking ties\n        return [\n            met_df.groupby([\"model_group_id\"])[\"within_dist\"]\n            .mean()\n            .sample(frac=1)\n            .idxmax()\n        ]\n    else:\n        return (\n            met_df.groupby([\"model_group_id\"])[\"within_dist\"]\n            .mean()\n            .nlargest(n)\n            .index.tolist()\n        )\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rules.random_model_group","title":"<code>random_model_group(df, train_end_time, n=1)</code>","text":"<p>Pick a random model group (as a baseline)</p> <p>Parameters:</p> Name Type Description Default <code>train_end_time</code> <code>Timestamp</code> <p>current train end time</p> required <code>df</code> <code>DataFrame</code> <p>dataframe containing the columns     model_group_id,     train_end_time,     metric,     parameter,     raw_value,     below_best</p> required <code>n</code> <code>int</code> <p>the number of model group ids to return</p> <code>1</code> Source code in <code>src/triage/component/audition/selection_rules.py</code> <pre><code>def random_model_group(df, train_end_time, n=1):\n    \"\"\"Pick a random model group (as a baseline)\n\n    Arguments:\n        train_end_time (Timestamp): current train end time\n        df (pandas.DataFrame): dataframe containing the columns\n                model_group_id,\n                train_end_time,\n                metric,\n                parameter,\n                raw_value,\n                below_best\n        n (int): the number of model group ids to return\n    Returns: (int) the model group id to select, with highest current raw metric value\n    \"\"\"\n    return df[\"model_group_id\"].drop_duplicates().sample(frac=1).tolist()[:n]\n</code></pre>"},{"location":"api/audition/selection_rules/#rulemakers","title":"RuleMakers","text":"<p>Triage uses <code>RuleMaker</code> classes to conveniently format the parameter grids accepted by <code>make_selection_rule_grid</code>. Each type of <code>RuleMaker</code> class holds methods that build parameter grids for a subset of the available selection rules.</p> <p>The arguments of each <code>add_rule_</code> method map to the arguments of the corresponding model selection function.</p>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.BaseRules","title":"<code>BaseRules</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>class BaseRules:\n    def __init__(self):\n        self._metric = None\n        self._parameter = None\n        self.shared_parameters = []\n        self.selection_rules = []\n\n    def _does_parameters_exist(self, params_dict):\n        return params_dict in self.shared_parameters\n\n    def _does_selection_rule_exisit(self, rule_dict):\n        return rule_dict in self.selection_rules\n\n    def _append(self, params_dict, rule_dict):\n        if not self._does_parameters_exist(params_dict):\n            self.shared_parameters.append(params_dict)\n        if not self._does_selection_rule_exisit(rule_dict):\n            self.selection_rules.append(rule_dict)\n\n    def create(self):\n        return [\n            {\n                \"shared_parameters\": self.shared_parameters,\n                \"selection_rules\": self.selection_rules,\n            }\n        ]\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.BaseRules.selection_rules","title":"<code>selection_rules = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.BaseRules.shared_parameters","title":"<code>shared_parameters = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.BaseRules.__init__","title":"<code>__init__()</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def __init__(self):\n    self._metric = None\n    self._parameter = None\n    self.shared_parameters = []\n    self.selection_rules = []\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.BaseRules.create","title":"<code>create()</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def create(self):\n    return [\n        {\n            \"shared_parameters\": self.shared_parameters,\n            \"selection_rules\": self.selection_rules,\n        }\n    ]\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.RandomGroupRuleMaker","title":"<code>RandomGroupRuleMaker</code>","text":"<p>               Bases: <code>BaseRules</code></p> <p>The <code>RandomGroupRuleMaker</code> class generates a rule that randomly selects <code>n</code> model groups for each train set.</p> <p>Unlike the other two RuleMaker classes, it generates its selection rule spec on <code>__init__</code></p> Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>class RandomGroupRuleMaker(BaseRules):\n\n    \"\"\"\n    The `RandomGroupRuleMaker` class generates a rule that randomly selects `n`\n    model groups for each train set.\n\n    Unlike the other two RuleMaker classes, it generates its selection rule spec\n    on `__init__`\n    \"\"\"\n\n    def __init__(self, n=1):\n        self.shared_parameters = [{}]\n        self.selection_rules = [{\"name\": \"random_model_group\", \"n\": n}]\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.RandomGroupRuleMaker.selection_rules","title":"<code>selection_rules = [{'name': 'random_model_group', 'n': n}]</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.RandomGroupRuleMaker.shared_parameters","title":"<code>shared_parameters = [{}]</code>  <code>instance-attribute</code>","text":""},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.RandomGroupRuleMaker.__init__","title":"<code>__init__(n=1)</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def __init__(self, n=1):\n    self.shared_parameters = [{}]\n    self.selection_rules = [{\"name\": \"random_model_group\", \"n\": n}]\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.SimpleRuleMaker","title":"<code>SimpleRuleMaker</code>","text":"<p>               Bases: <code>BaseRules</code></p> <p>Holds methods that generate parameter grids for selection rules that evaluate the performance of a model group in terms of a single metric. These include:</p> <ul> <li>best_current_value</li> <li>best_average_value</li> <li>lowest_metric_variance</li> <li>most_frequent_best_dist</li> <li>best_avg_var_penalized</li> <li>best_avg_recency_weight</li> </ul> Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>class SimpleRuleMaker(BaseRules):\n\n    \"\"\"\n    Holds methods that generate parameter grids for selection rules that\n    evaluate the performance of a model group in terms of a single metric.\n    These include:\n\n    - [best_current_value][triage.component.audition.selection_rules.best_current_value]\n    - [best_average_value][triage.component.audition.selection_rules.best_average_value]\n    - [lowest_metric_variance][triage.component.audition.selection_rules.lowest_metric_variance]\n    - [most_frequent_best_dist][triage.component.audition.selection_rules.most_frequent_best_dist]\n    - [best_avg_var_penalized][triage.component.audition.selection_rules.best_avg_var_penalized]\n    - [best_avg_recency_weight][triage.component.audition.selection_rules.best_avg_recency_weight]\n    \"\"\"\n\n    def add_rule_best_current_value(self, metric=None, parameter=None, n=1):\n        if metric is not None:\n            self._metric = metric\n        if parameter is not None:\n            self._parameter = parameter\n        params_dict = {\"metric\": self._metric, \"parameter\": self._parameter}\n        rule_dict = {\"name\": \"best_current_value\", \"n\": n}\n        self._append(params_dict, rule_dict)\n        return self.create()\n\n    def add_rule_best_average_value(self, metric=None, parameter=None, n=1):\n        if metric is not None:\n            self._metric = metric\n        if parameter is not None:\n            self._parameter = parameter\n        params_dict = {\"metric\": self._metric, \"parameter\": self._parameter}\n        rule_dict = {\"name\": \"best_average_value\", \"n\": n}\n        self._append(params_dict, rule_dict)\n        return self.create()\n\n    def add_rule_lowest_metric_variance(self, metric=None, parameter=None, n=1):\n        if metric is not None:\n            self._metric = metric\n        if parameter is not None:\n            self._parameter = parameter\n        params_dict = {\"metric\": self._metric, \"parameter\": self._parameter}\n        rule_dict = {\"name\": \"lowest_metric_variance\", \"n\": n}\n        self._append(params_dict, rule_dict)\n        return self.create()\n\n    def add_rule_most_frequent_best_dist(\n        self,\n        metric=None,\n        parameter=None,\n        n=1,\n        dist_from_best_case=[0.01, 0.05, 0.1, 0.15],\n    ):\n        if metric is not None:\n            self._metric = metric\n        if parameter is not None:\n            self._parameter = parameter\n        params_dict = {\"metric\": self._metric, \"parameter\": self._parameter}\n        rule_dict = {\n            \"name\": \"most_frequent_best_dist\",\n            \"dist_from_best_case\": dist_from_best_case,\n            \"n\": n,\n        }\n        self._append(params_dict, rule_dict)\n        return self.create()\n\n    def add_rule_best_avg_recency_weight(\n        self,\n        metric=None,\n        parameter=None,\n        n=1,\n        curr_weight=[1.5, 2.0, 5.0],\n        decay_type=[\"linear\"],\n    ):\n        if metric is not None:\n            self._metric = metric\n        if parameter is not None:\n            self._parameter = parameter\n        params_dict = {\"metric\": metric, \"parameter\": parameter}\n        rule_dict = {\n            \"name\": \"best_avg_recency_weight\",\n            \"curr_weight\": curr_weight,\n            \"decay_type\": decay_type,\n            \"n\": n,\n        }\n        self._append(params_dict, rule_dict)\n        return self.create()\n\n    def add_rule_best_avg_var_penalized(\n        self, metric=None, parameter=None, stdev_penalty=0.5, n=1\n    ):\n        if metric is not None:\n            self._metric = metric\n        if parameter is not None:\n            self._parameter = parameter\n        params_dict = {\"metric\": metric, \"parameter\": parameter}\n        rule_dict = {\n            \"name\": \"best_avg_var_penalized\",\n            \"stdev_penalty\": stdev_penalty,\n            \"n\": n,\n        }\n        self._append(params_dict, rule_dict)\n        return self.create()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.SimpleRuleMaker.add_rule_best_average_value","title":"<code>add_rule_best_average_value(metric=None, parameter=None, n=1)</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def add_rule_best_average_value(self, metric=None, parameter=None, n=1):\n    if metric is not None:\n        self._metric = metric\n    if parameter is not None:\n        self._parameter = parameter\n    params_dict = {\"metric\": self._metric, \"parameter\": self._parameter}\n    rule_dict = {\"name\": \"best_average_value\", \"n\": n}\n    self._append(params_dict, rule_dict)\n    return self.create()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.SimpleRuleMaker.add_rule_best_avg_recency_weight","title":"<code>add_rule_best_avg_recency_weight(metric=None, parameter=None, n=1, curr_weight=[1.5, 2.0, 5.0], decay_type=['linear'])</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def add_rule_best_avg_recency_weight(\n    self,\n    metric=None,\n    parameter=None,\n    n=1,\n    curr_weight=[1.5, 2.0, 5.0],\n    decay_type=[\"linear\"],\n):\n    if metric is not None:\n        self._metric = metric\n    if parameter is not None:\n        self._parameter = parameter\n    params_dict = {\"metric\": metric, \"parameter\": parameter}\n    rule_dict = {\n        \"name\": \"best_avg_recency_weight\",\n        \"curr_weight\": curr_weight,\n        \"decay_type\": decay_type,\n        \"n\": n,\n    }\n    self._append(params_dict, rule_dict)\n    return self.create()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.SimpleRuleMaker.add_rule_best_avg_var_penalized","title":"<code>add_rule_best_avg_var_penalized(metric=None, parameter=None, stdev_penalty=0.5, n=1)</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def add_rule_best_avg_var_penalized(\n    self, metric=None, parameter=None, stdev_penalty=0.5, n=1\n):\n    if metric is not None:\n        self._metric = metric\n    if parameter is not None:\n        self._parameter = parameter\n    params_dict = {\"metric\": metric, \"parameter\": parameter}\n    rule_dict = {\n        \"name\": \"best_avg_var_penalized\",\n        \"stdev_penalty\": stdev_penalty,\n        \"n\": n,\n    }\n    self._append(params_dict, rule_dict)\n    return self.create()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.SimpleRuleMaker.add_rule_best_current_value","title":"<code>add_rule_best_current_value(metric=None, parameter=None, n=1)</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def add_rule_best_current_value(self, metric=None, parameter=None, n=1):\n    if metric is not None:\n        self._metric = metric\n    if parameter is not None:\n        self._parameter = parameter\n    params_dict = {\"metric\": self._metric, \"parameter\": self._parameter}\n    rule_dict = {\"name\": \"best_current_value\", \"n\": n}\n    self._append(params_dict, rule_dict)\n    return self.create()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.SimpleRuleMaker.add_rule_lowest_metric_variance","title":"<code>add_rule_lowest_metric_variance(metric=None, parameter=None, n=1)</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def add_rule_lowest_metric_variance(self, metric=None, parameter=None, n=1):\n    if metric is not None:\n        self._metric = metric\n    if parameter is not None:\n        self._parameter = parameter\n    params_dict = {\"metric\": self._metric, \"parameter\": self._parameter}\n    rule_dict = {\"name\": \"lowest_metric_variance\", \"n\": n}\n    self._append(params_dict, rule_dict)\n    return self.create()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.SimpleRuleMaker.add_rule_most_frequent_best_dist","title":"<code>add_rule_most_frequent_best_dist(metric=None, parameter=None, n=1, dist_from_best_case=[0.01, 0.05, 0.1, 0.15])</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def add_rule_most_frequent_best_dist(\n    self,\n    metric=None,\n    parameter=None,\n    n=1,\n    dist_from_best_case=[0.01, 0.05, 0.1, 0.15],\n):\n    if metric is not None:\n        self._metric = metric\n    if parameter is not None:\n        self._parameter = parameter\n    params_dict = {\"metric\": self._metric, \"parameter\": self._parameter}\n    rule_dict = {\n        \"name\": \"most_frequent_best_dist\",\n        \"dist_from_best_case\": dist_from_best_case,\n        \"n\": n,\n    }\n    self._append(params_dict, rule_dict)\n    return self.create()\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.TwoMetricsRuleMaker","title":"<code>TwoMetricsRuleMaker</code>","text":"<p>               Bases: <code>BaseRules</code></p> <p>The <code>TwoMetricsRuleMaker</code> class allows for the specification of rules that  evaluate a model group's performance in terms of two metrics. It currently  supports one rule:</p> <ul> <li>best_average_two_metrics</li> </ul> Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>class TwoMetricsRuleMaker(BaseRules):\n\n    \"\"\"\n    The `TwoMetricsRuleMaker` class allows for the specification of rules that \n    evaluate a model group's performance in terms of two metrics. It currently\n     supports one rule:\n\n     - [best_average_two_metrics][triage.component.audition.selection_rules.best_average_two_metrics]\n\n    \"\"\"\n\n    def add_rule_best_average_two_metrics(\n        self,\n        metric1=\"precision@\",\n        parameter1=\"100_abs\",\n        metric2=\"recall@\",\n        parameter2=\"300_abs\",\n        metric1_weight=[0.5],\n        n=1,\n    ):\n        params_dict = {\"metric1\": metric1, \"parameter1\": parameter1}\n        rule_dict = {\n            \"name\": \"best_average_two_metrics\",\n            \"metric1_weight\": metric1_weight,\n            \"metric2\": [metric2],\n            \"parameter2\": [parameter2],\n            \"n\": n,\n        }\n        self._append(params_dict, rule_dict)\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.TwoMetricsRuleMaker.add_rule_best_average_two_metrics","title":"<code>add_rule_best_average_two_metrics(metric1='precision@', parameter1='100_abs', metric2='recall@', parameter2='300_abs', metric1_weight=[0.5], n=1)</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def add_rule_best_average_two_metrics(\n    self,\n    metric1=\"precision@\",\n    parameter1=\"100_abs\",\n    metric2=\"recall@\",\n    parameter2=\"300_abs\",\n    metric1_weight=[0.5],\n    n=1,\n):\n    params_dict = {\"metric1\": metric1, \"parameter1\": parameter1}\n    rule_dict = {\n        \"name\": \"best_average_two_metrics\",\n        \"metric1_weight\": metric1_weight,\n        \"metric2\": [metric2],\n        \"parameter2\": [parameter2],\n        \"n\": n,\n    }\n    self._append(params_dict, rule_dict)\n</code></pre>"},{"location":"api/audition/selection_rules/#triage.component.audition.rules_maker.create_selection_grid","title":"<code>create_selection_grid(*args)</code>","text":"Source code in <code>src/triage/component/audition/rules_maker.py</code> <pre><code>def create_selection_grid(*args):\n    return list(map(lambda r: r.create()[0], args))\n</code></pre>"},{"location":"api/audition/selection_rules/#selection-grid","title":"Selection Grid","text":""},{"location":"api/audition/selection_rules/#triage.component.audition.selection_rule_grid.make_selection_rule_grid","title":"<code>make_selection_rule_grid(rule_groups)</code>","text":"<p>Convert a compact selection rule group representation to a     list of bound selection rules.</p> <p>Parameters:</p> Name Type Description Default <code>rule_groups</code> <code>list</code> <p>List of dicts used to specify selection rule grid. </p> required <p>Most users will want to use rulemaker objects to generate their <code>rule_group</code> specifications.</p> <p>An example rule_groups specification:</p> <p><pre><code>[{\n        'shared_parameters': [\n                {'metric': 'precision@', 'parameter': '100_abs'},\n                {'metric': 'recall@', 'parameter': '100_abs'},\n            ],\n            'selection_rules': [\n                {'name': 'most_frequent_best_dist', 'dist_from_best_case': [0.1, 0.2, 0.3]},\n                {'name': 'best_current_value'}\n            ]\n    }, {\n        'shared_parameters': [\n            {'metric1': 'precision@', 'parameter1': '100_abs'},\n        ],\n        'selection_rules': [\n            {\n                'name': 'best_average_two_metrics',\n                'metric2': ['recall@'],\n                'parameter2': ['100_abs'],\n                'metric1_weight': [0.4, 0.5, 0.6]\n            },\n        ]\n    }]\n</code></pre> Returns:     list: list of audition.selection_rules.BoundSelectionRule objects</p> Source code in <code>src/triage/component/audition/selection_rule_grid.py</code> <pre><code>def make_selection_rule_grid(rule_groups):\n    \"\"\"Convert a compact selection rule group representation to a\n        list of bound selection rules.\n\n    Arguments:\n        rule_groups (list): List of dicts used to specify selection rule grid. \n\n    Most users will want to use [rulemaker objects](#rulemakers)\n    to generate their `rule_group` specifications.\n\n    An example rule_groups specification:\n\n    ```\n    [{\n            'shared_parameters': [\n                    {'metric': 'precision@', 'parameter': '100_abs'},\n                    {'metric': 'recall@', 'parameter': '100_abs'},\n                ],\n                'selection_rules': [\n                    {'name': 'most_frequent_best_dist', 'dist_from_best_case': [0.1, 0.2, 0.3]},\n                    {'name': 'best_current_value'}\n                ]\n        }, {\n            'shared_parameters': [\n                {'metric1': 'precision@', 'parameter1': '100_abs'},\n            ],\n            'selection_rules': [\n                {\n                    'name': 'best_average_two_metrics',\n                    'metric2': ['recall@'],\n                    'parameter2': ['100_abs'],\n                    'metric1_weight': [0.4, 0.5, 0.6]\n                },\n            ]\n        }]\n    ```\n    Returns:\n        list: list of audition.selection_rules.BoundSelectionRule objects\"\"\"\n\n\n    rules = []\n    logger.debug(\"Expanding selection rule groups into full grid\")\n    for rule_group in rule_groups:\n        logger.debug(\"Expanding rule group %s\", rule_group)\n        for shared_param_set, selection_rule in product(\n            rule_group[\"shared_parameters\"], rule_group[\"selection_rules\"]\n        ):\n            logger.debug(\n                \"Expanding shared param set %s and selection rules %s\",\n                shared_param_set,\n                selection_rule,\n            )\n            new_rules = _bound_rules_from(shared_param_set, selection_rule)\n            logger.debug(\"Found %s new rules\", len(new_rules))\n            rules += new_rules\n    logger.debug(\n        \"Found %s total selection rules. Full list: %s\",\n        len(rules),\n        [rule.descriptive_name for rule in rules],\n    )\n    return rules\n</code></pre>"},{"location":"api/timechop/","title":"Index","text":""},{"location":"api/timechop/#timechop-reference","title":"Timechop Reference","text":"<p>Timechop handles temporal logic in the Triage Experiment pipeline.</p> Page Timechop The Timechop class is the main entry point for Timechop. Plotting Tools for visualizing Timechop"},{"location":"api/timechop/plotting/","title":"Plotting","text":"<p>Visualize time chops of a given Timechop object using matplotlib</p> <p>Parameters:</p> Name Type Description Default <code>chopper</code> <code>Timechop</code> <p>A fully-configured Timechop object</p> required <code>show_as_of_times</code> <code>bool</code> <p>Whether or not to draw horizontal lines for as-of-times</p> <code>True</code> <code>show_boundaries</code> <code>bool</code> <p>Whether or not to show a rectangle around matrices and dashed lines around feature/label boundaries</p> <code>True</code> <code>save_target</code> <code>str or file-like object</code> <p>A save target for matplotlib to save the figure to. Defaults to None, which won't save anything</p> <code>None</code> Source code in <code>src/triage/component/timechop/plotting.py</code> <pre><code>def visualize_chops(chopper, show_as_of_times=True, show_boundaries=True, save_target=None):\n    \"\"\"Visualize time chops of a given Timechop object using matplotlib\n\n    Args:\n        chopper (triage.component.timechop.Timechop): A fully-configured Timechop object\n        show_as_of_times (bool): Whether or not to draw horizontal lines\n            for as-of-times\n        show_boundaries (bool): Whether or not to show a rectangle around matrices\n            and dashed lines around feature/label boundaries\n        save_target (str or file-like object): A save target for matplotlib to save\n            the figure to. Defaults to None, which won't save anything\n    \"\"\"\n    chops = chopper.chop_time()\n\n    chops.reverse()\n\n    fig, ax = plt.subplots(nrows=len(chops), sharex=True, sharey=True, squeeze=False, figsize=FIG_SIZE)\n\n    for idx, chop in enumerate(chops):\n        train_as_of_times = chop[\"train_matrix\"][\"as_of_times\"]\n        test_as_of_times = chop[\"test_matrices\"][0][\"as_of_times\"]\n\n        test_label_timespan = chop[\"test_matrices\"][0][\"test_label_timespan\"]\n        training_label_timespan = chop[\"train_matrix\"][\"training_label_timespan\"]\n\n        color_rgb = np.random.random(3)\n\n        if show_as_of_times:\n            # Train matrix (as_of_times)\n            ax[idx][0].hlines(\n                [x for x in range(len(train_as_of_times))],\n                [x.date() for x in train_as_of_times],\n                [\n                    x.date() + convert_str_to_relativedelta(training_label_timespan)\n                    for x in train_as_of_times\n                ],\n                linewidth=3,\n                color=color_rgb,\n                label=f\"train_{idx}\",\n            )\n\n            # Test matrix\n            ax[idx][0].hlines(\n                [x for x in range(len(test_as_of_times))],\n                [x.date() for x in test_as_of_times],\n                [\n                    x.date() + convert_str_to_relativedelta(test_label_timespan)\n                    for x in test_as_of_times\n                ],\n                linewidth=3,\n                color=color_rgb,\n                label=f\"test_{idx}\",\n            )\n\n        if show_boundaries:\n            # Limits: train\n            ax[idx][0].axvspan(\n                chop[\"train_matrix\"][\"first_as_of_time\"],\n                chop[\"train_matrix\"][\"last_as_of_time\"],\n                color=color_rgb,\n                alpha=0.3,\n            )\n\n            ax[idx][0].axvline(\n                chop[\"train_matrix\"][\"matrix_info_end_time\"], color=\"k\", linestyle=\"--\"\n            )\n\n            # Limits: test\n            ax[idx][0].axvspan(\n                chop[\"test_matrices\"][0][\"first_as_of_time\"],\n                chop[\"test_matrices\"][0][\"last_as_of_time\"],\n                color=color_rgb,\n                alpha=0.3,\n            )\n\n            ax[idx][0].axvline(\n                chop[\"feature_start_time\"], color=\"k\", linestyle=\"--\", alpha=0.2\n            )\n            ax[idx][0].axvline(\n                chop[\"feature_end_time\"], color=\"k\", linestyle=\"--\", alpha=0.2\n            )\n            ax[idx][0].axvline(\n                chop[\"label_start_time\"], color=\"k\", linestyle=\"--\", alpha=0.2\n            )\n            ax[idx][0].axvline(\n                chop[\"label_end_time\"], color=\"k\", linestyle=\"--\", alpha=0.2\n            )\n\n            ax[idx][0].axvline(\n                chop[\"test_matrices\"][0][\"matrix_info_end_time\"],\n                color=\"k\",\n                linestyle=\"--\",\n            )\n\n        ax[idx][0].yaxis.set_major_locator(plt.NullLocator())\n        ax[idx][0].yaxis.set_label_position(\"right\")\n        ax[idx][0].set_ylabel(f'Label timespan \\n {test_label_timespan} (test), {training_label_timespan} (training)',\n                              rotation=\"vertical\", labelpad=30)\n\n        ax[idx][0].xaxis.set_major_formatter(md.DateFormatter(\"%Y\"))\n        ax[idx][0].xaxis.set_major_locator(md.YearLocator())\n        ax[idx][0].xaxis.set_minor_locator(md.MonthLocator())\n\n    ax[0][0].set_title(\"Timechop: Temporal cross-validation blocks\")\n    fig.subplots_adjust(hspace=0)\n    plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)\n    if save_target:\n        plt.savefig(save_target)\n    plt.show()\n</code></pre>"},{"location":"api/timechop/timechop/","title":"Timechop","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop-attributes","title":"Attributes","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.logger","title":"<code>logger = verboselogs.VerboseLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop-classes","title":"Classes","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop","title":"<code>Timechop</code>","text":"Source code in <code>src/triage/component/timechop/timechop.py</code> <pre><code>class Timechop:\n    def __init__(\n        self,\n        feature_start_time,\n        feature_end_time,\n        label_start_time,\n        label_end_time,\n        model_update_frequency,\n        training_as_of_date_frequencies,\n        max_training_histories,\n        training_label_timespans,\n        test_as_of_date_frequencies,\n        test_durations,\n        test_label_timespans,\n    ):\n\n        '''\n        Date strings should follow the format `YYYY-MM-DD`. Date intervals\n        should be strings of the Postgres interval input format.\n\n        This class is often used within the Triage experiment pipeline, and\n        initialized using parameters from a Triage [experiment config](../../../experiments/experiment-config/#time-splitting)\n\n        Arguments:\n            feature_start_time (str): Earliest date included in any feature\n            feature_end_time (str): Day after last feature date (all data\n                included in features are before this date)\n            label_start_time (str): Earliest date for which labels are available\n            label_end_time (str): Day AFTER last label date (all dates in any\n                model are before this date)\n            model_update_frequency (str): how frequently to retrain models\n            training_as_of_date_frequencies (str): time between rows for same\n                entity in train matrix\n            max_training_histories (str): Interval specifying how much history\n                for each entity to train on\n            training_label_timespans (str): how much time is included in a label\n                in the train matrix\n            test_as_of_date_frequencies (str): time between rows for same entity\n                in test matrix\n            test_durations (str): How long into the future to make predictions\n                for each entity. Controls the length of time included in a test\n                matrix\n            test_label_timespans (str): How much time is included in a label\n                in the test matrix.\n        '''\n        self.feature_start_time = dt_from_str(\n            feature_start_time\n        )\n        self.feature_end_time = dt_from_str(\n            feature_end_time\n        )\n        if self.feature_start_time &gt; self.feature_end_time:\n            raise ValueError(\"Feature start time after feature end time.\")\n\n        self.label_start_time = dt_from_str(\n            label_start_time\n        )\n        self.label_end_time = dt_from_str(\n            label_end_time\n        )\n        if self.label_start_time &gt; self.label_end_time:\n            raise ValueError(\"Label start time after label end time.\")\n\n        self.model_update_frequency = convert_str_to_relativedelta(\n            model_update_frequency\n        )\n\n        self.training_as_of_date_frequencies = utils.convert_to_list(\n            training_as_of_date_frequencies\n        )\n\n        self.test_as_of_date_frequencies = utils.convert_to_list(\n            test_as_of_date_frequencies\n        )\n\n        self.max_training_histories = utils.convert_to_list(max_training_histories)\n\n        self.test_durations = utils.convert_to_list(test_durations)\n\n        self.training_label_timespans = utils.convert_to_list(training_label_timespans)\n\n        self.test_label_timespans = utils.convert_to_list(test_label_timespans)\n\n    def chop_time(self):\n        \"\"\" Given the attributes of the object, define all train/test splits\n        for all combinations of the temporal parameters.\n\n        return:\n            list: a list of dictionaries defining train/test splits\n        \"\"\"\n        matrix_set_definitions = []\n        # in our example, we just have one value for each of these: 6month, 6month, and 3month\n        for (\n            training_label_timespan,\n            test_label_timespan,\n            test_duration,\n        ) in itertools.product(\n            self.training_label_timespans,\n            self.test_label_timespans,\n            self.test_durations,\n        ):\n            # calculating the train-test split times starts from the end and walks backwards\n            # e.g., train_test_split_times for our example with a 1 year model_update_frequency\n            # will be every Oct. 1 from 2012 to 2016 (see comments in the method for details\n            # on the calculation):\n            # train_test_split_times = [2012-10-01, 2013-10-01, 2014-10-01, 2015-10-01, 2016-10-01]\n            logger.spam(\n                f\"Calculating train/test split times for training prediction span {training_label_timespan}, \"\n                f\"test prediction span {test_label_timespan}, test span {test_duration}\"\n            )\n            train_test_split_times = self.calculate_train_test_split_times(\n                training_label_timespan=convert_str_to_relativedelta(\n                    training_label_timespan\n                ),\n                test_label_timespan=convert_str_to_relativedelta(test_label_timespan),\n                test_duration=test_duration,\n            )\n            logger.spam(f\"Train/test split times: {train_test_split_times}\")\n\n            # handle each training_as_of_date_frequency and max_training_history separately\n            # to create matrices for each train_test_split_time.\n            # in our example, we only have one value for each: 1day and 2year\n            for (\n                training_as_of_date_frequency,\n                max_training_history,\n            ) in itertools.product(\n                self.training_as_of_date_frequencies, self.max_training_histories\n            ):\n                logger.spam(\n                    f\"Generating matrix definitions for training_as_of_date_frequency {training_as_of_date_frequency}, \"\n                    f\"max_training_history {max_training_history}\"\n                )\n                for train_test_split_time in train_test_split_times:\n                    logger.spam(f\"Generating matrix definitions for split {train_test_split_time}\")\n                    matrix_set_definitions.append(\n                        self.generate_matrix_definitions(\n                            train_test_split_time=train_test_split_time,\n                            training_as_of_date_frequency=training_as_of_date_frequency,\n                            max_training_history=max_training_history,\n                            test_duration=test_duration,\n                            training_label_timespan=training_label_timespan,\n                            test_label_timespan=test_label_timespan,\n                        )\n                    )\n        return matrix_set_definitions\n\n    def calculate_train_test_split_times(\n        self, training_label_timespan, test_label_timespan, test_duration\n    ):\n        \"\"\" Calculate the split times between train and test matrices. All\n        label spans in train matrices will end at this time, and this will be\n        the first as of time in the respective test matrix.\n\n        Arguments:\n            training_label_timespan (dateutil.relativedelta.relativedelta): how much\n                time is included in training labels\n            test_label_timespan (dateutil.relativedelta.relativedelta): how much time is included in test labels\n            test_duration (str): for how long after the end of a training matrix are\n                test predictions made\n\n        Returns:\n            list: all split times for the temporal parameters\n        Raises:\n            ValueError: if there are no valid split times in the temporal\n            config\n        \"\"\"\n\n        # we always want to be sure we're using the most recent data, so for the splits,\n        # we start from the very end of time for which we have labels and walk backwards,\n        # ensuring we leave enough of a buffer for the test_label_timespan to get a full\n        # set of labels for our last testing as_of_date\n        #\n        # in our example, last_test_label_time = 2017-07-01 - 6month = 2017-01-01\n        last_test_label_time = self.label_end_time - test_label_timespan\n\n        # final label must be able to have feature data associated with it\n        if last_test_label_time &gt; self.feature_end_time:\n            last_test_label_time = self.feature_end_time\n            raise ValueError(\n                \"Final test label date cannot be after end of feature time.\"\n            )\n        logger.spam(f\"Final label as of date: {last_test_label_time}\")\n\n        # all split times have to allow at least one training label before them\n        # e.g., earliest_possible_split_time = max(1995-01-01, 2012-01-01) + 6month = 2012-01-01\n        earliest_possible_split_time = training_label_timespan + max(\n            self.feature_start_time, self.label_start_time\n        )\n        logger.spam(f\"Earliest possible train/test split time: {earliest_possible_split_time}\")\n\n        # last split is the first as of time in the final test matrix\n        # that is, starting from the label_end_time, we've walked back by the test_label_timespan\n        # (above) to allow a buffer for labels and now we walk back further by the test_duration to\n        # ensure we have a full set of test data in the latest test matrix.\n        #\n        # e.g., last_split_time = 2017-01-01 - 3month = 2016-10-01\n        test_delta = convert_str_to_relativedelta(test_duration)\n        last_split_time = last_test_label_time - test_delta\n        logger.spam(f\"Final split time: {last_split_time}\")\n        if last_split_time &lt; earliest_possible_split_time:\n            raise ValueError(\"No valid train/test split times in temporal config.\")\n\n        train_test_split_times = []\n        train_test_split_time = last_split_time\n\n        # finally, starting from our last_split_time, simply step backwards by the\n        # model_update_frequency until we hit the earliest allowable time to\n        # yield the set of train_test_split_times\n        #\n        # e.g., train_test_split_times for our example with a 1 year model_update_frequency\n        # will be every Oct. 1 from 2012 to 2016:\n        # train_test_split_times = [2012-10-01, 2013-10-01, 2014-10-01, 2015-10-01, 2016-10-01]\n        while train_test_split_time &gt;= earliest_possible_split_time:\n            train_test_split_times.insert(0, train_test_split_time)\n            train_test_split_time -= self.model_update_frequency\n\n        return train_test_split_times\n\n    # matrix_end_time is now matrix_end_time - label_window\n    def calculate_as_of_times(\n        self, as_of_start_limit, as_of_end_limit, data_frequency, forward=False\n    ):\n        \"\"\" Given a start and stop time, a frequncy, and a direction, calculate the\n        as of times for a matrix.\n\n        Arguments:\n            as_of_start_limit (datetime.datetime): the earliest possible as of time for a matrix\n            as_of_end_limit (datetime.datetime): the last possible as of time for the matrix\n            data_frequency (str): The time interval that should pass between rows\n                of a single entity. Of the format `'date unit'`. For example,\n                `'1 month'`.\n            forward (boolean): whether to generate times forward from the start time\n                            (True) or backward from the end time (False)\n\n        return:\n            list: list of as of times for the matrix\n        \"\"\"\n        logger.spam(f\"Calculating as_of_times from {as_of_start_limit} to {as_of_end_limit} using example frequency {data_frequency}\")\n\n        as_of_times = []\n\n        # in our example, this will apply to the test matrix with parameters\n        #   as_of_start_limit = 2016-10-01, as_of_end_limit = 2017-01-01,\n        #   data_frequency = 1month, forward=True\n        # so, we'll start at 2016-10-01 and append this to the list of\n        # as_of_times, then step forward one month at a time until we hit (but\n        # do not include) 2017-01-01, yielding three values:\n        #   [2016-10-01, 2016-11-01, 2016-12-01]\n        if forward:\n            as_of_time = as_of_start_limit\n            # essentially a do-while loop for test matrices since\n            # identical start and end times should include the first\n            # date (e.g., ['2017-01-01', '2017-01-01') should give\n            # preference to the inclusive side)\n            as_of_times.append(as_of_time)\n            as_of_time += data_frequency\n            while as_of_time &lt; as_of_end_limit:\n                as_of_times.append(as_of_time)\n                as_of_time += data_frequency\n\n        # in our example, this will apply to the training matrix with parameters\n        #   as_of_start_limit = 2014-04-01, as_of_end_limit = 2016-04-01,\n        #   data_frequency = 1day, forward=False\n        # so, we'll start from 2016-04-01 and step back by one day at a time\n        # appending the results to the list of as_of_times until we hit 2014-04-01\n        # (which will also be included)\n        else:\n            as_of_time = as_of_end_limit\n            while as_of_time &gt;= as_of_start_limit:\n                as_of_times.insert(0, as_of_time)\n                as_of_time -= data_frequency\n\n        return as_of_times\n\n    def generate_matrix_definitions(\n        self,\n        train_test_split_time,\n        training_as_of_date_frequency,\n        max_training_history,\n        test_duration,\n        training_label_timespan,\n        test_label_timespan,\n    ):\n        \"\"\" Given a split time and parameters for train and test matrices,\n        generate as of times and metadata for the matrices in the split.\n\n        Arguments:\n            train_test_split_time (datetime.datetime): the limit of the last label in the matrix\n            training_as_of_date_frequency (str): how much time between rows for an entity\n                                            in a training matrix\n            max_training_history (str): how far back from split do train\n                                        as_of_times go\n            test_duration (str): how far forward from split do test as_of_times go\n            training_label_timespan (str): how much time covered by train labels\n            test_label_timespan (str): how much time is covered by test labels\n\n        returns:\n            dict: dictionary defining the train and test matrices for a split\n        \"\"\"\n\n        # continuing our example, let's consider the case when this is called for the last\n        # train_test_split_time, so the parameters here are:\n        #   train_test_split_time = 2016-10-01\n        #   training_as_of_date_frequency = 1day\n        #   max_training_history = 2year\n        #   test_duration = 3month\n        #   training_label_timespan = 6month\n        #   test_label_timespan = 6month\n\n        # for the example, the train matrix will contain as_of_dates for every day from\n        # 2014-04-01 through 2016-04-01, including _both_ endpoints, providing a 6 month\n        # buffer between the last as_of_time and the train-test split time for the last\n        # set of labels (see comments in the method for details)\n        train_matrix_definition = self.define_train_matrix(\n            train_test_split_time=train_test_split_time,\n            training_label_timespan=training_label_timespan,\n            max_training_history=max_training_history,\n            training_as_of_date_frequency=training_as_of_date_frequency,\n        )\n\n        # for the example, the test matrix will contain three as_of_dates:\n        #   [2016-10-01, 2016-11-01, 2016-12-01]\n        # since we start at the train_test_split_time (2016-10-01) and walk forward by\n        # the test_as_of_date_frequency (1 month) until we've exhausted the test_duration\n        # (3 months), exclusive (see comments in the method for details)\n        test_matrix_definitions = self.define_test_matrices(\n            train_test_split_time=train_test_split_time,\n            test_duration=test_duration,\n            test_label_timespan=test_label_timespan,\n        )\n\n        matrix_set_definition = {\n            \"feature_start_time\": self.feature_start_time,\n            \"feature_end_time\": self.feature_end_time,\n            \"label_start_time\": self.label_start_time,\n            \"label_end_time\": self.label_end_time,\n            \"train_matrix\": train_matrix_definition,\n            \"test_matrices\": test_matrix_definitions,\n        }\n        logger.spam(f\"Matrix definitions for train/test split {train_test_split_time}: {matrix_set_definition}\")\n\n        return matrix_set_definition\n\n    def define_train_matrix(\n        self,\n        train_test_split_time,\n        training_label_timespan,\n        max_training_history,\n        training_as_of_date_frequency,\n    ):\n        \"\"\" Given a split time and the parameters of a training matrix, generate\n        the as of times and metadata for a train matrix.\n\n        Arguments:\n            train_test_split_time (datetime.datetime): the limit of the last label in the matrix\n            training_label_timespan (str): how much time is covered by the labels\n            max_training_history (str): how far back from split do as_of_times go\n            training_as_of_date_frequency (str): how much time between rows for an entity\n\n        return:\n            dict: dictionary containing the temporal parameters and as of times\n                  for a train matrix\n        \"\"\"\n        logger.debug(f\"Generating train matrix definitions for train/test split {train_test_split_time}\")\n        # for our example, this will be called with:\n        #   train_test_split_time = 2016-10-01\n        #   training_label_timespan = 6month\n        #   max_training_history = 2year\n        #   training_as_of_date_frequency = 1day\n\n        # last as of time in the matrix is 1 label span before split to provide\n        # enough of a buffer for the label data to avoid spilling into the test\n        # matrix and causing a leakage problem.\n        #\n        # e.g., last_train_as_of_time = 2016-10-01 - 6month = 2016-04-01\n        training_prediction_delta = convert_str_to_relativedelta(\n            training_label_timespan\n        )\n        last_train_as_of_time = train_test_split_time - training_prediction_delta\n\n        # earliest time in matrix can't be farther back than the latest of the beginning\n        # of label time or the beginning of feature time -- whichever is latest is the\n        # limit if the amount of history we want to take would go further back.\n        #\n        # e.g., 2016-04-01 - 2year = 2014-04-01, which is later than both our\n        # label_start_time (2012-01-01) and our feature_start_time (1995-01-01), so we\n        # can use earliest_possible_train_as_of_time = 2014-04-01\n        max_training_delta = convert_str_to_relativedelta(max_training_history)\n        earliest_possible_train_as_of_time = last_train_as_of_time - max_training_delta\n        experiment_as_of_time_limit = max(\n            self.label_start_time, self.feature_start_time\n        )\n        if earliest_possible_train_as_of_time &lt; experiment_as_of_time_limit:\n            earliest_possible_train_as_of_time = experiment_as_of_time_limit\n        logger.spam(f\"Earliest possible train as of time: {earliest_possible_train_as_of_time}\")\n\n        # with the last as of time and the earliest possible time known,\n        # calculate all the as of times for the matrix, stepping backwards\n        # from the last as of time (to ensure that we use the latest possible\n        # training data even if there's a gap and things don't line up\n        # exactly) by the training_as_of_date_frequency\n        #\n        # for our example, this will give us a list of every day from 2014-04-01\n        # through 2016-04-01, including _both_ endpoints\n        train_as_of_times = self.calculate_as_of_times(\n            as_of_start_limit=earliest_possible_train_as_of_time,\n            as_of_end_limit=last_train_as_of_time,\n            data_frequency=convert_str_to_relativedelta(training_as_of_date_frequency),\n        )\n        logger.spam(f\"Train as of times: {train_as_of_times}\")\n\n        # create a dict of the matrix metadata\n        matrix_definition = {\n            \"first_as_of_time\": min(train_as_of_times),\n            \"last_as_of_time\": max(train_as_of_times),\n            \"matrix_info_end_time\": train_test_split_time,\n            \"as_of_times\": AsOfTimeList(train_as_of_times),\n            \"training_label_timespan\": training_label_timespan,\n            \"training_as_of_date_frequency\": training_as_of_date_frequency,\n            \"max_training_history\": max_training_history,\n        }\n\n        return matrix_definition\n\n    def define_test_matrices(\n        self, train_test_split_time, test_duration, test_label_timespan\n    ):\n        \"\"\" Given a train/test split time and a set of testing parameters,\n        generate the metadata and as of times for the test matrices in a split.\n\n        Arguments:\n            train_test_split_time (datetime.datetime): the limit of the last label in the matrix\n            test_duration (str): how far forward from split do test as_of_times go\n            test_label_timespan (str): how much time is covered by test labels\n\n        return:\n            list: list of dictionaries defining the test matrices for a split\n        \"\"\"\n\n        # for our example, this will be called with:\n        #   train_test_split_time = 2016-10-01\n        #   test_duration = 3month\n        #   test_label_timespan = 6month\n\n        # the as_of_time_limit is simply the split time plus the test_duration and we\n        # can avoid checking here for any issues with the label_end_time or\n        # feature_end_time since we've guaranteed that those limits would be\n        # satisfied when we calculated the train_test_split_times initially\n        #\n        # for the example, as_of_time_limit = 2016-10-01 + 3month = 2017-01-01\n        # (note as well that this will be treated as an _exclusive_ limit)\n        logger.debug(f\"Generating test matrix definitions for train/test split {train_test_split_time}\")\n        test_definitions = []\n        test_delta = convert_str_to_relativedelta(test_duration)\n        as_of_time_limit = train_test_split_time + test_delta\n        logger.spam(\"All test as of times before %s\", as_of_time_limit)\n\n        # calculate the as_of_times associated with each test data frequency\n        # for our example, we just have one, 1month\n        for test_as_of_date_frequency in self.test_as_of_date_frequencies:\n            logger.spam(f\"Generating test matrix definitions for test data frequency {test_as_of_date_frequency}\")\n\n            # for test as_of_times we step _forwards_ from the train_test_split_time\n            # to ensure that we always have a prediction set made immediately after\n            # training is done (so, the freshest possible predictions) even if the\n            # frequency doesn't divide the test_duration evenly so there's a gap before\n            # the as_of_time_limit\n            #\n            # for our example, this will give three as_of_dates:\n            #   [2016-10-01, 2016-11-01, 2016-12-01]\n            # since we start at the train_test_split_time (2016-10-01) and walk forward by\n            # the test_as_of_date_frequency (1 month) until we've exhausted the test_duration\n            # (3 months), exclusive (see comments in the method for details)\n            test_as_of_times = self.calculate_as_of_times(\n                as_of_start_limit=train_test_split_time,\n                as_of_end_limit=as_of_time_limit,\n                data_frequency=convert_str_to_relativedelta(test_as_of_date_frequency),\n                forward=True,\n            )\n            logger.spam(f\"test as of times: {test_as_of_times}\")\n            test_definition = {\n                \"first_as_of_time\": train_test_split_time,\n                \"last_as_of_time\": max(test_as_of_times),\n                \"matrix_info_end_time\": max(test_as_of_times)\n                + convert_str_to_relativedelta(test_label_timespan),\n                \"as_of_times\": AsOfTimeList(test_as_of_times),\n                \"test_label_timespan\": test_label_timespan,\n                \"test_as_of_date_frequency\": test_as_of_date_frequency,\n                \"test_duration\": test_duration,\n            }\n            test_definitions.append(test_definition)\n        return test_definitions\n</code></pre>"},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop-attributes","title":"Attributes","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.feature_end_time","title":"<code>feature_end_time = dt_from_str(feature_end_time)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.feature_start_time","title":"<code>feature_start_time = dt_from_str(feature_start_time)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.label_end_time","title":"<code>label_end_time = dt_from_str(label_end_time)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.label_start_time","title":"<code>label_start_time = dt_from_str(label_start_time)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.max_training_histories","title":"<code>max_training_histories = utils.convert_to_list(max_training_histories)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.model_update_frequency","title":"<code>model_update_frequency = convert_str_to_relativedelta(model_update_frequency)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.test_as_of_date_frequencies","title":"<code>test_as_of_date_frequencies = utils.convert_to_list(test_as_of_date_frequencies)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.test_durations","title":"<code>test_durations = utils.convert_to_list(test_durations)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.test_label_timespans","title":"<code>test_label_timespans = utils.convert_to_list(test_label_timespans)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.training_as_of_date_frequencies","title":"<code>training_as_of_date_frequencies = utils.convert_to_list(training_as_of_date_frequencies)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.training_label_timespans","title":"<code>training_label_timespans = utils.convert_to_list(training_label_timespans)</code>  <code>instance-attribute</code>","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop-functions","title":"Functions","text":""},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.__init__","title":"<code>__init__(feature_start_time, feature_end_time, label_start_time, label_end_time, model_update_frequency, training_as_of_date_frequencies, max_training_histories, training_label_timespans, test_as_of_date_frequencies, test_durations, test_label_timespans)</code>","text":"<p>Date strings should follow the format <code>YYYY-MM-DD</code>. Date intervals should be strings of the Postgres interval input format.</p> <p>This class is often used within the Triage experiment pipeline, and initialized using parameters from a Triage experiment config</p> <p>Parameters:</p> Name Type Description Default <code>feature_start_time</code> <code>str</code> <p>Earliest date included in any feature</p> required <code>feature_end_time</code> <code>str</code> <p>Day after last feature date (all data included in features are before this date)</p> required <code>label_start_time</code> <code>str</code> <p>Earliest date for which labels are available</p> required <code>label_end_time</code> <code>str</code> <p>Day AFTER last label date (all dates in any model are before this date)</p> required <code>model_update_frequency</code> <code>str</code> <p>how frequently to retrain models</p> required <code>training_as_of_date_frequencies</code> <code>str</code> <p>time between rows for same entity in train matrix</p> required <code>max_training_histories</code> <code>str</code> <p>Interval specifying how much history for each entity to train on</p> required <code>training_label_timespans</code> <code>str</code> <p>how much time is included in a label in the train matrix</p> required <code>test_as_of_date_frequencies</code> <code>str</code> <p>time between rows for same entity in test matrix</p> required <code>test_durations</code> <code>str</code> <p>How long into the future to make predictions for each entity. Controls the length of time included in a test matrix</p> required <code>test_label_timespans</code> <code>str</code> <p>How much time is included in a label in the test matrix.</p> required Source code in <code>src/triage/component/timechop/timechop.py</code> <pre><code>def __init__(\n    self,\n    feature_start_time,\n    feature_end_time,\n    label_start_time,\n    label_end_time,\n    model_update_frequency,\n    training_as_of_date_frequencies,\n    max_training_histories,\n    training_label_timespans,\n    test_as_of_date_frequencies,\n    test_durations,\n    test_label_timespans,\n):\n\n    '''\n    Date strings should follow the format `YYYY-MM-DD`. Date intervals\n    should be strings of the Postgres interval input format.\n\n    This class is often used within the Triage experiment pipeline, and\n    initialized using parameters from a Triage [experiment config](../../../experiments/experiment-config/#time-splitting)\n\n    Arguments:\n        feature_start_time (str): Earliest date included in any feature\n        feature_end_time (str): Day after last feature date (all data\n            included in features are before this date)\n        label_start_time (str): Earliest date for which labels are available\n        label_end_time (str): Day AFTER last label date (all dates in any\n            model are before this date)\n        model_update_frequency (str): how frequently to retrain models\n        training_as_of_date_frequencies (str): time between rows for same\n            entity in train matrix\n        max_training_histories (str): Interval specifying how much history\n            for each entity to train on\n        training_label_timespans (str): how much time is included in a label\n            in the train matrix\n        test_as_of_date_frequencies (str): time between rows for same entity\n            in test matrix\n        test_durations (str): How long into the future to make predictions\n            for each entity. Controls the length of time included in a test\n            matrix\n        test_label_timespans (str): How much time is included in a label\n            in the test matrix.\n    '''\n    self.feature_start_time = dt_from_str(\n        feature_start_time\n    )\n    self.feature_end_time = dt_from_str(\n        feature_end_time\n    )\n    if self.feature_start_time &gt; self.feature_end_time:\n        raise ValueError(\"Feature start time after feature end time.\")\n\n    self.label_start_time = dt_from_str(\n        label_start_time\n    )\n    self.label_end_time = dt_from_str(\n        label_end_time\n    )\n    if self.label_start_time &gt; self.label_end_time:\n        raise ValueError(\"Label start time after label end time.\")\n\n    self.model_update_frequency = convert_str_to_relativedelta(\n        model_update_frequency\n    )\n\n    self.training_as_of_date_frequencies = utils.convert_to_list(\n        training_as_of_date_frequencies\n    )\n\n    self.test_as_of_date_frequencies = utils.convert_to_list(\n        test_as_of_date_frequencies\n    )\n\n    self.max_training_histories = utils.convert_to_list(max_training_histories)\n\n    self.test_durations = utils.convert_to_list(test_durations)\n\n    self.training_label_timespans = utils.convert_to_list(training_label_timespans)\n\n    self.test_label_timespans = utils.convert_to_list(test_label_timespans)\n</code></pre>"},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.calculate_as_of_times","title":"<code>calculate_as_of_times(as_of_start_limit, as_of_end_limit, data_frequency, forward=False)</code>","text":"<p>Given a start and stop time, a frequncy, and a direction, calculate the as of times for a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>as_of_start_limit</code> <code>datetime</code> <p>the earliest possible as of time for a matrix</p> required <code>as_of_end_limit</code> <code>datetime</code> <p>the last possible as of time for the matrix</p> required <code>data_frequency</code> <code>str</code> <p>The time interval that should pass between rows of a single entity. Of the format <code>'date unit'</code>. For example, <code>'1 month'</code>.</p> required <code>forward</code> <code>boolean</code> <p>whether to generate times forward from the start time             (True) or backward from the end time (False)</p> <code>False</code> return Source code in <code>src/triage/component/timechop/timechop.py</code> <pre><code>def calculate_as_of_times(\n    self, as_of_start_limit, as_of_end_limit, data_frequency, forward=False\n):\n    \"\"\" Given a start and stop time, a frequncy, and a direction, calculate the\n    as of times for a matrix.\n\n    Arguments:\n        as_of_start_limit (datetime.datetime): the earliest possible as of time for a matrix\n        as_of_end_limit (datetime.datetime): the last possible as of time for the matrix\n        data_frequency (str): The time interval that should pass between rows\n            of a single entity. Of the format `'date unit'`. For example,\n            `'1 month'`.\n        forward (boolean): whether to generate times forward from the start time\n                        (True) or backward from the end time (False)\n\n    return:\n        list: list of as of times for the matrix\n    \"\"\"\n    logger.spam(f\"Calculating as_of_times from {as_of_start_limit} to {as_of_end_limit} using example frequency {data_frequency}\")\n\n    as_of_times = []\n\n    # in our example, this will apply to the test matrix with parameters\n    #   as_of_start_limit = 2016-10-01, as_of_end_limit = 2017-01-01,\n    #   data_frequency = 1month, forward=True\n    # so, we'll start at 2016-10-01 and append this to the list of\n    # as_of_times, then step forward one month at a time until we hit (but\n    # do not include) 2017-01-01, yielding three values:\n    #   [2016-10-01, 2016-11-01, 2016-12-01]\n    if forward:\n        as_of_time = as_of_start_limit\n        # essentially a do-while loop for test matrices since\n        # identical start and end times should include the first\n        # date (e.g., ['2017-01-01', '2017-01-01') should give\n        # preference to the inclusive side)\n        as_of_times.append(as_of_time)\n        as_of_time += data_frequency\n        while as_of_time &lt; as_of_end_limit:\n            as_of_times.append(as_of_time)\n            as_of_time += data_frequency\n\n    # in our example, this will apply to the training matrix with parameters\n    #   as_of_start_limit = 2014-04-01, as_of_end_limit = 2016-04-01,\n    #   data_frequency = 1day, forward=False\n    # so, we'll start from 2016-04-01 and step back by one day at a time\n    # appending the results to the list of as_of_times until we hit 2014-04-01\n    # (which will also be included)\n    else:\n        as_of_time = as_of_end_limit\n        while as_of_time &gt;= as_of_start_limit:\n            as_of_times.insert(0, as_of_time)\n            as_of_time -= data_frequency\n\n    return as_of_times\n</code></pre>"},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.calculate_train_test_split_times","title":"<code>calculate_train_test_split_times(training_label_timespan, test_label_timespan, test_duration)</code>","text":"<p>Calculate the split times between train and test matrices. All label spans in train matrices will end at this time, and this will be the first as of time in the respective test matrix.</p> <p>Parameters:</p> Name Type Description Default <code>training_label_timespan</code> <code>relativedelta</code> <p>how much time is included in training labels</p> required <code>test_label_timespan</code> <code>relativedelta</code> <p>how much time is included in test labels</p> required <code>test_duration</code> <code>str</code> <p>for how long after the end of a training matrix are test predictions made</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>all split times for the temporal parameters</p> Source code in <code>src/triage/component/timechop/timechop.py</code> <pre><code>def calculate_train_test_split_times(\n    self, training_label_timespan, test_label_timespan, test_duration\n):\n    \"\"\" Calculate the split times between train and test matrices. All\n    label spans in train matrices will end at this time, and this will be\n    the first as of time in the respective test matrix.\n\n    Arguments:\n        training_label_timespan (dateutil.relativedelta.relativedelta): how much\n            time is included in training labels\n        test_label_timespan (dateutil.relativedelta.relativedelta): how much time is included in test labels\n        test_duration (str): for how long after the end of a training matrix are\n            test predictions made\n\n    Returns:\n        list: all split times for the temporal parameters\n    Raises:\n        ValueError: if there are no valid split times in the temporal\n        config\n    \"\"\"\n\n    # we always want to be sure we're using the most recent data, so for the splits,\n    # we start from the very end of time for which we have labels and walk backwards,\n    # ensuring we leave enough of a buffer for the test_label_timespan to get a full\n    # set of labels for our last testing as_of_date\n    #\n    # in our example, last_test_label_time = 2017-07-01 - 6month = 2017-01-01\n    last_test_label_time = self.label_end_time - test_label_timespan\n\n    # final label must be able to have feature data associated with it\n    if last_test_label_time &gt; self.feature_end_time:\n        last_test_label_time = self.feature_end_time\n        raise ValueError(\n            \"Final test label date cannot be after end of feature time.\"\n        )\n    logger.spam(f\"Final label as of date: {last_test_label_time}\")\n\n    # all split times have to allow at least one training label before them\n    # e.g., earliest_possible_split_time = max(1995-01-01, 2012-01-01) + 6month = 2012-01-01\n    earliest_possible_split_time = training_label_timespan + max(\n        self.feature_start_time, self.label_start_time\n    )\n    logger.spam(f\"Earliest possible train/test split time: {earliest_possible_split_time}\")\n\n    # last split is the first as of time in the final test matrix\n    # that is, starting from the label_end_time, we've walked back by the test_label_timespan\n    # (above) to allow a buffer for labels and now we walk back further by the test_duration to\n    # ensure we have a full set of test data in the latest test matrix.\n    #\n    # e.g., last_split_time = 2017-01-01 - 3month = 2016-10-01\n    test_delta = convert_str_to_relativedelta(test_duration)\n    last_split_time = last_test_label_time - test_delta\n    logger.spam(f\"Final split time: {last_split_time}\")\n    if last_split_time &lt; earliest_possible_split_time:\n        raise ValueError(\"No valid train/test split times in temporal config.\")\n\n    train_test_split_times = []\n    train_test_split_time = last_split_time\n\n    # finally, starting from our last_split_time, simply step backwards by the\n    # model_update_frequency until we hit the earliest allowable time to\n    # yield the set of train_test_split_times\n    #\n    # e.g., train_test_split_times for our example with a 1 year model_update_frequency\n    # will be every Oct. 1 from 2012 to 2016:\n    # train_test_split_times = [2012-10-01, 2013-10-01, 2014-10-01, 2015-10-01, 2016-10-01]\n    while train_test_split_time &gt;= earliest_possible_split_time:\n        train_test_split_times.insert(0, train_test_split_time)\n        train_test_split_time -= self.model_update_frequency\n\n    return train_test_split_times\n</code></pre>"},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.chop_time","title":"<code>chop_time()</code>","text":"<p>Given the attributes of the object, define all train/test splits for all combinations of the temporal parameters.</p> return Source code in <code>src/triage/component/timechop/timechop.py</code> <pre><code>def chop_time(self):\n    \"\"\" Given the attributes of the object, define all train/test splits\n    for all combinations of the temporal parameters.\n\n    return:\n        list: a list of dictionaries defining train/test splits\n    \"\"\"\n    matrix_set_definitions = []\n    # in our example, we just have one value for each of these: 6month, 6month, and 3month\n    for (\n        training_label_timespan,\n        test_label_timespan,\n        test_duration,\n    ) in itertools.product(\n        self.training_label_timespans,\n        self.test_label_timespans,\n        self.test_durations,\n    ):\n        # calculating the train-test split times starts from the end and walks backwards\n        # e.g., train_test_split_times for our example with a 1 year model_update_frequency\n        # will be every Oct. 1 from 2012 to 2016 (see comments in the method for details\n        # on the calculation):\n        # train_test_split_times = [2012-10-01, 2013-10-01, 2014-10-01, 2015-10-01, 2016-10-01]\n        logger.spam(\n            f\"Calculating train/test split times for training prediction span {training_label_timespan}, \"\n            f\"test prediction span {test_label_timespan}, test span {test_duration}\"\n        )\n        train_test_split_times = self.calculate_train_test_split_times(\n            training_label_timespan=convert_str_to_relativedelta(\n                training_label_timespan\n            ),\n            test_label_timespan=convert_str_to_relativedelta(test_label_timespan),\n            test_duration=test_duration,\n        )\n        logger.spam(f\"Train/test split times: {train_test_split_times}\")\n\n        # handle each training_as_of_date_frequency and max_training_history separately\n        # to create matrices for each train_test_split_time.\n        # in our example, we only have one value for each: 1day and 2year\n        for (\n            training_as_of_date_frequency,\n            max_training_history,\n        ) in itertools.product(\n            self.training_as_of_date_frequencies, self.max_training_histories\n        ):\n            logger.spam(\n                f\"Generating matrix definitions for training_as_of_date_frequency {training_as_of_date_frequency}, \"\n                f\"max_training_history {max_training_history}\"\n            )\n            for train_test_split_time in train_test_split_times:\n                logger.spam(f\"Generating matrix definitions for split {train_test_split_time}\")\n                matrix_set_definitions.append(\n                    self.generate_matrix_definitions(\n                        train_test_split_time=train_test_split_time,\n                        training_as_of_date_frequency=training_as_of_date_frequency,\n                        max_training_history=max_training_history,\n                        test_duration=test_duration,\n                        training_label_timespan=training_label_timespan,\n                        test_label_timespan=test_label_timespan,\n                    )\n                )\n    return matrix_set_definitions\n</code></pre>"},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.define_test_matrices","title":"<code>define_test_matrices(train_test_split_time, test_duration, test_label_timespan)</code>","text":"<p>Given a train/test split time and a set of testing parameters, generate the metadata and as of times for the test matrices in a split.</p> <p>Parameters:</p> Name Type Description Default <code>train_test_split_time</code> <code>datetime</code> <p>the limit of the last label in the matrix</p> required <code>test_duration</code> <code>str</code> <p>how far forward from split do test as_of_times go</p> required <code>test_label_timespan</code> <code>str</code> <p>how much time is covered by test labels</p> required return Source code in <code>src/triage/component/timechop/timechop.py</code> <pre><code>def define_test_matrices(\n    self, train_test_split_time, test_duration, test_label_timespan\n):\n    \"\"\" Given a train/test split time and a set of testing parameters,\n    generate the metadata and as of times for the test matrices in a split.\n\n    Arguments:\n        train_test_split_time (datetime.datetime): the limit of the last label in the matrix\n        test_duration (str): how far forward from split do test as_of_times go\n        test_label_timespan (str): how much time is covered by test labels\n\n    return:\n        list: list of dictionaries defining the test matrices for a split\n    \"\"\"\n\n    # for our example, this will be called with:\n    #   train_test_split_time = 2016-10-01\n    #   test_duration = 3month\n    #   test_label_timespan = 6month\n\n    # the as_of_time_limit is simply the split time plus the test_duration and we\n    # can avoid checking here for any issues with the label_end_time or\n    # feature_end_time since we've guaranteed that those limits would be\n    # satisfied when we calculated the train_test_split_times initially\n    #\n    # for the example, as_of_time_limit = 2016-10-01 + 3month = 2017-01-01\n    # (note as well that this will be treated as an _exclusive_ limit)\n    logger.debug(f\"Generating test matrix definitions for train/test split {train_test_split_time}\")\n    test_definitions = []\n    test_delta = convert_str_to_relativedelta(test_duration)\n    as_of_time_limit = train_test_split_time + test_delta\n    logger.spam(\"All test as of times before %s\", as_of_time_limit)\n\n    # calculate the as_of_times associated with each test data frequency\n    # for our example, we just have one, 1month\n    for test_as_of_date_frequency in self.test_as_of_date_frequencies:\n        logger.spam(f\"Generating test matrix definitions for test data frequency {test_as_of_date_frequency}\")\n\n        # for test as_of_times we step _forwards_ from the train_test_split_time\n        # to ensure that we always have a prediction set made immediately after\n        # training is done (so, the freshest possible predictions) even if the\n        # frequency doesn't divide the test_duration evenly so there's a gap before\n        # the as_of_time_limit\n        #\n        # for our example, this will give three as_of_dates:\n        #   [2016-10-01, 2016-11-01, 2016-12-01]\n        # since we start at the train_test_split_time (2016-10-01) and walk forward by\n        # the test_as_of_date_frequency (1 month) until we've exhausted the test_duration\n        # (3 months), exclusive (see comments in the method for details)\n        test_as_of_times = self.calculate_as_of_times(\n            as_of_start_limit=train_test_split_time,\n            as_of_end_limit=as_of_time_limit,\n            data_frequency=convert_str_to_relativedelta(test_as_of_date_frequency),\n            forward=True,\n        )\n        logger.spam(f\"test as of times: {test_as_of_times}\")\n        test_definition = {\n            \"first_as_of_time\": train_test_split_time,\n            \"last_as_of_time\": max(test_as_of_times),\n            \"matrix_info_end_time\": max(test_as_of_times)\n            + convert_str_to_relativedelta(test_label_timespan),\n            \"as_of_times\": AsOfTimeList(test_as_of_times),\n            \"test_label_timespan\": test_label_timespan,\n            \"test_as_of_date_frequency\": test_as_of_date_frequency,\n            \"test_duration\": test_duration,\n        }\n        test_definitions.append(test_definition)\n    return test_definitions\n</code></pre>"},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.define_train_matrix","title":"<code>define_train_matrix(train_test_split_time, training_label_timespan, max_training_history, training_as_of_date_frequency)</code>","text":"<p>Given a split time and the parameters of a training matrix, generate the as of times and metadata for a train matrix.</p> <p>Parameters:</p> Name Type Description Default <code>train_test_split_time</code> <code>datetime</code> <p>the limit of the last label in the matrix</p> required <code>training_label_timespan</code> <code>str</code> <p>how much time is covered by the labels</p> required <code>max_training_history</code> <code>str</code> <p>how far back from split do as_of_times go</p> required <code>training_as_of_date_frequency</code> <code>str</code> <p>how much time between rows for an entity</p> required return Source code in <code>src/triage/component/timechop/timechop.py</code> <pre><code>def define_train_matrix(\n    self,\n    train_test_split_time,\n    training_label_timespan,\n    max_training_history,\n    training_as_of_date_frequency,\n):\n    \"\"\" Given a split time and the parameters of a training matrix, generate\n    the as of times and metadata for a train matrix.\n\n    Arguments:\n        train_test_split_time (datetime.datetime): the limit of the last label in the matrix\n        training_label_timespan (str): how much time is covered by the labels\n        max_training_history (str): how far back from split do as_of_times go\n        training_as_of_date_frequency (str): how much time between rows for an entity\n\n    return:\n        dict: dictionary containing the temporal parameters and as of times\n              for a train matrix\n    \"\"\"\n    logger.debug(f\"Generating train matrix definitions for train/test split {train_test_split_time}\")\n    # for our example, this will be called with:\n    #   train_test_split_time = 2016-10-01\n    #   training_label_timespan = 6month\n    #   max_training_history = 2year\n    #   training_as_of_date_frequency = 1day\n\n    # last as of time in the matrix is 1 label span before split to provide\n    # enough of a buffer for the label data to avoid spilling into the test\n    # matrix and causing a leakage problem.\n    #\n    # e.g., last_train_as_of_time = 2016-10-01 - 6month = 2016-04-01\n    training_prediction_delta = convert_str_to_relativedelta(\n        training_label_timespan\n    )\n    last_train_as_of_time = train_test_split_time - training_prediction_delta\n\n    # earliest time in matrix can't be farther back than the latest of the beginning\n    # of label time or the beginning of feature time -- whichever is latest is the\n    # limit if the amount of history we want to take would go further back.\n    #\n    # e.g., 2016-04-01 - 2year = 2014-04-01, which is later than both our\n    # label_start_time (2012-01-01) and our feature_start_time (1995-01-01), so we\n    # can use earliest_possible_train_as_of_time = 2014-04-01\n    max_training_delta = convert_str_to_relativedelta(max_training_history)\n    earliest_possible_train_as_of_time = last_train_as_of_time - max_training_delta\n    experiment_as_of_time_limit = max(\n        self.label_start_time, self.feature_start_time\n    )\n    if earliest_possible_train_as_of_time &lt; experiment_as_of_time_limit:\n        earliest_possible_train_as_of_time = experiment_as_of_time_limit\n    logger.spam(f\"Earliest possible train as of time: {earliest_possible_train_as_of_time}\")\n\n    # with the last as of time and the earliest possible time known,\n    # calculate all the as of times for the matrix, stepping backwards\n    # from the last as of time (to ensure that we use the latest possible\n    # training data even if there's a gap and things don't line up\n    # exactly) by the training_as_of_date_frequency\n    #\n    # for our example, this will give us a list of every day from 2014-04-01\n    # through 2016-04-01, including _both_ endpoints\n    train_as_of_times = self.calculate_as_of_times(\n        as_of_start_limit=earliest_possible_train_as_of_time,\n        as_of_end_limit=last_train_as_of_time,\n        data_frequency=convert_str_to_relativedelta(training_as_of_date_frequency),\n    )\n    logger.spam(f\"Train as of times: {train_as_of_times}\")\n\n    # create a dict of the matrix metadata\n    matrix_definition = {\n        \"first_as_of_time\": min(train_as_of_times),\n        \"last_as_of_time\": max(train_as_of_times),\n        \"matrix_info_end_time\": train_test_split_time,\n        \"as_of_times\": AsOfTimeList(train_as_of_times),\n        \"training_label_timespan\": training_label_timespan,\n        \"training_as_of_date_frequency\": training_as_of_date_frequency,\n        \"max_training_history\": max_training_history,\n    }\n\n    return matrix_definition\n</code></pre>"},{"location":"api/timechop/timechop/#triage.component.timechop.timechop.Timechop.generate_matrix_definitions","title":"<code>generate_matrix_definitions(train_test_split_time, training_as_of_date_frequency, max_training_history, test_duration, training_label_timespan, test_label_timespan)</code>","text":"<p>Given a split time and parameters for train and test matrices, generate as of times and metadata for the matrices in the split.</p> <p>Parameters:</p> Name Type Description Default <code>train_test_split_time</code> <code>datetime</code> <p>the limit of the last label in the matrix</p> required <code>training_as_of_date_frequency</code> <code>str</code> <p>how much time between rows for an entity                             in a training matrix</p> required <code>max_training_history</code> <code>str</code> <p>how far back from split do train                         as_of_times go</p> required <code>test_duration</code> <code>str</code> <p>how far forward from split do test as_of_times go</p> required <code>training_label_timespan</code> <code>str</code> <p>how much time covered by train labels</p> required <code>test_label_timespan</code> <code>str</code> <p>how much time is covered by test labels</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>dictionary defining the train and test matrices for a split</p> Source code in <code>src/triage/component/timechop/timechop.py</code> <pre><code>def generate_matrix_definitions(\n    self,\n    train_test_split_time,\n    training_as_of_date_frequency,\n    max_training_history,\n    test_duration,\n    training_label_timespan,\n    test_label_timespan,\n):\n    \"\"\" Given a split time and parameters for train and test matrices,\n    generate as of times and metadata for the matrices in the split.\n\n    Arguments:\n        train_test_split_time (datetime.datetime): the limit of the last label in the matrix\n        training_as_of_date_frequency (str): how much time between rows for an entity\n                                        in a training matrix\n        max_training_history (str): how far back from split do train\n                                    as_of_times go\n        test_duration (str): how far forward from split do test as_of_times go\n        training_label_timespan (str): how much time covered by train labels\n        test_label_timespan (str): how much time is covered by test labels\n\n    returns:\n        dict: dictionary defining the train and test matrices for a split\n    \"\"\"\n\n    # continuing our example, let's consider the case when this is called for the last\n    # train_test_split_time, so the parameters here are:\n    #   train_test_split_time = 2016-10-01\n    #   training_as_of_date_frequency = 1day\n    #   max_training_history = 2year\n    #   test_duration = 3month\n    #   training_label_timespan = 6month\n    #   test_label_timespan = 6month\n\n    # for the example, the train matrix will contain as_of_dates for every day from\n    # 2014-04-01 through 2016-04-01, including _both_ endpoints, providing a 6 month\n    # buffer between the last as_of_time and the train-test split time for the last\n    # set of labels (see comments in the method for details)\n    train_matrix_definition = self.define_train_matrix(\n        train_test_split_time=train_test_split_time,\n        training_label_timespan=training_label_timespan,\n        max_training_history=max_training_history,\n        training_as_of_date_frequency=training_as_of_date_frequency,\n    )\n\n    # for the example, the test matrix will contain three as_of_dates:\n    #   [2016-10-01, 2016-11-01, 2016-12-01]\n    # since we start at the train_test_split_time (2016-10-01) and walk forward by\n    # the test_as_of_date_frequency (1 month) until we've exhausted the test_duration\n    # (3 months), exclusive (see comments in the method for details)\n    test_matrix_definitions = self.define_test_matrices(\n        train_test_split_time=train_test_split_time,\n        test_duration=test_duration,\n        test_label_timespan=test_label_timespan,\n    )\n\n    matrix_set_definition = {\n        \"feature_start_time\": self.feature_start_time,\n        \"feature_end_time\": self.feature_end_time,\n        \"label_start_time\": self.label_start_time,\n        \"label_end_time\": self.label_end_time,\n        \"train_matrix\": train_matrix_definition,\n        \"test_matrices\": test_matrix_definitions,\n    }\n    logger.spam(f\"Matrix definitions for train/test split {train_test_split_time}: {matrix_set_definition}\")\n\n    return matrix_set_definition\n</code></pre>"},{"location":"api/timechop/timechop/#triage.component.timechop.timechop-functions","title":"Functions","text":""},{"location":"audition/audition_intro/","title":"Intro to Audition","text":""},{"location":"audition/audition_intro/#what-is-audition","title":"What is Audition?","text":"<p>Audition is the Triage model selection module. Model selection is the process of selecting a model group that is likely to perform well on future data. Audition makes model selection easy and repeatable, even when comparing hundreds of model groups.</p> <p>Start by defining a set of coarse filtering rules that prune the worst-performing model groups. You can then apply more complex selection rules that rank model groups on factors like their performance over time or variability in performance. </p> <p>Audition can compare the performance of these selection rules, helping you choose a selection rule that is likely to identify well-performing models in the future.</p> <p>Audition works well in a Jupyter notebook, or any other environment that will conveniently display Audition's matplotlib output.</p>"},{"location":"audition/audition_intro/#the-audition-python-interface","title":"The Audition Python interface","text":""},{"location":"audition/audition_intro/#initializing-auditioner","title":"Initializing Auditioner","text":"<p>This Auditioner object reads performance data from a database populated by a Triage Experiment. It loads information about the model groups selected by <code>model_group_ids</code>, over the training sets specified by <code>train_end_times</code>, and filters them as defined by <code>initial_metric_filters</code>. </p> <p><pre><code>from triage.component.audition import Auditioner\n\naud = Auditioner(\n    db_engine = conn, # connection to a database populated by a Triage experiment\n    model_group_ids=[i for i in range(101, 150)], # selecting model groups to evaluate\n    train_end_times=end_times,\n    initial_metric_filters=\n        [{'metric': 'precision@',\n          'parameter': '50_abs',\n          'max_from_best': 0.3,\n          'threshold_value': 0.5}],\n    models_table='models',\n    distance_table='best_dist',\n    agg_type='worst'\n)\n</code></pre> Here, Auditioner drops all models group that meet at least one of these conditions in at least one training set:</p> <ul> <li>Achieves precision at least 0.3 worse than the best performing model group</li> <li>Achieves precision worse than 0.5</li> </ul> <p>Note that the <code>agg_type</code> parameter is optional for aggregating metric values across multiple models for a given <code>model_group_id</code> and <code>train_end_time</code> combination (e.g., from different random seeds) -- <code>mean</code>, <code>best</code>, or <code>worst</code> (the default)</p>"},{"location":"audition/audition_intro/#selection-rules","title":"Selection rules","text":"<p>Selection rules allow you to pare down your model groups even more.</p> <p>Start by adding rules to a <code>RuleMaker</code> object:</p> <p><pre><code>from triage.component.audition.rules_maker import SimpleRuleMaker, create_selection_grid\n\nrule = SimpleRuleMaker()\n\nrule.add_rule_best_current_value(metric='precision@', parameter='50_abs', n=3)\nrule.add_rule_best_average_value(metric='precision@', parameter='50_abs', n=3)\n</code></pre> These rules select the top 3 models </p> <ul> <li>Ranked by precision in the most recent training set</li> <li>Ranked by average precision over all training sets.</li> </ul> <p>Create a selection grid from your <code>RuleMaker</code>, and register it in your <code>Auditioner</code> object. This will generate a set of plots showing the performance of each rule, and allow you to view the top <code>n</code> models selected by each rule.</p> <pre><code>grid = create_selection_grid(rule)\n\naud.register_selection_rule_grid(grid, plot=True)\naud.selection_rule_model_group_ids\n</code></pre>"},{"location":"audition/audition_intro/#metric-filters","title":"Metric Filters","text":"<p>Auditioner implements two coarse filters for pruning worst-performing model groups.</p> <p>These filters are defined on the metrics specified by the <code>metric</code> and <code>parameter</code> arguments in the <code>initial_metric_filters</code> dict. The combination of these two arguments should refer to a metric calculated by a Triage experiment.  </p> <p><code>max_from_best</code>: Model groups that perform this much worse than the best-performing model group in any period will be pruned.</p> <p><code>threshold_value</code>: Model groups that perform worse than this threshold in any period will be pruned.</p>"},{"location":"audition/audition_intro/#adding-metric-filters","title":"Adding metric filters","text":"<p>After defining an <code>Auditioner</code> instance, we can output the models permitted by the initial thresholds.</p> <pre><code># Output the thresholded model group ids\naud.thresholded_model_group_ids\n</code></pre> <p>If that didn't thin things out too much, let's get a bit more agressive with both parameters. If we want to have multiple filters, then use <code>update_metric_filters</code> to apply a set of filters to the model groups we're considering in order to eliminate poorly performing ones. The model groups will be plotted again after updating the filters.</p> <pre><code>aud.update_metric_filters([{\n    'metric': 'precision@',\n    'parameter': '50_abs',\n    'max_from_best': 0.5,\n    'threshold_value': 0.12\n}])\naud.thresholded_model_group_ids\n</code></pre>"},{"location":"audition/audition_intro/#the-audition-cli","title":"The Audition CLI","text":"<p>Besides its Python interface, Audition exposes a full-featured CLI.</p> <p>Start by defining an Audition config file. Parameters in an Audition config map to arguments in the Python interface introduced above.</p> <pre><code>triage -d dbconfig.yaml audition --config audition_config.yaml --directory audition_output\n</code></pre> <p>This command will run Audition against the database specified in <code>dbconfig.yaml</code>, using options from <code>audition_config.yaml</code>. It will store the resulting plots in the directory <code>audition_output</code>.</p>"},{"location":"audition/model_selection/","title":"Model Selection Concepts","text":""},{"location":"audition/model_selection/#introduction","title":"Introduction","text":"<p>Model selection is the process of evaluating model groups trained on historical data, and selecting one to make predictions on future data. Audition provides a formal, repeatable model selection workflow, with flexibility allowing you to prioritize the parameters important to your project.</p> <p>Often, a production-scale model training process will generate thousands of trained models. Sifting through all of those results can be time-consuming even after calculating the usual basic metrics like precision and recall. </p> <p>Which metrics matter most? Should you prioritize the best metric value over time or treat recent data as most important? Is low metric variance important? The answers to questions like these may not be obvious up front.</p> <p>The best solution to this problem is to incorporate model selection into one's modeling pipeline, allowing the automated evaluation and selection of models for future prediction, based on their historical performance.</p> <p>This article introduces the model selection process at a conceptual level, motivating the design of Audition.</p> <p>Unfinished Article</p> <p>This article is missing some content. It could would benefit from:</p> <ul> <li>Discussion on regret &amp; selection rule evaluation</li> <li>Links to postmodeling &amp; bias</li> </ul>"},{"location":"audition/model_selection/#an-example-model-selection-problem","title":"An example model selection problem","text":"<p>We can examine the problem of model selection with a basic example.</p> <p></p> <p>This plot shows the performance of three model groups on three train/test sets. The y-axis represents each model group's performance, on a metric like precision or recall, during each period specified by the x-axis.</p> <p>Now that we've trained our model groups on historical data, we need to select a model group that we think will perform well on the next set of data (to be generated in 2017).</p> <ul> <li>You can probably eliminate the yellow triangle model right off the bat.</li> <li>If we only looked at 2016, we\u2019d choose the light blue squares model, but although it does well in 2016, it performed the worst in 2015, so we don\u2019t know if we can trust its performance \u2013 what if it dips back down in 2017? Then again, what if 2015 was just some sort of anomaly? We don\u2019t know the future (which is why we need analysis like this), but we want to give ourselves the best advantage we can.</li> <li>To balance consistency and performance, we choose a model that reliably performs well (blue circles), even if it\u2019s not always the best.</li> </ul> <p>In this imaginary example, the \u201cselection process\u201d was easy. Model 1 was the clear best option. Of course, in real life we are likely to have to choose between more than three models groups - Triage makes it easy to train grids of dozens or hundreds of models.</p>"},{"location":"audition/model_selection/#selection-rules","title":"Selection Rules","text":"<p>The goal of audition is to narrow a very large number of model groups to a small number of best candidates, ideally making use of the full time series of information. There are several ways one could consider doing so, using over-time averages of the metrics of interest, weighted averages to balance between metrics, the distance from best metrics, and balancing metric average values and stability.</p> <p>Audition formalizes this idea through by introducing the concept of \"selection rules\". A selection rule is a function that: - Takes data about the performance of a set of model groups, up to some point in time - Ranks those models based on some criteria - Returns <code>n</code> highest-ranked models</p> <p>An ideal selection rule will always return the model group that performs best in the subsequent time period. Thus, a selection rule is evaluated by its <code>regret</code>: the difference in performance between its chosen model and the best-performing model in some time period.</p> <p>You can use the <code>Auditioner</code> class to register, evaluate, and update selection rules. Audition will run simulations of different model group selection rules allowing you to assess which rule(s) is the best for your needs. </p>"},{"location":"dirtyduck/","title":"Welcome!","text":"<p>This is a guide to <code>Triage</code>, a machine learning / data science tool initially developed at the Center for Data Science and Public Policy (DSaPP) at the University of Chicago and now being maintained at Carnegie Mellon University.</p> <p><code>Triage</code> helps build ML systems for two common problems: (a) Early warning systems (EWS or EIS), (b) resource prioritization (a.k.a \"an inspections problem\"). These problems require careful thought and design and their formulation and implementation are often done incorrectly.</p> <p>Info</p> <p>This tutorial may not be compatible  with the latest version of <code>triage</code> and was written for v4.2.0. We recommend starting with the tutorial hosted at colab</p> <p>How you can help to improve this tutorial</p> <p>If you want to contribute, please follow the suggestions in the triage\u2019s github repository.</p>"},{"location":"dirtyduck/#why-dirty-duck","title":"Why Dirty Duck??","text":"<p>There is a famous (and delicious) peking duck restaurant in Chicago called Sun Wah. We love that place, and as every restaurant in Chicago area, it gets inspected, so the naming is an homage to them.</p>"},{"location":"dirtyduck/#who-is-this-tutorial-for","title":"Who is this tutorial for?","text":"<p>We created this tutorial with two roles in mind:</p> <ul> <li> <p>Data scientists/ML practitioners who want to focus on the problem they are tackling, and not on the nitty-gritty details about how to configure and setup a Machine learning pipeline, model governance, reproducibility, model selection, etc.</p> </li> <li> <p>analytical policy team without too deep of a technical/engineering background who want to   learn how to formulate their policy problems as  Machine Learning   problems.</p> </li> </ul>"},{"location":"dirtyduck/#how-to-use-this-tutorial","title":"How to use this tutorial","text":"<p>First, clone this repository on your machine</p> <pre><code>git clone https://github.com/dssg/triage\n</code></pre> <p>Second, in the cloned repository's top-level directory run</p> <pre><code>./tutorial.sh up\n</code></pre> <p>This will take several minutes the first time you do it.</p> <p>After this, you may decide to do the quickstart tutorial.</p>"},{"location":"dirtyduck/#before-you-start","title":"Before you start","text":""},{"location":"dirtyduck/#what-you-need-for-this-tutorial","title":"What you need for this tutorial","text":"<p>Install Docker CE and Docker Compose. That's it! Follow the links for the installation instructions.</p> <p>Note that if you are using <code>GNU/Linux</code> you should add your user to the <code>docker</code> group following the instructions at this link.</p> <p>At the moment only operating systems with *nix-type command lines are supported, such as <code>GNU/Linux</code> and <code>MacOS</code>. Recent versions of <code>Windows</code> may also work.</p>"},{"location":"dirtyduck/aws_batch/","title":"Scaling out: AWS Batch","text":"<p>If your laptop choked in the previous sections or if you can't afford to look your laptop just lagging forever, you should read this section\u2026</p> <p>For bigger experiment, one option is use <code>[[https://aws.amazon.com/batch/][AWS Batch]]</code>. AWS Batch dynamically provisions the optimal quantity and type of compute resources based on the specific resource requirements of the tasks submitted. AWS Batch will manage (i.e. plans, schedules, and executes) the resources (CPU, Memory) that we need to run the pipeline. In other words, AWS Batch will provide you with a computer (an AWS EC2 machine) that satisfies your computing requirements, and then it will execute the software that you intend to run.</p> <p>AWS Batch dependes in other two technologies in order to work: Elastic Container Registry (Amazon ECR) as the Docker image registry (allowing AWS Batch to fetch the task images), and Elastic Compute Cloud (Amazon EC2) instances located in the cluster as the docker host (allowing AWS Batch to execute the task).</p> <p></p> <p>An AWS ECS task will be executed by an EC2 instance belonging to the ECS cluster (if there are resources available). The EC2 machine operates as a Docker host: it will run the task definition, download the appropriate image from the ECS registry, and execute the container.</p>"},{"location":"dirtyduck/aws_batch/#what-do-you-need-to-setup","title":"What do you need to setup?","text":"<p>AWS Batch requires setup the following infrastructure:</p> <ul> <li>An AWS S3 bucket for storing the original data and the successive transformations of it made by the pipeline.</li> <li>A PostgreSQL database (provided by AWS RDS) for storing the data in a relational form.</li> <li>An Elastic Container Registry (AWS ECR) for storing the triage's Docker image used in the pipeline.</li> <li>AWS Batch Job Queue configured and ready to go.</li> </ul>"},{"location":"dirtyduck/aws_batch/#assumptions","title":"Assumptions","text":"<ul> <li>You have <code>[[https://stedolan.github.io/jq/download/][jq]]</code> installed</li> <li>You have IAM credentials with permissions to run AWS Batch, read AWS S3 and create AWS EC2 machines.</li> <li>You installed <code>awscli</code> and configure your credentials following the standard instructions.</li> <li>You have access to a S3 bucket:</li> <li>You have a AWS ECR repository with the following form: <code>dsapp/triage-cli</code></li> <li>You have a AWS Batch job queue configured and have permissions for adding, running, canceling jobs.</li> </ul> <p>You can check if you have the <code>AWS S3</code> permissions like:</p> <pre><code>[AWS_PROFILE=your_profile] aws ls [your-bucket]   # (dont't forget the last backslash)\n</code></pre> <p>And for the <code>AWS Batch</code> part:</p> <pre><code>[AWS_PROFILE=your_profile] aws batch describe-job-queues\n[AWS_PROFILE=your_profile] aws batch describe-job-definitions\n</code></pre>"},{"location":"dirtyduck/aws_batch/#configuration","title":"Configuration","text":"<p>First we need to customize the file <code>.aws_env</code> (yes, another environment file).</p> <p>Copy the file <code>aws_env.example</code> to <code>.aws_env</code> and fill the blanks</p> <p>NOTE: Don't include the <code>s3://</code> protocol prefix in the <code>S3_BUCKET</code></p>"},{"location":"dirtyduck/aws_batch/#local-environment-variables","title":"(Local) Environment variables","text":"<pre><code>#!/usr/bin/env bash\n\nPROJECT_NAME=dirtyduck\nTRIAGE_VERSION=3.3.0\nENV=development\nAWS_REGISTRY={your-ecr-registry}\nAWS_JOB_QUEUE={your-job-queue}\nPOSTGRES_DB={postgresql://user:password@db_server/dbname}\nS3_BUCKET={your-bucket}\n</code></pre> <p>To check if everything is correct you can run:</p> <pre><code>[AWS_PROFILE=your_profile]  ./deploy.sh -h\n</code></pre> <p>Next, we need 3 files for running in AWS Batch, copy the files and remove the <code>.example</code> extension and adapt them to your case:</p>"},{"location":"dirtyduck/aws_batch/#job-definition","title":"Job definition","text":"<p>Change the <code>PROJECT_NAME</code> and <code>AWS_ACCOUNT</code> for their real values</p> <pre><code>{\n  \"containerProperties\": {\n    \"command\": [\n      \"--tb\",\n      \"Ref::experiment_file\",\n      \"--project-path\",\n      \"Ref::output_path\",\n      \"Ref::replace\",\n      \"Ref::save_predictions\",\n      \"Ref::profile\",\n      \"Ref::validate\"\n    ],\n    \"image\": \"AWS_ACCOUNT.dkr.ecr.us-west-2.amazonaws.com/YOUR_TRIAGE_IMAGE\",\n    \"jobRoleArn\": \"arn:aws:iam::AWS_ACCOUNT:role/dsappBatchJobRole\",\n    \"memory\": 16000,\n    \"vcpus\": 1\n  },\n  \"jobDefinitionName\": \"triage-cli-experiment\",\n  \"retryStrategy\": {\n    \"attempts\": 1\n  },\n  \"type\": \"container\"\n}\n</code></pre>"},{"location":"dirtyduck/aws_batch/#environment-variables-overrides-for-docker-container-inside-the-aws-ec2","title":"Environment variables overrides (for docker container inside the AWS EC2)","text":"<p>Fill out the missing values</p> <pre><code>{\n    \"environment\": [\n        {\n            \"name\":\"AWS_DEFAULT_REGION\",\n            \"value\":\"us-west-2\"\n        },\n        {\n            \"name\":\"AWS_JOB_QUEUE\",\n            \"value\":\"\"\n        },\n        {\n            \"name\":\"POSTGRES_PASSWORD\",\n            \"value\":\"\"\n        },\n        {\n            \"name\":\"POSTGRES_USER\",\n            \"value\":\"\"\n        },\n        {\n            \"name\":\"POSTGRES_DB\",\n            \"value\":\"\"\n        },\n        {\n            \"name\":\"POSTGRES_PORT\",\n            \"value\":\"\"\n        },\n        {\n            \"name\":\"POSTGRES_HOST\",\n            \"value\":\"\"\n        }\n    ]\n}\n</code></pre>"},{"location":"dirtyduck/aws_batch/#credentials-filter","title":"<code>credentials-filter</code>","text":"<p>Leave this file as is (We will use it for storing the temporal token in <code>deploy.sh</code>)</p> <pre><code>{\n        \"environment\": [\n                {\n                        \"name\": \"AWS_ACCESS_KEY_ID\",\n                        \"value\": .Credentials.AccessKeyId\n                },\n                {\n                        \"name\": \"AWS_SECRET_ACCESS_KEY\",\n                        \"value\": .Credentials.SecretAccessKey\n                },\n                {\n                        \"name\": \"AWS_SESSION_TOKEN\",\n                        \"value\": .Credentials.SessionToken\n                }\n        ]\n}\n</code></pre>"},{"location":"dirtyduck/aws_batch/#running-an-experiment","title":"Running an experiment","text":"<p>We provided a simple bash file for creating the image, uploading/updating the job definition and running the experiment:</p> <pre><code>    ./deploy.sh -h\n\n    Usage: ./deploy.sh (-h | -i | -u | -b | -r | -a | --sync_{to,from}_s3 )\n    OPTIONS:\n       -h|--help                   Show this message\n       -i|--info                   Show information about the environment\n       -b|--update-images          Build the triage image and push it to the AWS ECR\n       -u|--update-jobs            Update the triage job definition in AWS Batch\n       -r|--run-experiment         Run experiments on chile-dt data\n       --sync-to-s3                Uploads the experiments and configuration files to s3://your_project\n       --sync-from-s3              Gets the experiments and configuration files from s3://your_project\n    EXAMPLES:\n       Build and push the images to your AWS ECR:\n            $ ./deploy.sh -b\n       Update the job's definitions:\n            $ ./deploy.sh -u\n       Run triage experiments:\n            $ ./deploy.sh -r --experiment_file=s3://your_project/experiments/test.yaml,project_path=s3://your_project/triage,replace=--replace\n</code></pre> <p>If you have multiple AWS profiles use <code>deploy.sh</code> as follows:</p> <pre><code>[AWS_PROFILE=your_profile] ./deploy.sh -r [job-run-name] experiment_file=s3://{your_bucket}/experiments/simple_test_skeleton.yaml,output_path=s3://{your_bucket}/triage,replace=--no-replace,save_predictions=--no-save-predictions,profile=--profile,validate=--validate\n</code></pre> <p>Where <code>your_profile</code> is the name of the profile in <code>~/.aws/credentials</code></p>"},{"location":"dirtyduck/aws_batch/#suggested-workflow","title":"Suggested workflow","text":"<p>The workflow now is:</p> <ul> <li> <p>At the beginning of the project</p> <ul> <li>Set a <code>docker image</code> and publish it to the AWS ECR (if needed, or you can use the <code>triage</code> official one).</li> </ul> <p>You could create different images if you want to run something more tailored to you (like not using the <code>cli</code> interface)</p> <ul> <li>Create a job definition and publish it:</li> </ul> <pre><code>[AWS_PROFILE=your_profile] ./deploy.sh -u\n</code></pre> <p>You could create different jobs if, for example, you want to have different resources (maybe small resources for testing or a lot of resources for a big experiment)</p> </li> <li> <p>Every time that you have an idea about how to improve the results</p> <ul> <li>Create experiment files and publish them to the <code>s3</code> bucket:</li> </ul> <pre><code>[AWS_PROFILE=your_profile] ./deploy.sh --synt-to-s3\n</code></pre> <ul> <li>Run the experiments</li> </ul> <pre><code>[AWS_PROFILE=your_profile] ./deploy.sh -r [job-run-name] experiment_file=s3://{your_bucket}/experiments/simple_test_skeleton.yaml,output_path=s3://{your_bucket}/triage,replace=--no-replace,save_predictions=--no-save-predictions,profile=--profile,validate=--validate\n</code></pre> </li> </ul>"},{"location":"dirtyduck/choose_your_own_adventure/","title":"How to use this tutorial?","text":"<ul> <li>You are interested in the learn how to use <code>triage</code> and have a lot of time:<ul> <li>Problem description</li> <li>Infrastructure</li> <li>Data preparation</li> <li>Resource prioritization</li> <li>Early warning systems</li> <li>A deeper look into triage</li> <li>Scaling up</li> </ul> </li> <li>You want to know about <code>triage</code><ul> <li>A deeper look into triage</li> <li>Model governance</li> <li>Model selection</li> </ul> </li> <li>You want to learn about case studies<ul> <li>Quick setup</li> <li>Resource prioritization and/or Early warning systems</li> </ul> </li> <li>You already know <code>triage</code> but want to use it on the cloud<ul> <li>Scaling up</li> </ul> </li> <li>You just want to use the database for your own purposes<ul> <li>Quick setup</li> </ul> </li> </ul>"},{"location":"dirtyduck/data_preparation/","title":"Data preparation","text":"<p>We need to get the data and transform it into a shape that is suitable for the analysis.</p> <p>NOTE: Unless we say otherwise, you should run all the following commands inside <code>bastion</code>.</p>"},{"location":"dirtyduck/data_preparation/#load-the-data","title":"Load the data","text":"<p>Before loading the data into the database, verify that the database table is empty by running the following code:</p> <pre><code>select\n    count(*)\nfrom\n    raw.inspections;\n</code></pre> count 0 <p>We will use some <code>postgresql</code> magic in order to get the data in our database. In particular we will use the powerful copy command and the City of Chicago's data API:</p> <pre><code>\\copy raw.inspections from program 'curl \"https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD\"' HEADER CSV\n</code></pre> <p>Now, you should have some data:</p> <pre><code>select\n    to_char(count(*), '999,999') as \"facilities inspected\"\nfrom\n    raw.inspections;\n</code></pre> facilities inspected 186,426 <p>You'll probably get a different number because the data are updated every day. Let's peek inside the table<sup>1</sup></p> <pre><code>select\n    inspection,\n    dba_name,\n    risk,\n    results\nfrom\n    raw.inspections limit 1;\n</code></pre> inspection dba_name risk results 2268241 ANTOJITOS PUEBLITA INC Risk 1 (High) Pass w/ Conditions <p>Ok, now you have some data loaded! But we still need to munge it to use it in our machine learning task.</p>"},{"location":"dirtyduck/data_preparation/#transforming-and-cleaning-the-data","title":"Transforming (and cleaning) the data","text":""},{"location":"dirtyduck/data_preparation/#rationale","title":"Rationale","text":"<p>To tackle a machine learning problem, you need to identify the entities for your problem domain. Also, if your problem involves time, you will need to understand how those entities change, either what events happen to the entity or what events the entity affects.</p> <p>We will encode this information into two tables, one named <code>entities</code> and the other named <code>events</code>.</p> <p>The entity, in this example, is the food facility, and the events are the inspections on the facility.</p> <p>The <code>entities</code> table should contain a unique identifier for the entity and some data about that entity (like name, age and status). The <code>events</code> table will include data related to the inspection, including the two most important attributes: its spatial and temporal positions <sup>2</sup>.</p> <p>Before we start the data cleaning, make your life easier by following this rule:</p> <p>Important</p> <p>Do not change the original data</p> <p>The reason is, if you make a mistake or want to try a different data transformation, you can always can go back to the <code>raw</code> data and start over.</p>"},{"location":"dirtyduck/data_preparation/#data-road","title":"Data road","text":"<p>The transformation \"road\" that we will take in this tutorial is as follows:</p> <ol> <li>Put a copy of the data in the <code>raw</code> schema. (We just did that.)</li> <li>Apply some simple transformations and store the resulting data in the <code>cleaned</code> schema.</li> <li>Organize the data into two unnormalized<sup>3</sup> tables in the     semantic schema: <code>events</code> and <code>entities</code>.</li> <li>Run <code>triage</code>. It will create several schemas (<code>triage_metadata</code>, <code>test_results</code>, <code>train_results</code>).</li> </ol> <p></p> <p></p>"},{"location":"dirtyduck/data_preparation/#dataset-documentation","title":"Dataset documentation","text":"<p>Info</p> <p>For an updated version of the documentation of this dataset see Food Protection Services.</p> <p>Info</p> <p>The Food Code Rules (effective 2/1/2019) could be consulted here.</p> <p>The Chicago Food Inspection dataset has documentation here.</p> <p>We can use this documentation to better understand each column's meaning, and the process that generates the data.</p> <p>Most columns are self-explanatory, but some are not<sup>4</sup>:</p> <p>Risk category of facility (<code>risk</code>)</p> <p>Each establishment is categorized by its risk of adversely affecting the public\u2019s health, with 1 being the highest and 3 the lowest. The frequency of inspection is tied to this risk, with risk = 1 establishments inspected most frequently and risk = 3 least frequently.</p> <p>Inspection type (<code>type</code>)</p> <p>An inspection can be one of the following types:</p> <ul> <li>Canvass, the most common type of inspection performed at a frequency relative to the     risk of the establishment;</li> <li>Consultation, when the inspection is done at the request of the     owner prior to the opening of the establishment;</li> <li>Complaint, when the inspection is done in response to  a     complaint against the establishment</li> <li>License, when the inspection  is done as a requirement for the     establishment to receive its license to operate;</li> <li>Suspect food poisoning, when the inspection is done in response     to one or more persons claiming to have gotten ill as a result of     eating at the establishment (a specific type of  complaint-based inspection);</li> <li>Task-force inspection, when an inspection of a bar or tavern is     done.</li> <li>Re-inspections can occur for most types of these inspections and are indicated as such.</li> </ul> <p>Results (<code>results</code>)</p> <p>An inspection can pass, pass with conditions, or fail. Establishments receiving a \u2018pass\u2019 were found to have no critical or serious violations (violation number 1-14 and 15-29, respectively). Establishments receiving a \u2018pass with conditions\u2019 were found to have critical or serious violations, but these were corrected during the inspection. Establishments receiving a \u2018fail\u2019 were found to have critical or serious violations that were not correctable during the inspection. An establishment receiving a \u2018fail\u2019 does not necessarily mean the establishment\u2019s licensed is suspended. Establishments found to be out of business or not located are indicated as such.</p> <p>Important!</p> <p>The result of the inspections (pass, pass with conditions or fail) as well as the violations noted are based on the findings identified and reported by the inspector at the time of the inspection, and may not reflect the findings noted at other times.</p> <p>Violations (<code>violations</code>)</p> <p>An establishment can receive one or more of 45 distinct  violations (violation numbers 1-44 and 70). For each violation  number listed for a given establishment, the requirement the  establishment must meet in order for it to NOT receive a  violation is noted, followed by a specific description of the  findings that caused the violation to be issued.</p> <p>Data Changes</p> <p>On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here.</p> <p>Data Changes</p> <p>On 2/1/2019 the Chicago Department of Public Health\u2019s Food Protection unit changed the requirements that the facilities must follow. See here</p> <p>We added emphasis to the last one.</p> <p>From these definitions, we can infer the following:</p> <ol> <li>risk is related to the frequency of inspections of type canvass.</li> <li>consultation is an inspection before the facility opens (so we can remove it from the data). The same happens with license.</li> <li>complaint and suspected food poisoning are triggered by people.</li> <li>consultation is triggered by the owner of the facility.</li> <li>task-force occurs at bars or taverns.</li> <li>Critical violations are coded between <code>1-14</code>, serious     violations between <code>15-29</code>. We can assume that the violations     code <code>30</code> and higher are minor violations. (see below)</li> <li>violation describes the problems found, and the comment section     describes the steps the facility should take to fix the problem.</li> <li>There are only three possible results of the inspection. (Also, an     inspection may not happen if the facility was not located or went     out of business).</li> <li>There can be several <code>violations</code> per <code>inspection</code>.</li> </ol> <p>Data Changes</p> <p>On 7/1/2018 Critical violation changed to Priority (P) Violation, Serious violation changed to Priority Foundation (PF) Violation and Minor violation changed to Core (C) Violation.</p> <p>Data Changes</p> <p>On 7/1/2018 the number of potential violations has increased from 45 to 63.</p> <p>Data Changes</p> <p>On 7/1/2018 Corrected Dduring Inspection (CDI) has been changed to Corrected on Site (COS). Potentially Hazardous Foods (PHF) changed to Time/Temperature Control for Safety Foods (TCS Foods).</p>"},{"location":"dirtyduck/data_preparation/#reality-check","title":"Reality check","text":"<p>It is important to verify that the documentation is correct. Let's start by checking that the <code>risk</code> column only has three classifications:</p> <p>NOTE Execute this in <code>psql</code> inside the container <code>bastion</code>.</p> <pre><code>  select\n      risk,\n      to_char(count(*), '999,999') as \"number of inspections\"\n  from\n      raw.inspections\n  group by\n      risk\n  order by\n      count(*) desc;\n</code></pre> risk number of inspections Risk 1 (High) 133,170 Risk 2 (Medium) 36,597 Risk 3 (Low) 16,556 \u00a4 75 All 28 <p>Ok, there are two extra <code>risk</code> types, <code>All</code> and <code>NULL</code>, for a grand total of 5.</p> <p>What about <code>types</code> of inspections?</p> <pre><code>  select\n      count(distinct type) as \"types of inspections\"\n  from\n      raw.inspections;\n</code></pre> types of inspections 108 <p>Wow, there are 108 types of inspections instead of the expected 5!</p> <p>What are those types? How bad is it?</p> <pre><code>select\n    type,\n    to_char(count(*), '999,999') as \"number of inspections\"\nfrom\n    raw.inspections\ngroup by\n    type\norder by\n    count(*) desc\n    limit 10;\n</code></pre> type number of inspections Canvass 99,792 License 24,385 Canvass Re-Inspection 19,380 Complaint 17,289 License Re-Inspection 8,572 Complaint Re-Inspection 7,060 Short Form Complaint 6,534 Suspected Food Poisoning 834 Consultation 671 License-Task Force 605 <p>This column will require also cleaning.</p> <p>Finally, let's look <code>results</code> (should be 3)</p> <pre><code>  select\n      results,\n      to_char(count(*), '999,999') as \"number of inspections\"\n  from\n      raw.inspections\n  group by\n      results\n  order by\n      count(*) desc;\n</code></pre> results number of inspections Pass 103,528 Fail 35,948 Pass w/ Conditions 23,258 Out of Business 16,212 No Entry 5,784 Not Ready 1,630 Business Not Located 66 <p>Ok, disheartening. But that's the reality of real data. We'll try to clean this mess.</p>"},{"location":"dirtyduck/data_preparation/#cleaning","title":"Cleaning","text":"<p>Let's look at the data to figure out how we need to transform it. We'll start with all the columns except <code>violations</code>. We'll deal with that one later because it's more complex.</p> <p>First, we'll remove superfluous spaces; convert the columns <code>type, results, dba_name, aka_name, facility_type, address, city</code> to lower case; and clean <code>risk</code>, keeping only the description (e.g. <code>high</code> instead of <code>Risk 1 (High)</code>).</p> <p>We still need to clean further the column <code>type</code> (which contains more values than the seven mentioned in the documentation: canvass, complaint, license, re-inspection, task-force, consultation, and suspected food poisoning). For simplicity, we will use regular expressions and ignore re-inspection.</p> <p>For the column <code>risk</code>, we will impute as <code>high</code> all the <code>NULL</code> and <code>All</code> values<sup>5</sup>.</p> <p>As we have seen (and will continue see) through this tutorial, real data are messy; for example, the column <code>dba_name</code> has several spellings for the same thing: <code>SUBWAY</code> and <code>Subway</code>, <code>MCDONALDS</code> and <code>MC DONALD'S</code>, <code>DUNKIN DONUTS/BASKIN ROBBINS</code> and <code>DUNKIN DONUTS / BASKIN ROBBINS</code>, etc.</p> <p>We could use soundex or machine learning deduplication<sup>6</sup> to clean these names, but we'll go with a very simple cleaning strategy: convert all the names to lowercase, remove the trailing spaces, remove the apostrophe, and remove the spaces around \"<code>/</code>\". It won't completely clean those names, but it's good enough for this example project.</p> <p>Let's review the status of the spatial columns (<code>state, city, zip, latitude, longitude</code>). Beginning with <code>state</code>, all the facilities in the data should be located in Illinois:</p> <pre><code>select\n    state,\n    to_char(count(*), '999,999') as \"number of inspections\"\nfrom\n    raw.inspections\ngroup by\n    state;\n</code></pre> state number of inspections IL 186,392 \u00a4 34 <p>Ok, almost correct, there are some <code>NULL</code> values. We will assume that the <code>NULL</code> values are actually <code>IL</code> (i.e. we will impute them). Moving to the next spatial column, we expect that all the values in the column <code>city</code> are Chicago<sup>7</sup>:</p> <pre><code>select\n    lower(city) as city,\n    to_char(count(*), '999,999') as \"number of inspections\"\nfrom\n    raw.inspections\ngroup by\n    lower(city)\norder by\n    count(*) desc\n    limit 10;\n</code></pre> city number of inspections chicago 186,009 \u00a4 161 cchicago 44 schaumburg 23 maywood 16 elk grove village 13 evanston 10 chestnut street 9 cicero 9 inactive 8 <p>Oh boy. There are 150-ish rows with <code>NULL</code> values and forty-ish rows with the value <code>cchicago</code>. Farther down the list (if you dare), we even have <code>chicagochicago</code>. All the values are near Chicago, even if they're in different counties, so we will ignore this column (or equivalently, we will assume that all the records are from Chicago).</p> <p>Zip code has a similar <code>NULL</code> problem:</p> <pre><code>select\n    count(*) as \"number of inspections w/o zip code\"\nfrom\n    raw.inspections\nwhere\n    zip is null or btrim(zip) = '';\n</code></pre> number of inspections w/o zip code 75 <p>We could attempt to replace these <code>NULL</code> values using the location point or using similar names of restaurants, but for this tutorial we will remove them. Also, we will convert the coordinates latitude and longitude to a Postgres <code>Point</code><sup>8</sup><sup>9</sup><sup>10</sup>.</p> <p>We will drop the columns <code>state</code>, <code>latitude</code>, and <code>longitude</code> because the <code>Point</code> contains all that information. We also will remove the column <code>city</code> because almost everything happens in Chicago.</p> <p>If you're keeping count, we are only keeping two columns related to the spatial location of the events: the location of the facility (<code>location</code>) and one related to inspection assignments (<code>zip_code</code>).</p> <p>Additionally, we will keep the columns <code>wards, historical_wards, census_tracts</code> and <code>community_areas</code>.</p> <p>Each inspection can have multiple violations. To handle that as simply as possible, we'll put violations in their own table.</p> <p>Decisions regarding data</p> <p>We will inspections that occurred before <code>2018-07-01</code>. This is due the changes in the types and definition of the violations. See here</p> <p>Finally, we will improve the names of the columns (e.g. <code>results -&gt; result, dba_name -&gt; facility</code>, etc).</p> <p>We will create a new <code>schema</code> called <code>cleaned</code>. The objective of this schema is twofold: to keep our raw data as is<sup>11</sup> and to store our assumptions and cleaning decisions separate from the raw data in a schema that semantically transmits that \"this is our cleaned data.\"</p> <p>The <code>cleaned</code> schema will contain two tables: <code>cleaned.inspections</code> and <code>cleaned.violations</code>.</p> <pre><code>  create schema if not exists cleaned;\n</code></pre> <p>Then, we will create our mini ETL with our cleaning decisions:</p> <p>Data changes</p> <p>At least from May 2019 the dataset contains news columns: <code>zip_codes, historical_wards, wards, community_areas</code> and <code>census_tracts</code>. The most recent code reflects those changes.</p> <pre><code>create schema if not exists cleaned;\n\ndrop table if exists cleaned.inspections cascade;\n\ncreate table cleaned.inspections as (\n        with cleaned as (\n        select\n            inspection::integer,\n            btrim(lower(results)) as result,\n            license_num::integer,\n            btrim(lower(dba_name)) as facility,\n            btrim(lower(aka_name)) as facility_aka,\n            case when\n            facility_type is null then 'unknown'\n            else btrim(lower(facility_type))\n            end as facility_type,\n            lower(substring(risk from '\\((.+)\\)')) as risk,\n            btrim(lower(address)) as address,\n            zip as zip_code,\n            community_areas as community_area,\n            census_tracts as census_tracts,\n            historical_wards as historical_ward,\n            wards as ward,\n            substring(\n                btrim(lower(regexp_replace(type, 'liquor', 'task force', 'gi')))\n            from 'canvass|task force|complaint|food poisoning|consultation|license|tag removal') as type,\n            date,\n            -- point(longitude, latitude) as location\n            ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)::geography as location  -- We use geography so the measurements are in meters\n        from raw.inspections\n        where zip is not null  -- removing NULL zip codes\n        and date &lt; '2018-07-01'\n            )\n\n    select * from cleaned where type is not null\n        );\n</code></pre> <p>The number of inspections now is:</p> <pre><code>select\n    to_char(count(inspection), '999,999,999') as \"number of\n    inspections (until 07/01/2018)\"\nfrom cleaned.inspections;\n</code></pre> number of inspections (until 07/01/2018) 172,052 <p>Note that quantity is smaller than the one from <code>raw.inspections</code>, since we throw away some inspections.</p> <p>With the <code>cleaned.inspections</code> table created, let's take a closer look at the <code>violations</code> column to figure out how to clean it.</p> <p>The first thing to note is that the column <code>violation</code> has a lot of information: it describes the code violation, what's required to address it (see Dataset documentation), and the inspector's comments. The comments are free text, which means that they can contain line breaks, mispellings, etc. In particular, note that pipes (<code>|</code>) seperate multiple violations.</p> <p>The following <code>sql</code> code removes line breaks and multiple spaces and creates an array with all the violations for inspection number <code>2145736</code>:</p> <pre><code>select\n    unnest(string_to_array(regexp_replace(violations, '[\\n\\r]+', ' ', 'g' ), '|'))  as violations_array\nfrom raw.inspections\nwhere\n    inspection = '2145736';\n</code></pre> violations<sub>array</sub> 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: FIRST FLOOR GIRL'S WASHROOM,MIDDLE WASHBOWL SINK FAUCET NOT IN GOOD REPAIR, MUST REPAIR AND MAINTAIN.   ONE OUT OF TWO HAND DRYER NOT WORKING IN THE FOLLOWING WASHROOM: FIRST FLOOR  BOY'S AND GIRL'S WASHROOM, AND  BOY'S AND GIRL'S WASHROOM 2ND FLOOR. MUST REPAIR AND MAINTAIN. 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: DAMAGED FLOOR INSIDE THE BOY'S AND GIRL'S WASHROOM 2ND FLOOR. MUST REPAIR, MAKE THE FLOOR SMOOTH EASILY CLEANABLE. 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: MISSING PART OF THE COVING(BASEBOARD) BY THE EXPOSED HAND SINK IN THE KITCHEN. MUST REPAIR AND MAINTAIN.   WATER STAINED CEILING TILES IN THE LUNCH ROOM. MUST REPLACE CEILING TILES AND MAINTAIN.  PEELING PAINT ON THE CEILING AND WALLS THROUGHOUT THE SCHOOL. HALLWAYS, INSIDE THE CLASSROOMS, INSIDE THE WASHROOMS IN ALL FLOORS. INSTRUCTED TO SCRAPE PEELING PAINT AND RE PAINT. <p>This little piece of code is doing a lot: first it replaces all the line breaks <code>[\\n\\r]+</code> with spaces, then, it splits the string using the pipe and stores it in an array (<code>string_to_array</code>), finally it returns every violation description in a row (<code>unnest</code>).</p> <p>From this, we can learn that the structure of the <code>violations</code> column follows:</p> <ul> <li>If there are several violations reported, those violations will be separated by <code>'|'</code></li> <li>Every violation begins with a code and a description</li> <li>Every violation can have comments, which appear after the string <code>- Comments:</code></li> </ul> <p>We will create a new table called <code>cleaned.violations</code> to store</p> <ul> <li>inspection</li> <li>code</li> <li>description</li> <li>comments</li> </ul> <pre><code>   drop table if exists cleaned.violations cascade;\n\n   create table cleaned.violations as (\n   select\n       inspection::integer,\n       license_num::integer,\n       date::date,\n       btrim(tuple[1]) as code,\n       lower(btrim(tuple[2])) as description,\n       lower(btrim(tuple[3])) as comment,\n       (case\n           when btrim(tuple[1]) = '' then NULL\n           when btrim(tuple[1])::int between 1 and 14 then 'critical' -- From the documentation\n           when btrim(tuple[1])::int between 15 and 29  then 'serious'\n           else 'minor'\n           end\n           ) as severity from\n       (\n       select\n           inspection,\n           license_num,\n           date,\n           regexp_split_to_array(   -- Create an array we will split the code, description, comment\n               regexp_split_to_table( -- Create a row per each comment we split by |\n                   coalesce(            -- If there isn't a violation add '- Comments:'\n                       regexp_replace(violations, '[\\n\\r]+', '', 'g' )  -- Remove line breaks\n                       , '- Comments:')\n                   , '\\|')  -- Split the violations\n               , '(?&lt;=\\d+)\\.\\s*|\\s*-\\s*Comments:')  -- Split each violation in three\n            -- , '\\.\\s*|\\s*-\\s*Comments:')  -- Split each violation in three (Use this if your postgresql is kind off old\n           as tuple\n       from raw.inspections\n       where results in ('Fail', 'Pass', 'Pass w/ Conditions') and license_num is not null\n           ) as t\n       );\n</code></pre> <p>This code is in <code>/sql/create_violations_table.sql</code>.</p> <p>We can verify the result of the previous script</p> <pre><code>select\n    inspection, date, code, description\nfrom cleaned.violations\nwhere\n    inspection = 2145736\norder by\n    code asc;\n</code></pre> inspection date code description 2145736 2018-03-01 32 food and non-food contact surfaces properly designed, constructed and maintained 2145736 2018-03-01 34 floors: constructed per code, cleaned, good repair, coving installed, dust-less cleaning methods used 2145736 2018-03-01 35 walls, ceilings, attached equipment constructed per code: good repair, surfaces clean and dust-less cleaning methods <p>If everything worked correctly you should be able to run the following code<sup>12</sup>:</p> <pre><code>select\n    case\n    when\n    grouping(severity) = 1 then 'TOTAL'\n    else\n    severity\n    end as severity,\n    to_char(count(*), '999,999,999') as \"number of inspections\"\nfrom\n    cleaned.violations\ngroup by\n    rollup (severity)\norder by\n    severity nulls first;\n</code></pre> severity number of inspections \u00a4 26,415 critical 51,486 minor 478,340 serious 55,583 TOTAL 611,824 <p>As a last step, we should create from the cleaned tables the <code>entities</code> and <code>events</code> tables.</p>"},{"location":"dirtyduck/data_preparation/#semantic-tables","title":"Semantic tables","text":""},{"location":"dirtyduck/data_preparation/#entities-table","title":"Entities table","text":"<p>The <code>entities</code> table should uniquely identify each facility and contain descriptive attributes. First, we should investigate how we can uniquely identify a facility. Let's hope it's easy<sup>13</sup>.</p> <p>Let's start with the obvious option. Perhaps <code>license_num</code> is a unique identifier. Let's confirm our hypothesis with some queries.</p> <p>We will begin with the following query: What are 5 licenses with the most inspections?</p> <pre><code>select\n    license_num,\n    to_char(count(*), '999,999,999') as \"number of inspections\",\n    coalesce(count(*) filter (where result = 'fail'), 0)\n    as \"number of failed inspections\"\nfrom\n    cleaned.inspections\ngroup by\n    license_num\norder by\n     count(*) desc\n    limit 5;\n</code></pre> license<sub>num</sub> number of inspections number of failed inspections 0 442 111 1354323 192 1 14616 174 31 1574001 82 4 1974745 59 3 <p>This looks weird. There are three license numbers, in particular license number <code>0</code>, that have many more inspections than the rest. Let's investigate <code>license_num</code> = <code>0</code>.</p> <pre><code>  select\n      facility_type,\n      count(*) as \"number of inspections\",\n      coalesce(count(*) filter (where result = 'fail'), 0) as \"number of failed inspections\"\n  from\n      cleaned.inspections\n  where\n      license_num=0\n  group by\n      facility_type\n  order by\n      \"number of inspections\" desc\n  limit 10;\n</code></pre> facility<sub>type</sub> number of inspections number of failed inspections restaurant 103 43 special event 70 8 unknown 44 10 shelter 31 6 navy pier kiosk 30 4 church 30 3 grocery store 16 7 school 13 1 long term care 11 2 church kitchen 11 4 <p>It seems that <code>license_number</code> <code>0</code> is a generic placeholder: Most of these are related to special events, churches, festivals, etc. But what about the <code>restaurants</code> that have <code>license_num</code> = <code>0</code>? Are those the same restaurant?</p> <pre><code>  select\n      license_num,\n      facility,\n      address,\n      count(*) as \"number of inspections\",\n      coalesce(count(*) filter (where result = 'fail'), 0)\n      as \"number of failed inspections\"\n  from\n      cleaned.inspections\n  where\n      license_num = 0\n      and\n      facility_type = 'restaurant'\n  group by\n      license_num, facility, address\n  order by\n      \"number of inspections\" desc\n  limit 10;\n</code></pre> license<sub>num</sub> facility address number of inspections number of failed inspections 0 british airways 11601 w touhy ave 5 1 0 rib lady 2 4203 w cermak rd 4 3 0 taqueria la capital 3508 w 63<sup>rd</sup> st 3 1 0 nutricion familiar 3000 w 59<sup>th</sup> st 3 1 0 salvation army 506 n des plaines st 3 1 0 herbalife 6214 w diversey ave 3 2 0 la michoacana 4346 s california ave 3 1 0 las quecas 2500 s christiana ave 3 1 0 mrs ts southern fried chicken 3343 n broadway 3 1 0 unlicensed 7559 n ridge blvd 3 1 <p>Nope. Unfortunately, <code>license_num</code> is not a unique identifier.</p> <p>Perhaps <code>license_num</code> and <code>address</code> are a unique identifier.</p> <pre><code>  select\n  to_char(count(distinct license_num), '999,999') as \"number of licenses\",\n  to_char(count(distinct facility), '999,999') as \"number of facilities\",\n  to_char(count(distinct address), '999,999') as \"number of addresses\"\n  from cleaned.inspections;\n</code></pre> number of licenses number of facilities number of addresses 34,364 25,371 17,252 <p>We were expecting (naively) that we should get one <code>license_num</code> per <code>facility</code> per <code>address</code>, but that isn't the case. Perhaps several facilities share a name (e.g. \"Subway\" or \"McDonalds\") or license, or perhaps several facilities share the same address, such as facilities at the stadium or the airport.</p> <p>We will try to use the combination of <code>license_num</code>, <code>facility</code>, <code>facility_aka</code>, <code>facility_type</code>, and <code>address</code> to identify a facility:</p> <pre><code>select\n    license_num, facility, facility_type, facility_aka, address , count(*) as \"number of inspections\"\nfrom\n    cleaned.inspections\ngroup by\n    license_num, facility, facility_type, facility_aka, address\norder by\n    count(*) desc, facility, facility_aka, address, license_num, facility_type\nlimit 10;\n</code></pre> license_num facility facility_type facility_aka address number of inspections 1490035 mcdonald's restaurant mcdonald's 6900 s lafayette ave 46 1142451 jewel food  store # 3345 grocery store jewel food  store # 3345 1224 s wabash ave 45 1596210 food 4 less midwest #552 grocery store food 4 less 7030 s ashland ave 44 2083833 mariano's fresh market #8503 grocery store mariano's fresh market 333 e benton pl 41 1302136 mcdonald's restaurant mcdonald's 70 e garfield blvd 40 1476553 pete's produce grocery store pete's produce 1543 e 87<sup>th</sup> st 40 1000572 jewel food store #3030 grocery store jewel food store #3030 7530 s stony island ave 39 1094 one stop food &amp; liquor store grocery store one stop food &amp; liquor store 4301-4323 s lake park ave 39 60184 taqueria el ranchito restaurant taqueria el ranchito 2829 n milwaukee ave 39 9154 jimmy g's restaurant jimmy g's 307 s kedzie ave 37 <p>Yay, it looks like these columns enable us to identify a facility!<sup>14</sup></p> <p>The <code>entities</code> table should store two other types of attributes. The first type describe the entity no matter the time. If the entity were a person, date of birth would be an example but age would not because the latter changes but the former does not. We'll include <code>zip_code</code> and <code>location</code> as two facility attributes.</p> <p>The second type describes when the entity is available for action (e.g. inspection). In this case, the columns <code>start_time, end_time</code> describe the interval in which the facility is in business or active. These columns are important because we don't want to make predictions for inactive entities.</p> <p>The data don't contain active/inactive date columns, so we will use the date of the facility's first inspection as <code>start_time</code>, and either <code>NULL</code> or the date of inspection if the result was <code>out of business</code> or <code>business not located</code> as <code>end_time</code>.</p> <pre><code>create schema if not exists semantic;\n\ndrop table if exists semantic.entities cascade;\n\ncreate table semantic.entities as (\n        with entities as (\n        select\n            distinct on (\n                license_num,\n                facility,\n                facility_aka,\n                facility_type,\n                address\n                )\n            license_num,\n            facility,\n            facility_aka,\n            facility_type,\n            address,\n            zip_code,\n            location,\n            min(date) over (partition by license_num, facility, facility_aka, facility_type, address) as start_time,\n            max(case when result in ('out of business', 'business not located')\n                then date\n                else NULL\n                end)\n            over (partition by license_num, facility, facility_aka, address) as end_time\n        from cleaned.inspections\n        order by\n            license_num asc, facility asc, facility_aka asc, facility_type asc, address asc,\n            date asc -- IMPORTANT!!\n            )\n\n    select\n        row_number() over (order by start_time asc, license_num asc, facility asc, facility_aka asc, facility_type asc, address asc ) as entity_id,\n        license_num,\n        facility,\n        facility_aka,\n        facility_type,\n        address,\n        zip_code,\n        location,\n        start_time,\n        end_time,\n        daterange(start_time, end_time) as activity_period\n    from entities\n        );\n</code></pre> <p>Note that we added a unique identifier (<code>entity_id</code>) to this table. This identifier was assigned using a PostgreSQL idiom: <code>distinct on()</code>. <code>DISTINCT ON</code> keeps the \"first\" row of each group. If you are interested in this powerful technique see this blogpost.</p> <pre><code>select\n    to_char(count(entity_id), '999,999') as entities\nfrom\n    semantic.entities;\n</code></pre> entities 35,668 <p>We will add some indexes to this table<sup>15</sup>:</p> <pre><code>create index entities_ix on semantic.entities (entity_id);\ncreate index entities_license_num_ix on semantic.entities (license_num);\ncreate index entities_facility_ix on semantic.entities (facility);\ncreate index entities_facility_type_ix on semantic.entities (facility_type);\ncreate index entities_zip_code_ix on semantic.entities (zip_code);\n\n-- Spatial index\ncreate index entities_location_gix on semantic.entities using gist (location);\n\ncreate index entities_full_key_ix on semantic.entities (license_num, facility, facility_aka, facility_type, address);\n</code></pre>"},{"location":"dirtyduck/data_preparation/#events-table","title":"Events table","text":"<p>We are ready to create the events table. This table will describe the inspection, like the type of inspection, when and where the inspection happened, and the inspection result. We will add the violations as a <code>JSONB</code> column<sup>16</sup>. Finally, we'll rename <code>inspection</code> as <code>event_id</code><sup>17</sup>.</p> <pre><code>drop table if exists semantic.events cascade;\n\ncreate table semantic.events as (\n\n        with entities as (\n        select * from semantic.entities\n            ),\n\n        inspections as (\n        select\n            i.inspection, i.type, i.date, i.risk, i.result,\n            i.license_num, i.facility, i.facility_aka,\n            i.facility_type, i.address, i.zip_code, i.location,\n            jsonb_agg(\n                jsonb_build_object(\n                    'code', v.code,\n                    'severity', v.severity,\n                'description', v.description,\n                'comment', v.comment\n                )\n            order  by code\n                ) as violations\n        from\n            cleaned.inspections as i\n            inner join\n            cleaned.violations as v\n            on i.inspection = v.inspection\n        group by\n            i.inspection, i.type, i.license_num, i.facility,\n            i.facility_aka, i.facility_type, i.address, i.zip_code, i.location,\n            i.date, i.risk, i.result\n            )\n\n    select\n        i.inspection as event_id,\n        e.entity_id, i.type, i.date, i.risk, i.result,\n        e.facility_type, e.zip_code, e.location,\n        i.violations\n    from\n        entities as e\n        inner join\n        inspections as i\n        using (license_num, facility, facility_aka, facility_type, address, zip_code)\n        );\n\n-- Add some indices\ncreate index events_entity_ix on semantic.events (entity_id asc nulls last);\ncreate index events_event_ix on semantic.events (event_id asc nulls last);\ncreate index events_type_ix on semantic.events (type);\ncreate index events_date_ix on semantic.events(date asc nulls last);\ncreate index events_facility_type_ix on semantic.events  (facility_type);\ncreate index events_zip_code_ix on semantic.events  (zip_code);\n\n-- Spatial index\ncreate index events_location_gix on semantic.events using gist (location);\n\n-- JSONB indices\ncreate index events_violations on semantic.events using gin(violations);\ncreate index events_violations_json_path on semantic.events using gin(violations jsonb_path_ops);\n\ncreate index events_event_entity_zip_code_date on semantic.events (event_id asc nulls last, entity_id asc nulls last, zip_code, date desc nulls last);\n</code></pre> <p>Success! We have one row per event<sup>18</sup> Our semantic data looks like:</p> <pre><code>select\n    event_id,\n    entity_id,\n    type,\n    date,\n    risk,\n    result,\n    facility_type,\n    zip_code\nfrom\n    semantic.events limit 1;\n</code></pre> event_id entity_id type date risk result facility_type zip_code 1343315 22054 canvass 2013-06-06 low fail newsstand 60623 <p>We omitted <code>violations</code> and <code>location</code> for brevity. The total number of inspections is</p> <pre><code>select\n    to_char(count(event_id), '999,999,999') as events\nfrom semantic.events;\n</code></pre> events 148,724 <p>Now that we have our data in a good shape, we are ready to use Triage.</p>"},{"location":"dirtyduck/data_preparation/#lets-eda","title":"Let's EDA \u2026","text":"<p>It is always a good idea to do some Exploratory Data Analysis<sup>19</sup> or EDA for short. This will help us to learn more about the dynamics of the entities or the inspections.</p> <p>We will generate a few plots, just to know:</p> <ul> <li>how many entities/events are every month?</li> <li>how many entities/events ended in a failed state every month? and,</li> <li>how many entities/events have in a critical violation in a failed inspection?</li> </ul>"},{"location":"dirtyduck/data_preparation/#inspections-over-time","title":"Inspections over time","text":"<p>First, we will try the answer the question: how many inspections are realized every month?</p> <p></p>"},{"location":"dirtyduck/data_preparation/#number-of-facilities-inspected-over-time","title":"Number of facilities inspected over time","text":"<p>The previous plot was about the number of events every month, now we will plot how many entities are acted every month.</p> <p>One question, that is useful to answer is: Are there facilities that are inspected more than once in a month?</p> <p>Note</p> <p>We are doing an emphasis in inspected since our data set doesn't contain all the facilities in Chicago. This will have an effect on the modeling stage.</p> <p></p>"},{"location":"dirtyduck/data_preparation/#number-of-failed-inspections-over-time","title":"Number of failed inspections over time","text":"<p>What is the proportion of inspections every month that actually end in a failed inspection?</p> <p></p>"},{"location":"dirtyduck/data_preparation/#number-of-facilities-with-failed-inspections-over-time","title":"Number of facilities with failed inspections over time","text":"<p>Now let's see the behavior of the outcomes of the inspection across time. First just if the inspection failed.</p> <p></p>"},{"location":"dirtyduck/data_preparation/#number-of-severe-violations-found-in-a-failed-inspection-over-time","title":"Number of severe violations found in a failed inspection over time","text":"<p>Finally let's analyze the evolution of failed inspections with severe violations (violation code in 15-29)</p> <p></p>"},{"location":"dirtyduck/data_preparation/#number-of-facilities-with-severe-violations-found-in-a-failed-inspection-over-time","title":"Number of facilities with severe violations found in a failed inspection over time","text":"<p>This few plots give us a sense of how the data behaves and will help us in detect weird bugs or model-behavior later.</p>"},{"location":"dirtyduck/data_preparation/#whats-next","title":"What\u2019s next?","text":"<ul> <li>Learn more about triage</li> <li>Learn more about early warning systems</li> <li>Learn more about resource prioritization systems</li> </ul> <ol> <li> <p>If you want to try different columns (or you don't remember which columns try <code>\\d raw.inspectios</code> first\u00a0\u21a9</p> </li> <li> <p>We are following the event's definition from physics: \"an event is the instantaneous physical situation or occurrence associated with a point in spacetime\"\u00a0\u21a9</p> </li> <li> <p>It will make your life easier and most of the Machine Learning algorithms only accept data in matrix form (i.e. one big table)\u00a0\u21a9</p> </li> <li> <p>Verbatim from the datasource documentation.\u00a0\u21a9</p> </li> <li> <p>A controversial decision, I know.\u00a0\u21a9</p> </li> <li> <p>This problem is related to the process of deduplication and there is another DSaPP tool for that: matching-tool.\u00a0\u21a9</p> </li> <li> <p>It is the Chicago Food Inspections dataset, after all.\u00a0\u21a9</p> </li> <li> <p>We could also use the default geometric data type from postgresql: <code>point</code> (https://www.postgresql.org/docs/10/datatype-geometric.html)\u00a0\u21a9</p> </li> <li> <p>We will store the <code>Point</code> as a <code>geography</code> object. As a result, spatial database operations (like calculating the distances between two facilities) will return answers in meters instead of degrees. See this.\u00a0\u21a9</p> </li> <li> <p>As a real geographical object check the PostGIS documentation \u21a9</p> </li> <li> <p>Remember our tip at the beginning of this section!\u00a0\u21a9</p> </li> <li> <p>If the code looks funny to you, it is because we are using grouping sets, in particular <code>rollup</code>. See the docs.\u00a0\u21a9</p> </li> <li> <p>Yeah, you wish\u00a0\u21a9</p> </li> <li> <p>Almost. At least good for this tutorial. Look carefully.\u00a0\u21a9</p> </li> <li> <p>ALWAYS add indexes to your tables!\u00a0\u21a9</p> </li> <li> <p>If you want to have a deep explanation about why is this good check this blog post \u21a9</p> </li> <li> <p>As a general rule I hate to add the suffix <code>_id</code>, I would rather prefer to name them as <code>event</code> and <code>entity</code> instead of <code>event_id</code> and <code>entity_id</code>. But <code>triage</code> named those columns in that way and for that we are stuck with that nomenclature.\u00a0\u21a9</p> </li> <li> <p>This will simplify the creation of features for our machine learning models.\u00a0\u21a9</p> </li> <li> <p>Defined by John Tukey as: Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. \u21a9</p> </li> </ol>"},{"location":"dirtyduck/dirty_duckling/","title":"Dirty duckling: the quick start guide","text":"<p>This quickstart guide follows the workflow explained here. The goal is to show you an instance of that workflow using the Chicago Food Inspections dataset data source.</p> <p>We packed this a sample of Chicago Food Inspections data source as part of the dirty duck tutorial.  Just run in the folder that contains the <code>triage</code> local repository:</p> <pre><code>./tutorial.sh up\n</code></pre> <p>from you command line. This will start the database.</p>"},{"location":"dirtyduck/dirty_duckling/#1-install-triage-check","title":"1. Install Triage: Check!","text":"<p>We also containerized <code>triage</code>, so, in this tutorial it is already installed for you! Just run</p> <pre><code>./tutorial.sh bastion\n</code></pre> <p>The prompt in your command line should change to something like</p> <pre><code>[triage@dirtyduck$:/dirtyduck]#\n</code></pre> <p>Type <code>triage</code>, if no error. You completed this step!</p> <p>Now you have <code>triage</code> installed, with all its power at the point of your fingers.</p>"},{"location":"dirtyduck/dirty_duckling/#2-structure-your-data-events-and-entities","title":"2. Structure your data: Events (and entities)","text":"<p>As mentioned in the quickstart workflow, at least you need one table that contains events, i.e. something that happened to your entities of interest somewhere at sometime. So you need at least three columns in your data: <code>entity_id</code>, <code>event_id</code>, <code>date</code> (and <code>location</code> if you have it would be a nice addition).</p> <p>In dirtyduck, we provide you with two tables: <code>semantic.entities</code> and <code>semantic.events</code>. The latter is the required minimum. We added the <code>semantic.entities</code> table as a good practice.</p> <p>This is the  simplest way to structure your data: as a series of events connected to your entity of interest (people, organization, business, etc.) that take place at a certain time. Each row of the data will be an event.</p> <p>For this quickstart tutorial, you don't need to interact manually with the database, but, if you are curious you can peek inside it, and verify how the <code>events</code> table look like.</p> <p>Inside <code>bastion</code> you can connect to the database typing</p> <pre><code>psql $DATABASE_URL\n</code></pre> <p>This will change the prompt one more time to</p> <pre><code>food=#\n</code></pre> <p>Now, type (or copy-paste) the following</p> <pre><code>select\n  event_id\n  entity_id,\n  date,\n  zip_code,\n  type\n  from\n      semantic.events\n where random() &lt; 0.001\n limit 5;\n</code></pre> entity_id date zip_code type 1092838 2014-02-27 60657 license 1325036 2014-05-19 60612 canvass 1385431 2014-06-25 60651 complaint 1395315 2014-01-08 60707 canvass 1395916 2014-02-03 60641 canvass <p>Each row in this table is an event with <code>event_id</code> and <code>entity_id</code> (which links to the entity it happened to), a <code>date</code> (when it happened), as well a location (the <code>zip_code</code> column). The event will have attributes that describe it in its particularity, in this case we are just showing one of those attributes: the type of the inspection (<code>type</code>)</p> <p>And, if you also want to see the entities in your data</p> <pre><code>select\n  entity_id, license_num, facility, facility_type, activity_period\n  from\n      semantic.entities\n where random() &lt; 0.001 limit 5;\n</code></pre> entity_id license_num facility facility_type activity_period 2218 1223576 loretto hospital hospital [2014-02-27,) 2353 1804587 subway restaurant [2014-03-05,) 636 2002788 duck walk restaurant [2014-01-17,2016-02-29) 3748 1904141 zaragoza restaurant restaurant [2014-04-03,) 5118 2224978 saint cajetan school [2014-05-06,) <p>Triage needs a field named <code>entity_id</code> (that needs to be of type integer) to refer to the primary entities of interest in our project.</p> <p>When you're done exploring the database, you can exit the postgres command line interface by typing <code>\\q</code></p>"},{"location":"dirtyduck/dirty_duckling/#3-set-up-dirty-ducks-triage-configuration-file","title":"3. Set up Dirty duck's triage configuration file","text":"<p>The configuration file sets up the modeling process to mirror the operational scenario the models will be used in. This involves defining the cohort to train/predict on, the outcome we're predicting, how far in the future we're predicting, how often will the model be updated, how often will the predicted list be used for interventions, what are the resources available to intervene to define the evaluation metric,  etc.</p> <p>Here's the sample configuration file called <code>dirty-duckling.yaml</code></p> <p>If you wish, you can check the content of the file with <code>cat experiments/dirty-ducking.yaml</code></p> <pre><code>config_version: 'v8'\n\nmodel_comment: 'dirtyduck-quickstart'\n\nrandom_seed: 1234\n\ntemporal_config:\n    label_timespans: ['3months']\n\nlabel_config:\n  query: |\n    select\n    entity_id,\n    bool_or(result = 'fail')::integer as outcome\n    from semantic.events\n    where '{as_of_date}'::timestamp &lt;= date\n    and date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\n    group by entity_id\n  name: 'failed_inspections'\n\nfeature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates_imputation:\n      count:\n        type: 'zero_noflag'\n\n    aggregates:\n      -\n        quantity:\n          total: \"*\"\n        metrics:\n          - 'count'\n\n    intervals: ['all']\n\nmodel_grid_preset:  'quickstart'\n\nscoring:\n    testing_metric_groups:\n        -\n          metrics: [precision@]\n          thresholds:\n            percentiles: [10]\n\n\n    training_metric_groups:\n      -\n          metrics: [precision@]\n          thresholds:\n            percentiles: [10]\n</code></pre> <p>This is the minimum configuration file, and it still has a lot of sections (ML is a complex business!).</p> <p>Warning</p> <p>If you use the minimum configuration file several parameters will fill up using defaults. Most of the time those defaults are not the values that your modeling of the problem needs! Please check here to see which values are being used and act accordingly.</p> <p><code>triage</code> uses/needs a data connection in order to work. The connection will be created using the database credentials information (name of the database, server, username, and password).</p> <p>You could use a database configuration file here's an example database configuation file or you can setup an environment variable named <code>$DATABASE_URL</code>, this is the approach taken in the dirtyduck tutorial, its value inside <code>bastion</code> is</p> <pre><code>   postgresql://food_user:some_password@food_db/food\n</code></pre> <p>For the quick explanation of the sections check the quickstart workflow guide. For a detailed explanation about each section of the configuration file look here</p>"},{"location":"dirtyduck/dirty_duckling/#4-run-triage","title":"4. Run triage","text":"<p>Now we are ready for run something! First we will validate the configuration files by running:</p> <pre><code>triage experiment experiments/dirty-duckling.yaml --validate-only\n</code></pre> <p>If everything was OK (it should!), you will see this in your screen:</p> <pre><code>2020-08-20 16:55:34 - SUCCESS Experiment validation ran to completion with no errors\n2020-08-20 16:55:34 - SUCCESS Experiment (a336de4800cec8964569d051dc56f85d)'s configuration file is OK!\n</code></pre> <p>Now you can run the experiment with:</p> <p><pre><code>triage experiment experiments/dirty-duckling.yaml\n</code></pre> That's it! If you see this message in your screen:</p> <pre><code> 2020-08-20 16:56:56 - SUCCESS Training, testing and evaluating models completed\n 2020-08-20 16:56:56 - SUCCESS All matrices that were supposed to be build were built. Awesome!\n 2020-08-20 16:56:56 - SUCCESS All models that were supposed to be trained were trained. Awesome!\n 2020-08-20 16:56:56 - SUCCESS Experiment (a336de4800cec8964569d051dc56f85d) ran through completion\n</code></pre> <p>it would mean that <code>triage</code> actually built (in this order) cohort (table <code>cohort_all_entities...</code>), labels (table <code>labels_failed_inspections...</code>), features (schema <code>features</code>), matrices (table <code>model_metdata.matrices</code> and folder <code>matrices</code>), models (tables <code>triage_metadata.models</code> and <code>triage_metadata.model_groups</code>; folder <code>trained_models</code>), predictions (table <code>test_results.predictions</code>) and evaluations (table <code>test_results.evaluations</code>).</p>"},{"location":"dirtyduck/dirty_duckling/#5-look-at-results-of-your-duckling","title":"5. Look at results of your duckling!","text":"<p>Next, let's quickly check the tables in the schemas <code>triage_metadata</code> and <code>test_results</code> to make sure everything worked. There you will find a lot of information related to the performance of your models.</p> <p>Still connected to the <code>bastion</code> docker container, you can connect to the database by typing:</p> <pre><code>psql $DATABASE_URL\n</code></pre> <p>Again, you should see the postgreSQL prompt:</p> <pre><code>food=#\n</code></pre> <p>Tables in the <code>triage_metadata</code> schema have some general information about experiments that you've run and the models they created. The <code>quickstart</code> model grid preset should have built 3 models. Let's check with:</p> <pre><code>select\n  model_id, model_group_id, model_type\n  from\n      triage_metadata.models;\n</code></pre> <p>This should give you a result that looks something like:</p> model_id model_group_id model_type 1 1 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression 2 2 sklearn.tree.DecisionTreeClassifier 3 3 sklearn.dummy.DummyClassifier <p>If you want to see predictions for individual entities, you can check out <code>test_results.predictions</code>, for instance:</p> <pre><code>select\n  model_id, entity_id, as_of_date, score, label_value\n  from\n      test_results.predictions\n  where entity_id = 15596\n  order by model_id;\n</code></pre> <p>This will give you something like:</p> model_id entity_id as_of_date score label_value 1 15596 2017-09-29 00:00:00 0.21884 0 2 15596 2017-09-29 00:00:00 0.22831 0 3 15596 2017-09-29 00:00:00 0.25195 0 <p>Finally, <code>test_results.evaluations</code> holds some aggregate information on model performance. In our config above, we only focused on precision in the top ten percent, so let's see how the models are doing based on this:</p> <pre><code>select\n  model_id, metric, parameter,\n  round(stochastic_value,3) as stochatic_value\n  from\n      test_results.evaluations\n  where metric = 'precision@'\n        and parameter='10_pct'\n  order by model_id;\n</code></pre> model_id metric parameter stochastic_value 1 precision@ 10_pct 0.287 2 precision@ 10_pct 0.292 3 precision@ 10_pct 0.237 <p>Not great! But then again, these were just a couple of overly simple model specifications to get things up and running...</p> <p>Feel free to explore some of the other tables in these schemas (note that there's also a <code>train_results</code> schema with performance on the training set as well as feature importances, where defined). When you're done exploring the database, you can exit the postgres command line interface by typing <code>\\q</code></p> <p>With a real modeling run you could (should) do model selection, postmodeling, bias audit, etc. <code>triage</code> provides tools for doing all of that, but we the purpose of this little experiment was just to get things up and running. If you have successfully arrived to this point, you are all set to do your own modeling (here's a good place to start), but if you want to go deeper in this example and learn about these other <code>triage</code> functions, continue reading our in-depth tutorial.</p>"},{"location":"dirtyduck/dirty_duckling/#6-whats-next","title":"6. What's next?","text":""},{"location":"dirtyduck/dirty_duckling/#take-a-deeper-look-at-triage-through-this-example","title":"Take a deeper look at triage through this example","text":""},{"location":"dirtyduck/dirty_duckling/#get-started-with-your-own-project-and-data","title":"Get started with your own project and data","text":""},{"location":"dirtyduck/eis/","title":"An Early Intervention System: Chicago food inspections","text":"<p>Before continue, Did you\u2026?</p> <p>This case study, part of the dirtyduck tutorial,  assumes that you already setup the tutorial\u2019s infrastructure and load the dataset.</p> <ul> <li> <p>If you didn\u2019t setup the infrastructure go here,</p> </li> <li> <p>If you didn't load the data, you can do it very quickly   or you can follow all the steps and explanations about the data.</p> </li> </ul>"},{"location":"dirtyduck/eis/#problem-description","title":"Problem description","text":"<p><code>Triage</code> is designed to build, among other things, early warning systems (also called early intervention, EIS). While there are several differences between modeling early warnings and inspection prioritization, perhaps the biggest differences is that the entity is active (i.e. it is doing stuff for which an outcome will happen) in EIS, but passive (e.g it is inspected) in resource prioritization. Among other things, this difference affects the way the outcome is built.</p> <p>Saying that, here's the question we want to answer:</p> <p>Will my restaurant be inspected in the next Y period of time?</p> <p>Where X could be 3 days, 2 months, 1 year, etc.</p> <p>We will translate that problem to</p> <p>Will my restaurant be at the top-X facilities most likely to be inspected in the next Y period of time?</p> <p>Knowing the answer to this question enables you (as the restaurant owner or manager) to prepare for the inspection.</p>"},{"location":"dirtyduck/eis/#what-are-the-outcomes","title":"What are the outcomes?","text":"<p>The trick to note is that on any given day there are two possible outcomes: the facility was inspected and the facility wasn't inspected. Our outcomes table will be larger than in the resource prioritization example because we need an outcome for every active facility on every date. The following image tries to exemplify this reasoning:</p> <p> Figure. The image shows three facilities (blue, red and orange), and next to each, a temporal line with 6 days (0-5). Each dot represents the event (whether an inspection happened). Yellow means the inspection happened (<code>TRUE</code>outcome) and blue means it didn't (<code>FALSE</code> outcome). Each facility in the image had two inspections, six in total.</p> <p>Fortunately, <code>triage</code> will help us to create this table.</p>"},{"location":"dirtyduck/eis/#what-are-the-entities-of-interest-the-cohort","title":"What are the entities of interest? The cohort","text":"<p>We are interested in predict only in active facilities (remember, in this case study, you own a restaurant, What is the point on predict if your restaurant is already closed for good?). This is the same cohort as the cohort table in the resource prioritization case study</p> <p>Experiment description file</p> <p>You could check the meaning about experiment description files (or configuration files) in A deeper look into triage.</p> <p>First the usual stuff. Note that we are changing <code>model_comment</code> and <code>label_definition</code> (remember that this is used for generating the hash that differentiates models and model groups).</p> <pre><code>config_version: 'v8'\n\nmodel_comment: 'eis: 01'\nrandom_seed: 23895478\n\nuser_metadata:\n  label_definition: 'inspected'\n  experiment_type: 'eis'\n  description: |\n    EIS 01\n  purpose: 'model creation'\n  org: 'DSaPP'\n  team: 'Tutorial'\n  author: 'Your name here'\n  etl_date: '2019-05-07'\n\nmodel_group_keys:\n  - 'class_path'\n  - 'parameters'\n  - 'feature_names'\n  - 'feature_groups'\n  - 'cohort_name'\n  - 'state'\n  - 'label_name'\n  - 'label_timespan'\n  - 'training_as_of_date_frequency'\n  - 'max_training_history'\n  - 'label_definition'\n  - 'experiment_type'\n  - 'org'\n  - 'team'\n  - 'author'\n  - 'etl_date'\n</code></pre> <p>For the labels the query is pretty simple, if the facility showed in the data, it will get a positive outcome, if not they will get a negative outcome</p> <pre><code>label_config:\n  query: |\n    select\n    entity_id,\n    True::integer as outcome\n    from semantic.events\n    where '{as_of_date}'::timestamp &lt;= date\n    and date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\n    group by entity_id\n  include_missing_labels_in_train_as: False\n  name: 'inspected'\n</code></pre> <p>Note the two introduced changes in this block, first, the outcome is <code>True</code> , because all our observations represent inspected facilities (see discussion above and in particular previous image), second, we added the line <code>include_missing_labels_in_train_as: False</code>. This line tells <code>triage</code> to incorporate all the missing facilities in the training matrices with <code>False</code> as the label.</p> <p>As stated we will use the same configuration block for cohorts that we used in inspections:</p> <pre><code>cohort_config:\n  query: |\n    select e.entity_id\n    from semantic.entities as e\n    where\n    daterange(start_time, end_time, '[]') @&gt; '{as_of_date}'::date\n  name: 'active_facilities'\n</code></pre>"},{"location":"dirtyduck/eis/#modeling-using-machine-learning","title":"Modeling Using Machine Learning","text":"<p>We need to specify the temporal configuration, this section should reflect the operationalization of the model.</p> <p>Let\u2019s assume that every facility owner needs 6 months to prepare for an inspection. So, the model needs to answer the question: Will my restaurant be inspected in the next 6 months?</p>"},{"location":"dirtyduck/eis/#temporal-configuration","title":"Temporal configuration","text":"<pre><code>    temporal_config:\n        feature_start_time: '2010-01-04'\n        feature_end_time: '2018-06-01'\n        label_start_time: '2014-06-01'\n        label_end_time: '2018-06-01'\n\n        model_update_frequency: '6month'\n        training_label_timespans: ['6month']\n        training_as_of_date_frequencies: '6month'\n\n        test_durations: '6month'\n        test_label_timespans: ['6month']\n        test_as_of_date_frequencies: '6month'\n\n        max_training_histories: '5y'\n</code></pre> <p>As before, you can generate the image of the temporal blocks:</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntriage experiment experiments/eis_01.yaml --show-timechop\n</code></pre> <p>What? \u2026 Bastion?</p> <p><code>bastion</code> is the docker container that contains all the setup required to run this tutorial, if this is the first time that you see this word, you should stop and revisit setup infrastructure.</p> <p> Figure. Temporal blocks for the Early Warning System. We want to predict the most likely facilities to be inspected in the following 6 months</p>"},{"location":"dirtyduck/eis/#features","title":"Features","text":"<p>Regarding the features, we will use the same ones that were used in inspections prioritization:</p> <pre><code>    feature_aggregations:\n      -\n        prefix: 'inspections'\n        from_obj: 'semantic.events'\n        knowledge_date_column: 'date'\n\n        aggregates_imputation:\n          count:\n            type: 'zero_noflag'\n\n        aggregates:\n          -\n            quantity:\n              total: \"*\"\n            metrics:\n              - 'count'\n\n        intervals: ['1month', '3month', '6month', '1y', 'all']\n\n      -\n        prefix: 'risks'\n        from_obj: 'semantic.events'\n        knowledge_date_column: 'date'\n\n        categoricals_imputation:\n          sum:\n            type: 'zero'\n          avg:\n            type: 'zero'\n\n        categoricals:\n          -\n            column: 'risk'\n            choices: ['low', 'medium', 'high']\n            metrics:\n              - 'sum'\n              - 'avg'\n\n        intervals: ['1month', '3month', '6month', '1y', 'all']\n\n      -\n        prefix: 'results'\n        from_obj: 'semantic.events'\n        knowledge_date_column: 'date'\n\n        categoricals_imputation:\n          all:\n            type: 'zero'\n\n        categoricals:\n          -\n            column: 'result'\n            choice_query: 'select distinct result from semantic.events'\n            metrics:\n              - 'sum'\n              - 'avg'\n\n        intervals: ['1month', '3month', '6month', '1y', 'all']\n\n      -\n        prefix: 'inspection_types'\n        from_obj: 'semantic.events'\n        knowledge_date_column: 'date'\n\n        categoricals_imputation:\n          sum:\n            type: 'zero_noflag'\n\n        categoricals:\n          -\n            column: 'type'\n            choice_query: 'select distinct type from semantic.events where type is not null'\n            metrics:\n              - 'sum'\n\n        intervals: ['1month', '3month', '6month', '1y', 'all']\n</code></pre> <p>We specify that we want to use all possible feature-group combinations for training:</p> <pre><code>    feature_group_definition:\n       prefix:\n         - 'inspections'\n         - 'results'\n         - 'risks'\n         - 'inspection_types'\n\n    feature_group_strategies: ['all']\n</code></pre> <p>i.e. <code>all</code> will train models with all the features groups, <code>leave-one-in</code> will use only one of the feature groups for traning, and lastly, <code>leave-one-out</code> will train the model with all the features except one.</p>"},{"location":"dirtyduck/eis/#algorithm-and-hyperparameters","title":"Algorithm and hyperparameters","text":"<p>We will begin defining some basic models as baselines.</p> <pre><code>'triage.component.catwalk.baselines.thresholders.SimpleThresholder':\n  rules:\n    - ['inspections_entity_id_1month_total_count &gt; 0']\n    - ['results_entity_id_1month_result_fail_sum &gt; 0']\n    - ['risks_entity_id_1month_risk_high_sum &gt; 0']\n\n'triage.component.catwalk.baselines.rankers.PercentileRankOneFeature':\n  feature: ['risks_entity_id_all_risk_high_sum', 'inspections_entity_id_all_total_count', 'results_entity_id_all_result_fail_sum']\n  low_value_high_score: [True]\n\n'sklearn.dummy.DummyClassifier':\n  strategy: ['prior', 'stratified']\n\n'sklearn.tree.DecisionTreeClassifier':\n  criterion: ['gini']\n  max_features: ['sqrt']\n  max_depth: [1,2,5,~]\n  min_samples_split: [2]\n\n'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression':\n  penalty: ['l1','l2']\n  C: [0.000001, 0.0001, 0.01,  1.0]\n</code></pre> <p>How did I know the name of the features?</p> <p><code>triage</code> has a very useful utility called <code>featuretest</code></p> <pre><code>    triage featuretest experiments/eis_01.yaml 2018-01-01\n</code></pre> <p>You can use for testing the definition of your features and also to see if the way that the features are calculated is actually what do you expect.</p> <p>Here we are using it just to check the name of the generated features.</p> <p><code>triage</code> will create 20 model groups: algorithms and hyperparameters (4 <code>DecisionTreeClassifier</code>, 8 <code>ScaledLogisticRegression</code>, 2 <code>DummyClassifier</code>, 3 <code>SimpleThresholder</code> and 3 <code>PercentileRankOneFeature</code>) \u00d7 1 features sets (1 <code>all</code>). The total number of models is three times that (we have 6 time blocks, so 120 models).</p> <pre><code>    scoring:\n        testing_metric_groups:\n            -\n              metrics: [precision@, recall@]\n              thresholds:\n                percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n                top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]\n\n\n        training_metric_groups:\n          -\n            metrics: [accuracy]\n          -\n            metrics: [precision@, recall@]\n            thresholds:\n              percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n              top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]\n</code></pre> <p>As a last step, we validate that the configuration file is correct:</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntriage experiment experiments/eis_01.yaml  --validate-only\n</code></pre> <p>And then just run it:</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntime triage experiment experiments/eis_01.yaml\n</code></pre> <p>Protip</p> <p>We are including the command <code>time</code> in order to get the total running time of the experiment. You can remove it, if you like.</p> <p>This will take a lot amount of time (on my computer took 3h 42m), so, grab your coffee, chat with your coworkers, check your email, or read the DSSG blog. It's taking that long for several reasons:</p> <ol> <li>There are a lot of models, parameters, etc.</li> <li>We are running in serial mode (i.e. not in parallel).</li> <li>The database is running on your laptop.</li> </ol> <p>You can solve 2 and 3. For the second point you could use the <code>docker</code> container that has the multicore option enabled. For 3, I recommed you to use a PostgreSQL database in the cloud, such as Amazon's PostgreSQL RDS (we will explore this later in running triage in AWS Batch).</p> <p>After the experiment finishes, we can create the following table:</p> <pre><code>    with features_groups as (\n    select\n        model_group_id,\n        split_part(unnest(feature_list), '_', 1) as feature_groups\n    from\n        triage_metadata.model_groups\n    ),\n\n    features_arrays as (\n    select\n        model_group_id,\n        array_agg(distinct feature_groups) as feature_groups\n    from\n        features_groups\n    group by\n        model_group_id\n    )\n\n    select\n        model_group_id,\n        regexp_replace(model_type, '^.*\\.', '') as model_type,\n        hyperparameters,\n        feature_groups,\n        array_agg(model_id order by train_end_time asc) as models,\n        array_agg(train_end_time::date order by train_end_time asc) as times,\n        array_agg(to_char(stochastic_value, '0.999') order by\n    train_end_time asc) as \"precision@10% (stochastic)\"\n    from\n        triage_metadata.models\n        join\n        features_arrays using(model_group_id)\n        join\n        test_results.evaluations using(model_id)\n    where\n        model_comment ~ 'eis'\n        and\n        metric || parameter = 'precision@10_pct'\n    group by\n        model_group_id,\n        model_type,\n        hyperparameters,\n        feature_groups\n    order by\n        model_group_id;\n</code></pre> model_group_id model_type hyperparameters feature_groups models times precision@10% (stochastic) 1 SimpleThresholder {\"rules\": [\"inspections_entity_id_1month_total_count &gt; 0\"]} {inspection,inspections,results,risks} {1,19,37,55,73,91} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.358\",\" 0.231\",\" 0.321\",\" 0.267\",\" 0.355\",\" 0.239\"} 2 SimpleThresholder {\"rules\": [\"results_entity_id_1month_result_fail_sum &gt; 0\"]} {inspection,inspections,results,risks} {2,20,38,56,74,92} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.316\",\" 0.316\",\" 0.323\",\" 0.344\",\" 0.330\",\" 0.312\"} 3 SimpleThresholder {\"rules\": [\"risks_entity_id_1month_risk_high_sum &gt; 0\"]} {inspection,inspections,results,risks} {3,21,39,57,75,93} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.364\",\" 0.248\",\" 0.355\",\" 0.286\",\" 0.371\",\" 0.257\"} 4 PercentileRankOneFeature {\"low_value_high_score\": true, \"feature\": \"risks_entity_id_all_risk_high_sum\"} {inspection,inspections,results,risks} {4,22,40,58,76,94} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.121\",\" 0.193\",\" 0.124\",\" 0.230\",\" 0.112\",\" 0.161\"} 5 PercentileRankOneFeature {\"low_value_high_score\": true, \"feature\": \"inspections_entity_id_all_total_count\"} {inspection,inspections,results,risks} {5,23,41,59,77,95} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.076\",\" 0.133\",\" 0.098\",\" 0.101\",\" 0.086\",\" 0.082\"} 6 PercentileRankOneFeature {\"low_value_high_score\": true, \"feature\": \"results_entity_id_all_result_fail_sum\"} {inspection,inspections,results,risks} {6,24,42,60,78,96} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.237\",\" 0.274\",\" 0.250\",\" 0.275\",\" 0.225\",\" 0.221\"} 7 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {7,25,43,61,79,97} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.284\",\" 0.441\",\" 0.559\",\" 0.479\",\" 0.463\",\" 0.412\"} 8 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {8,26,44,62,80,98} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.401\",\" 0.388\",\" 0.533\",\" 0.594\",\" 0.519\",\" 0.649\"} 9 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {9,27,45,63,81,99} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.594\",\" 0.876\",\" 0.764\",\" 0.843\",\" 0.669\",\" 0.890\"} 10 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {10,28,46,64,82,100} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.484\",\" 0.542\",\" 0.566\",\" 0.589\",\" 0.565\",\" 0.546\"} 11 ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {11,29,47,65,83,101} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.272\",\" 0.318\",\" 0.306\",\" 0.328\",\" 0.292\",\" 0.281\"} 12 ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {12,30,48,66,84,102} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.382\",\" 0.187\",\" 0.375\",\" 0.261\",\" 0.419\",\" 0.233\"} 13 ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {13,31,49,67,85,103} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.275\",\" 0.314\",\" 0.306\",\" 0.329\",\" 0.462\",\" 0.421\"} 14 ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {14,32,50,68,86,104} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.562\",\" 0.454\",\" 0.765\",\" 0.821\",\" 0.758\",\" 0.828\"} 15 ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {15,33,51,69,87,105} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.745\",\" 0.863\",\" 0.807\",\" 0.867\",\" 0.826\",\" 0.873\"} 16 ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {16,34,52,70,88,106} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.739\",\" 0.863\",\" 0.793\",\" 0.870\",\" 0.822\",\" 0.874\"} 17 ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {17,35,53,71,89,107} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.706\",\" 0.769\",\" 0.796\",\" 0.846\",\" 0.822\",\" 0.868\"} 18 ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {18,36,54,72,90,108} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.694\",\" 0.779\",\" 0.793\",\" 0.845\",\" 0.823\",\" 0.867\"} 19 DummyClassifier {\"strategy\": \"prior\"} {inspection,inspections,results,risks} {109,111,113,115,117,119} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.273\",\" 0.316\",\" 0.306\",\" 0.332\",\" 0.295\",\" 0.282\"} 20 DummyClassifier {\"strategy\": \"stratified\"} {inspection,inspections,results,risks} {110,112,114,116,118,120} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.272\",\" 0.314\",\" 0.301\",\" 0.343\",\" 0.292\",\" 0.287\"} <p>Protip</p> <p>You could have a \"real time\" version of the previous query while you are running the experiment config file with triage. Just execute <code>\\watch n</code> in the <code>psql</code> console and it will be refreshed every n seconds</p>"},{"location":"dirtyduck/eis/#lets-explore-more-second-grid","title":"Let\u2019s explore more: second grid","text":"<p>After the baseline we will explore a more robust set of algorithms. We will use a different experiment config file: <code>eis_02.yaml</code>.</p> <p>The only differences between this experiment config file and the previous are in the <code>user_metadata</code> section:</p> <pre><code>config_version: 'v8'\n\nmodel_comment: 'eis: 02'\nrandom_seed: 23895478\n\nuser_metadata:\n  label_definition: 'inspected'\n  experiment_type: 'eis'\n  description: |\n    EIS 02\n  purpose: 'model creation'\n  org: 'DSaPP'\n  team: 'Tutorial'\n  author: 'Your name here'\n  etl_date: '2019-05-07'\n</code></pre> <p>and in the <code>grid_config</code>:</p> <pre><code>grid_config:\n    ## Boosting\n   'sklearn.ensemble.AdaBoostClassifier':\n     n_estimators: [1000, 2000]\n\n   'sklearn.ensemble.GradientBoostingClassifier':\n     n_estimators: [1000, 2000]\n     learning_rate : [0.01, 1.0]\n     subsample: [0.5, 1.0]\n     min_samples_split: [2]\n     max_depth: [2,5]\n\n   ## Forest\n   'sklearn.tree.DecisionTreeClassifier':\n     criterion: ['gini']\n     max_depth: [2, 5, 10]\n     min_samples_split: [2, 10, 50]\n\n   'sklearn.ensemble.RandomForestClassifier':\n     n_estimators: [10000]\n     criterion: ['gini']\n     max_depth: [2, 5, 10]\n     max_features: ['sqrt']\n     min_samples_split: [2, 10, 50]\n     n_jobs: [-1]\n\n   'sklearn.ensemble.ExtraTreesClassifier':\n     n_estimators: [10000]\n     criterion: ['gini']\n     max_depth: [2, 5, 10]\n     max_features: ['sqrt']\n     min_samples_split: [2, 10, 50]\n     n_jobs: [-1]\n</code></pre> <p>This new experiment configuration file will add 37 models groups to our experiment.</p> <p>You can run this experiment with:</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntime triage experiment experiments/eis_02.yaml\n</code></pre>"},{"location":"dirtyduck/eis/#audition-so-many-models-how-can-i-choose-the-best-one","title":"Audition: So many models, how can I choose the best one?","text":"<p>Let\u2019s select the best model groups, using Audition. We need to make small changes to the <code>/triage/audition/eis_audition_config.yaml</code> compared to the inspection\u2019s one:</p> <pre><code># CHOOSE MODEL GROUPS\nmodel_groups:\n    query: |\n        select distinct(model_group_id)\n        from triage_metadata.model_groups\n        where model_config -&gt;&gt; 'experiment_type' ~ 'eis'\n# CHOOSE TIMESTAMPS/TRAIN END TIMES\ntime_stamps:\n    query: |\n        select distinct train_end_time\n        from triage_metadata.models\n        where model_group_id in ({})\n        and extract(day from train_end_time) in (1)\n        and train_end_time &gt;= '2014-01-01'\n# FILTER\nfilter:\n    metric: 'precision@' # metric of interest\n    parameter: '10_pct' # parameter of interest\n    max_from_best: 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time.\n    threshold_value: 0.0 # The worst absolute value that the given metric should be.\n    distance_table: 'eis_distance_table' # name of the distance table\n    models_table: 'models' # name of the models table\n\n# RULES\nrules:\n    -\n        shared_parameters:\n            -\n                metric: 'precision@'\n                parameter: '10_pct'\n\n        selection_rules:\n            -\n                name: 'best_current_value' # Pick the model group with the best current metric value\n                n: 5\n            -\n                name: 'best_average_value' # Pick the model with the highest average metric value\n                n: 5\n            -\n                name: 'lowest_metric_variance' # Pick the model with the lowest metric variance\n                n: 5\n            -\n                name: 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case`\n                dist_from_best_case: [0.05]\n                n: 5\n</code></pre> <p>And then we run the simulation of the rules againts the experiment as:</p> <pre><code>triage audition -c ./audition/eis_audition_config.yaml --directory audition/eis\n</code></pre> <p><code>Audition</code> will create several plots that will help you to sort out which is the best model group to use (like in a production setting or just to generate your list).</p>"},{"location":"dirtyduck/eis/#filtering-model-groups","title":"Filtering model groups","text":"<p><code>Audition</code> will generate two plots that are meant to be used together: model performance over time and distance from best.</p> <p> Figure. Model group performance over time. In this case the metric show is <code>precision@10%</code>. The black dashed line represents the (theoretical) system's performance if we select the best performant model in a every evaluation date. The colored lines represents different model groups. All the model groups that share an algorithm will be colored the same.</p> <p> Figure. Proportion of all the models in a model group that are separated from the best model. The distance is measured in percentual points, i.e. How much less precision at 10 percent of the population compared to the best model in that date.</p>"},{"location":"dirtyduck/eis/#selecting-the-best-rule-or-strategy-for-choosing-model-groups","title":"Selecting the best rule or strategy for choosing model groups","text":"<p>In this phase of the audition, you will see what will happen in the next time if you choose your model group with an specific strategy or rule.</p> <p>You then, can calculate the regret. Regret is defined as the difference between the performance of the best model evaluated on the \"next time\" and the performance of the model selected by a particular rule.</p> <p> Figure. Given a strategy for selecting model groups (in the plot 4 are shown), What will be the performace of the model group chosen by that strategy in the next evaluation date?</p> <p> Figure. Given a strategy for selecting model groups (in the plot 4 are shown). What will be the distance (*regret) to the best theoretical model in the following evaluation date.*</p> <p> Figure. Expected regret for the strategies. The less the better.</p> <p>It seems that the worst strategy (the one with the bigger \u201cregret\u201d) for selecting a model_group is <code>lowest_metric_variance_precision</code>. The other three seem almost indistinguishable. We will dig in using Postmodeling. And afterwards instead of using the feature importance to characterize the facilities, we will explore how the model is splitting the facilities using crosstabs.</p> <p>As before, the best 3 model groups per strategy will be stored in the file <code>/triage/audition/eis/results_model_group_ids.json</code></p> <pre><code>{\n    \"most_frequent_best_dist_precision@_10_pct_0.05\": [\n        15,\n        16,\n        17,\n        9,\n        18\n    ],\n    \"lowest_metric_variance_precision@_10_pct\": [\n        2,\n        5,\n        19,\n        11,\n        6\n    ],\n    \"best_average_value_precision@_10_pct\": [\n        15,\n        16,\n        17,\n        18,\n        9\n    ],\n    \"best_current_value_precision@_10_pct\": [\n        9,\n        16,\n        15,\n        17,\n        18\n    ]\n}\n</code></pre>"},{"location":"dirtyduck/eis/#postmodeling-inspecting-the-best-models-closely","title":"Postmodeling: Inspecting the best models closely","text":"<p>Given that almost all the strategies perform well, we will change the parameter <code>model_group_id</code> in the postmodeling's configuration file and we will use the complete set of model groups selected by audition:</p> <pre><code># Postmodeling Configuration File\n\n  project_path: '/triage' # Project path defined in triage with matrices and models\n  audition_output_path: '/triage/audition/eis/results_model_group_ids.json'\n\n  thresholds: # Thresholds for2 defining positive predictions\n        rank_abs: [50, 100, 250]\n        rank_pct: [5, 10, 25]\n\n  baseline_query: | # SQL query for defining a baseline for comparison in plots. It needs a metric and parameter\n      select g.model_group_id,\n             m.model_id,\n             extract('year' from m.evaluation_end_time) as as_of_date_year,\n             m.metric,\n             m.parameter,\n             m.stochastic_value,\n             m.num_labeled_examples,\n             m.num_labeled_above_threshold,\n             m.num_positive_labels\n       from test_results.evaluations m\n       left join triage_metadata.models g\n       using(model_id)\n       where g.model_group_id = 20\n             and metric = 'precision@'\n             and parameter = '10_pct'\n\n  max_depth_error_tree: 5 # For error trees, how depth the decision trees should go?\n  n_features_plots: 10 # Number of features for importances\n  figsize: [12, 12] # Default size for plots\n  fontsize: 20 # Default fontsize for plots\n</code></pre> <p>Launch jupyter in <code>bastion</code>:</p> <pre><code>jupyter-notebook \u2013-ip=0.0.0.0  --port=56406 --allow-root\n</code></pre> <p>And then in your browser location bar type: http://0.0.0.0:56406</p>"},{"location":"dirtyduck/eis/#setup","title":"Setup","text":"<pre><code>%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom collections import OrderedDict\nfrom triage.component.postmodeling.contrast.utils.aux_funcs import create_pgconn, get_models_ids\nfrom triage.component.catwalk.storage import ProjectStorage, ModelStorageEngine, MatrixStorageEngine\nfrom triage.component.postmodeling.contrast.parameters import PostmodelParameters\nfrom triage.component.postmodeling.contrast.model_evaluator import ModelEvaluator\nfrom triage.component.postmodeling.contrast.model_group_evaluator import ModelGroupEvaluator\n\nparams = PostmodelParameters('../triage/eis_postmodeling_config.yaml')\n\nengine = create_pgconn('database.yaml')\n\n# Model group object (useful to compare across model_groups and models in time)\naudited_models_class = ModelGroupEvaluator(tuple(params.model_group_id), engine)\n</code></pre>"},{"location":"dirtyduck/eis/#model-groups","title":"Model groups","text":"<p>Let\u2019s start with the behavior in time of the selected model groups</p> <pre><code>audited_models_class.plot_prec_across_time(param_type='rank_pct',\n                                           param=10,\n                                           baseline=True,\n                                           baseline_query=params.baseline_query,\n                                           metric='precision@',\n                                           figsize=params.figsize)\n</code></pre> <p></p> <p>Every model selected by audition has a very similar performance across time, and they are ~2.5 times above the baseline in precision@10%. We could also check the recall of the model groups.</p> <pre><code>audited_models_class.plot_prec_across_time(param_type='rank_pct',\n                                           param=10,\n                                           metric='recall@',\n                                           figsize=params.figsize)\n</code></pre> <p></p> <p>That behavior is similar for the recall@10%, except for the model group 69</p> <pre><code>audited_models_class.plot_jaccard_preds(param_type='rank_pct',\n                                        param=10,\n                                        temporal_comparison=True)\n</code></pre> <p></p> <p>There are a high jaccard similarity between some model groups across time. This could be an indicator that they are so similar that you can choose any and it won\u2019t matter.</p>"},{"location":"dirtyduck/eis/#going-deeper-with-a-model","title":"Going deeper with a model","text":"<p>We will choose the model group 64 as the winner.</p> <pre><code>select\n    mg.model_group_id,\n    mg.model_type,\n    mg.hyperparameters,\n    array_agg(model_id order by train_end_time) as models\nfrom\n    triage_metadata.model_groups as mg\n    inner join\n    triage_metadata.models\n    using (model_group_id)\nwhere model_group_id = 76\ngroup by 1,2,3\n</code></pre> model<sub>group</sub><sub>id</sub> model<sub>type</sub> hyperparameters models 64 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max<sub>features</sub>\": \"sqrt\", \"n<sub>estimators</sub>\": 500, \"min<sub>samples</sub><sub>leaf</sub>\": 1, \"min<sub>samples</sub><sub>split</sub>\": 50} {190,208,226} <p>But before going to production and start making predictions in unseen data, let\u2019s see what the particular models are doing. Postmodeling created a <code>ModelEvaluator</code> (similar to the <code>ModelGroupEvaluator</code>) to do this exploration:</p> <pre><code>models_76 = { f'{model}': ModelEvaluator(76, model, engine) for model in [198,216,234] }\n</code></pre> <p>In this tutorial, we will just show some parts of the analysis in the most recent model, but feel free of exploring the behavior of all the models in this model group, and check if you can detect any pattern.</p> <ul> <li> <p>Feature importances</p> <pre><code>models_76['234'].plot_feature_importances(path=params.project_path,\n                                          n_features_plots=params.n_features_plots,\n                                          figsize=params.figsize)\n</code></pre> <p></p> <pre><code>models_76['234'].plot_feature_group_aggregate_importances()\n</code></pre> <p></p> </li> </ul>"},{"location":"dirtyduck/eis/#crosstabs-how-are-the-entities-classified","title":"Crosstabs: How are the entities classified?","text":"<p>Model interpretation is a huge topic nowadays, the most obvious path is using the features importance from the model. This could be useful, but we could do a lot better.</p> <p><code>Triage</code> uses <code>crosstabs</code> as a different approach that complements the list of features importance. <code>crosstabs</code> will run statistical tests to compare the predicted positive and the predicted false facilities in each feature.</p> <pre><code>output:\n  schema: 'test_results'\n  table: 'eis_crosstabs'\n\nthresholds:\n    rank_abs: [50]\n    rank_pct: [5]\n\n#(optional): a list of entity_ids to subset on the crosstabs analysis\nentity_id_list: []\n\nmodels_list_query: \"select unnest(ARRAY[226]) :: int as model_id\"\n\nas_of_dates_query: \"select generate_series('2017-12-01'::date, '2018-09-01'::date, interval '1month')  as as_of_date\"\n\n#don't change this query unless strictly necessary. It is just validating pairs of (model_id,as_of_date)\n#it is just a join with distinct (model_id, as_of_date) in a predictions table\nmodels_dates_join_query: |\n  select model_id,\n  as_of_date\n  from models_list_query as m\n  cross join as_of_dates_query a join (select distinct model_id, as_of_date from test_results.predictions) as p\n  using (model_id, as_of_date)\n\n#features_query must join models_dates_join_query with 1 or more features table using as_of_date\nfeatures_query: |\n  select m.model_id, m.as_of_date, f4.entity_id, f4.results_entity_id_1month_result_fail_avg, f4.results_entity_id_3month_result_fail_avg, f4.results_entity_id_6month_result_fail_avg,\n  f2.inspection_types_entity_id_1month_type_canvass_sum, f3.risks_entity_id_1month_risk_high_sum, f4.results_entity_id_6month_result_pass_avg,\n  f3.risks_entity_id_all_risk_high_sum, f2.inspection_types_entity_id_3month_type_canvass_sum, f4.results_entity_id_6month_result_pass_sum,\n  f2.inspection_types_entity_id_all_type_canvass_sum\n  from features.inspection_types_aggregation_imputed as f2\n  inner join features.risks_aggregation_imputed as f3 using (entity_id, as_of_date)\n  inner join features.results_aggregation_imputed as f4 using (entity_id, as_of_date)\n  inner join models_dates_join_query as m using (as_of_date)\n\n#the predictions query must return model_id, as_of_date, entity_id, score, label_value, rank_abs and rank_pct\n#it must join models_dates_join_query using both model_id and as_of_date\npredictions_query: |\n  select model_id,\n      as_of_date,\n      entity_id,\n      score,\n      label_value,\n      coalesce(rank_abs_no_ties, row_number() over (partition by (model_id, as_of_date) order by score desc)) as rank_abs,\n      coalesce(rank_pct_no_ties*100, ntile(100) over (partition by (model_id, as_of_date) order by score desc)) as rank_pct\n      from test_results.predictions\n      join models_dates_join_query using(model_id, as_of_date)\n      where model_id in (select model_id from models_list_query)\n      and as_of_date in (select as_of_date from as_of_dates_query)\n</code></pre> <pre><code>triage --tb crosstabs /triage/eis_crosstabs_config.yaml\n</code></pre> <p>When it finishes, you could explore the table with the following code:</p> <pre><code>with significant_features as (\nselect\n    feature_column,\n    as_of_date,\n    threshold_unit\nfrom\n    test_results.eis_crosstabs\nwhere\n    metric = 'ttest_p'\n    and\n    value &lt; 0.05 and as_of_date = '2018-09-01'\n    )\n\nselect\n    distinct\n    model_id,\n    as_of_date::date as as_of_date,\n    format('%s %s', threshold_value, t1.threshold_unit) as threshold,\n    feature_column,\n    value as \"ratio PP / PN\"\nfrom\n    test_results.eis_crosstabs as t1\n    inner join\n    significant_features as t2 using(feature_column, as_of_date)\nwhere\n    metric = 'ratio_predicted_positive_over_predicted_negative'\n    and\n    t1.threshold_unit = 'pct'\norder by value desc\n</code></pre> model<sub>id</sub> as<sub>of</sub><sub>date</sub> threshold feature<sub>column</sub> ratio PP / PN 226 2018-09-01 5 pct results<sub>entity</sub><sub>id</sub><sub>1month</sub><sub>result</sub><sub>fail</sub><sub>avg</sub> 11.7306052855925 226 2018-09-01 5 pct results<sub>entity</sub><sub>id</sub><sub>3month</sub><sub>result</sub><sub>fail</sub><sub>avg</sub> 3.49082798996376 226 2018-09-01 5 pct results<sub>entity</sub><sub>id</sub><sub>6month</sub><sub>result</sub><sub>fail</sub><sub>avg</sub> 1.27344759545161 226 2018-09-01 5 pct risks<sub>zip</sub><sub>code</sub><sub>1month</sub><sub>risk</sub><sub>high</sub><sub>sum</sub> 1.17488357227451 226 2018-09-01 5 pct inspection<sub>types</sub><sub>entity</sub><sub>id</sub><sub>all</sub><sub>type</sub><sub>canvass</sub><sub>sum</sub> 0.946432281075976 226 2018-09-01 5 pct inspection<sub>types</sub><sub>zip</sub><sub>code</sub><sub>3month</sub><sub>type</sub><sub>canvass</sub><sub>sum</sub> 0.888940127100436 226 2018-09-01 5 pct results<sub>entity</sub><sub>id</sub><sub>6month</sub><sub>result</sub><sub>pass</sub><sub>sum</sub> 0.041806916457784 226 2018-09-01 5 pct results<sub>entity</sub><sub>id</sub><sub>6month</sub><sub>result</sub><sub>pass</sub><sub>avg</sub> 0.0232523724927717 <p>This table represents the ratio between the predicted positives at the top 5% and predicted negatives (the rest). For example, you can see that in PP are eleven times more inspected if they have a failed inspection in the last month, 3.5 times more if they have a failed inspection in the previous 3 months, etc.</p>"},{"location":"dirtyduck/eis/#where-to-go-from-here","title":"Where to go from here","text":"<p>Ready to get started with your own data? Check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project.</p> <p>Want to work through another example? Take a look at our resource prioritization case study</p>"},{"location":"dirtyduck/for_the_impatient/","title":"For the impatient","text":"<p>If you want to skip all the cleaning and transformations and dive  directly into <code>triage</code> you can execute the following inside bastion:</p> <pre><code>     psql ${DATABASE_URL} -c \"\\copy raw.inspections from program 'curl \"https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD\"' HEADER CSV\"\n\n     psql ${DATABASE_URL} &lt; /sql/create_cleaned_inspections_table.sql\n\n     psql ${DATABASE_URL} &lt; /sql/create_violations_table.sql\n\n     psql ${DATABASE_URL} &lt; /sql/create_semantic_tables.sql\n</code></pre> <p>If everything works, you should end with two new schemas: <code>cleaned</code> and <code>semantic</code>.</p> <p>You could check that (from <code>psql</code>) With</p> <pre><code>\\dn\n</code></pre> List of schemas Name Owner cleaned food_user postgis food_user public postgres raw food_user semantic food_user"},{"location":"dirtyduck/infrastructure/","title":"Setting up the Infrastructure","text":"<p>In every data science project you will need several tools to help analyze the data in an efficient<sup>1</sup> manner. Examples include a place to store the data (e.g. database management system or DBMS); a way to put your model to work, e.g. a way that allows the model to ingest new data and make predictions (an API); and a way to examine the performance of trained models (e.g. monitor tools).</p> <p>This tutorial includes a script for managing the infrastructure<sup>2</sup> in a transparent way.</p> <p>The infrastructure of this tutorial has three pieces:</p> <ul> <li>a <code>postgresql</code> database called <code>food_db</code>,</li> <li>a container that executes <code>triage</code> experiments (we will use this when trying to scale up),</li> <li>a container for interacting with the data called <code>bastion</code>.</li> </ul> <p><code>bastion</code> includes a <code>postgresql</code> client (so you can interact with the database)<sup>3</sup> and a full <code>python</code> environment (so you can code or modify the things for the tutorial).</p> <p>The only thing you need installed on your laptop is <code>docker</code>.</p> <p>From your command line (terminal) run the following command from the repo directory:</p> <pre><code>    ./tutorial.sh\n</code></pre> <pre><code>Usage: ./tutorial.sh {up|down|build|rebuild|run|logs|status|clean}\n\nOPTIONS:\n   -h|help             Show this message\n   up                  Starts Food DB\n   down                Stops Food DB\n   build               Builds images (food_db and bastion)\n   rebuild             Builds images (food_db and bastion) ignoring if they already exists\n   -l|logs             Shows container's logs\n   status              Shows status of the containers\n   -d|clean            Removes containers, images, volumes, netrowrks\n\nINFRASTRUCTURE:\n   Build the DB's infrastructure:\n        $ ./tutorial.sh up\n\n   Check the status of the containers:\n        $ ./tutorial.sh status\n\n   Stop the tutorial's DB's infrastructure:\n        $ ./tutorial.sh down\n\n   Destroy all the resources related to the tutorial:\n        $ ./tutorial.sh clean\n\n   View the infrastructure logs:\n        $ ./tutorial.sh -l\n</code></pre> <p>Following the instructions on the screen, we can start the infrastructure with:</p> <pre><code>    ./tutorial.sh up\n</code></pre> <p>You can check that everything is running smoothly with <code>status</code> by using the following command:</p> <pre><code>    ./tutorial.sh status\n</code></pre> <pre><code>    Name                Command              State           Ports\n   ------------------------------------------------------------------------\n   tutorial_db   docker-entrypoint.sh postgres   Up      0.0.0.0:5434-&gt;5432/tcp\n</code></pre> <p>To access <code>bastion</code>, where the <code>postgresql</code> client is, submit the command:</p> <pre><code>   ./tutorial.sh bastion\n</code></pre> <p>Your prompt should change to something like:</p> <pre><code>[triage@dirtyduck$:/dirtyduck]#\n</code></pre> <p>NOTE: The number you see will be different (i.e. not <code>485373fb3c64</code>).</p> <p>Inside <code>bastion</code>, type the next command to connect to the database</p> <pre><code>   psql ${DATABASE_URL}\n</code></pre> <p>The prompt will change again to (or something very similar):</p> <pre><code>psql (12.3 (Debian 12.3-1.pgdg100+1), server 12.2 (Debian 12.2-2.pgdg100+1))\nType \"help\" for help.\n\nfood=#\n</code></pre> <p>The previous command is using <code>psql</code>, a powerful command line client for the Postgresql database. If you want to use this client fully, check psql's documentation.</p> <p>The database is now running and is named <code>food</code>. It should contain a single table named <code>inspections</code> in the <code>schema</code> <code>raw</code>. Let's check the structure of the <code>inspections</code> table. Type the following command:</p> <pre><code>    \\d raw.inspections\n</code></pre> Column Type Collation Nullable Default inspection character varying not null dba_name character varying aka_name character varying license_num numeric facility_type character varying risk character varying address character varying city character varying state character varying zip character varying date date type character varying results character varying violations character varying latitude numeric longitude numeric location character varying <p>That's it! We will work with this table of raw inspections data.</p> <p>You can disconnect from the database by typing <code>\\q</code>. But don't leave the database yet! We still need to do a lot of things<sup>4</sup></p> <ol> <li> <p>Reproducible, scalable, flexible, etc.\u00a0\u21a9</p> </li> <li> <p>And other things through this tutorial, like the execution of the model training, etc.\u00a0\u21a9</p> </li> <li> <p>If you have a postgresql client installed, you can use <code>psql -h 0.0.0.0 -p 5434 -d food -U food_user</code> rather than the <code>bastion</code> container.\u00a0\u21a9</p> </li> <li> <p>Welcome to the not-so-sexy part of the (supposedly) sexiest job of the XXI century.\u00a0\u21a9</p> </li> </ol>"},{"location":"dirtyduck/inspections/","title":"Resource prioritization systems: Chicago food inspections","text":"<p>Before continue, Did you\u2026?</p> <p>This case study, part of the dirtyduck tutorial,  assumes that you already setup the tutorial\u2019s infrastructure and load the dataset.</p> <ul> <li> <p>If you didn\u2019t setup the infrastructure go here,</p> </li> <li> <p>If you didn't load the data, you can do it very quickly   or you can follow all the steps and explanations about the data.</p> </li> </ul>"},{"location":"dirtyduck/inspections/#problem-description","title":"Problem description","text":"<p>We want to generate a list of facilities that will have a critical or serious food violation if inspected.</p> <p>The scenario is the following: you work for the City of Chicago and you have limited food inspectors, so you try to prioritize them to focus on the highest-risk facilities. So you will use the data to answer the next question:</p> <p>Which X facilities are most likely to fail a food inspection in the following Y period of time?</p> <p>A more technical way of writing the question is: What is the probability distribution of facilities that are at risk of fail a food inspection if they are inspected in the following period of time?<sup>1</sup></p> <p>If you want to focus on major violations only, you can do that too:</p> <p>Which X facilities are most likely to have a critical or serious violation in the following Y period of time?</p> <p>This situation is very common in governmental agencies that provide social services: they need to prioritize their resources and use them in the facilities that are most likely to have problems</p> <p>We will use machine learning to accomplish this. This means that we will use historical data to train our models, and we will use temporal cross validation to test the performance of them.</p> <p>For the resource prioritization problems there are commonly two problems with the data: (a) bias and (b) incompleteness.</p> <p>First, note that our data have bias: We only have data on facilities that were inspected. That means that our data set contains information about the probability of have a violation (V) given that the facility was inspected (I), P(V|I). But the probability that we try to find is P(V).</p> <p>A different problem that our data set could have is if our dataset contains all the facilities in Chicago, i.e. if our entities table represents the Universe of facilities. There are almost 40,000 entities in our database. We could make the case that every facility in Chicago is in the database, since every facility that opens will be subject to an inspection. We will assume that all the facilities are in our data.</p>"},{"location":"dirtyduck/inspections/#what-do-you-want-to-predict","title":"What do you want to predict?","text":"<p>We are interested in two different outcomes:</p> <ul> <li>Which facilities are likely to fail an inspection?</li> </ul> <p>The outcome takes a 1 if the inspection had at least one <code>result</code> = <code>'fail'</code> and a 0 otherwise.</p> <ul> <li>Which facilities fail an inspection with a major violation?</li> </ul> <p>Critical violations are coded between <code>1-14</code>, serious violations between <code>15-29</code>, everything above <code>30</code> is assumed to be a minor violation. The label takes a 1 if the inspection had at least one <code>result</code> = <code>'fail'</code> and a violation between 1 and 29, and a 0 otherwise.</p> <p>I want to learn more about the data</p> <p>Check the Data preparation section!</p> <p>Data Changes</p> <p>On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here.</p> <p>We can extract the severity of the violation using the following code:</p> <pre><code>select\n    event_id,\n    entity_id,\n    date,\n    result,\n    array_agg(distinct obj -&gt;&gt;'severity') as violations_severity,\n    (result = 'fail') as failed,\n    coalesce(\n    (result = 'fail' and\n        ('serious' = ANY(array_agg(obj -&gt;&gt; 'severity')) or 'critical' = ANY(array_agg(obj -&gt;&gt; 'severity')))\n        ), false\n    ) as failed_major_violation\nfrom\n    (\n    select\n        event_id,\n        entity_id,\n        date,\n        result,\n        jsonb_array_elements(violations::jsonb) as obj\n    from\n        semantic.events\n        limit 20\n        ) as t1\ngroup by\n    entity_id, event_id, date, result\norder by\n   date desc;\n</code></pre> event_id entity_id date result violations_severity failed failed_major_violation 1770568 30841 2016-05-11 pass {minor} f f 1763967 30841 2016-05-03 fail {critical,minor,serious} t t 1434534 21337 2014-04-03 pass {NULL} f f 1343315 22053 2013-06-06 fail {minor,serious} t t 1235707 21337 2013-03-27 pass {NULL} f f 537439 13458 2011-06-10 fail {NULL} t f 569377 5570 2011-06-01 pass {NULL} f f <p>The outcome will be used by <code>triage</code> to generate the labels (once that we define a time span of interest). The following image tries to show the meaning of the outcomes for the inspection failed problem definition.</p> <p> Figure. The image shows three facilities (blue, red and orange) and, next to each, a temporal line with 6 days (0-5). Each dot represents an inspection. Color is the outcome of the inspection. Green means the facility passed the inspection, and red means it failed. Each facility in the image had two inspections, but only the facility in the middle passed both.</p>"},{"location":"dirtyduck/inspections/#modeling-using-machine-learning","title":"Modeling Using Machine Learning","text":"<p>It is time to put these steps together. All the coding is complete (<code>triage</code> dev team did that for us); we just need to modify the <code>triage</code> experiment\u2019s configuration file.</p>"},{"location":"dirtyduck/inspections/#defining-a-baseline","title":"Defining a baseline","text":"<p>As a first step, lets do an experiment that defines our baseline. The rationale of this is that the knowing the baseline will allow us to verify if our Machine Learning model is better than the baseline. The baseline in our example will be a random selection of facilities<sup>2</sup>. This is implemented as a <code>DummyClassifier</code> in <code>scikit-learn</code>. Another advantage of starting with the baseline, is that is very fast to train (<code>DummyClassifier</code> is not computationally expensive) , so it will help us to verify that the experiment configuration is correct without waiting for a long time.</p> <p>Experiment description file</p> <p>You could check the meaning about experiment description files (or configuration files) in A deeper look into triage.</p> <p>We need to write the experiment config file for that. Let's break it down and explain their sections.</p> <p>The config file for this first experiment is located in inspections_baseline.yaml.</p> <p>The first lines of the experiment config file specify the config-file version (<code>v8</code> at the moment of writing this tutorial), a comment (<code>model_comment</code>, which will end up as a value in the <code>triage_metadata.models</code> table), and a list of user-defined metadata (<code>user_metadata</code>) that can help to identify the resulting model groups. For this example, if you run experiments that share a temporal configuration but that use different label definitions (say, labeling inspections with any violation as positive versus only labeling inspections with major violations as positive), you can use the user metadata keys to indicate that the matrices from these experiments have different labeling criteria. The matrices from the two experiments will have different filenames (and should not be overwritten or incorrectly used), and if you add the <code>label_definition</code> key to the <code>model_group_keys</code>, models made on different label definitions will belong to different model groups.</p> <pre><code>config_version: 'v8'\n\nmodel_comment: 'inspections: baseline'\nrandom_seed: 23895478\n\nuser_metadata:\n    label_definition: 'failed'\n    file_name: 'inspections_baseline.yaml'\n    experiment_type: 'inspections prioritization'\n    description: |\n      Baseline calculation\n    purpose: 'baseline'\n    org: 'DSaPP'\n    team: 'Tutorial'\n    author: 'Your name here'\n    etl_date: '2019-05-07'\n\nmodel_group_keys:\n  - 'class_path'\n  - 'parameters'\n  - 'feature_names'\n  - 'feature_groups'\n  - 'cohort_name'\n  - 'state'\n  - 'label_name'\n  - 'label_timespan'\n  - 'training_as_of_date_frequency'\n  - 'max_training_history'\n  - 'label_definition'\n  - 'experiment_type'\n  - 'org'\n  - 'team'\n  - 'author'\n  - 'etl_date'\n</code></pre> <p>Note</p> <p>Obviously, change <code>'Your name here'</code> for your name (if you like)</p> <p>Next comes the temporal configuration section. The first four parameters are related to the availability of data: How much data you have for feature creation? How much data you have for label generation?</p> <p>Data Changes</p> <p>On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here.</p> <p>The next parameters are related to the training intervals:</p> <ul> <li>How frequently to retrain models? (<code>model_update_frequency</code>)</li> <li>How many rows per entity in the train matrices? (<code>training_as_of_date_frequencies</code>)</li> <li>How much time is covered by labels in the training matrices? (<code>training_label_timespans</code>)</li> </ul> <p>The remaining elements are related to the testing matrices. For inspections, you can choose them as follows:</p> <ul> <li><code>test_as_of_date_frequencies</code> is planning/scheduling frequency</li> <li><code>test_durations</code> how far ahead do you schedule inspections?</li> <li><code>test_label_timespan</code> is equal to <code>test_durations</code></li> </ul> <p>Let's assume that we need to do rounds of inspections every 6 months (<code>test_as_of_date_frequencies = 6month</code>) and we need to complete that round in exactly in that time (i.e. 6 months) (<code>test_durations = test_label_timespan = 6month</code>).</p> <p>We will assume that the data is more or less stable<sup>3</sup>, at least for now, so <code>model_update_frequency</code> = <code>6month.</code></p> <pre><code>temporal_config:\n    feature_start_time: '2010-01-04'\n    feature_end_time: '2018-06-01'\n    label_start_time: '2015-01-01'\n    label_end_time: '2018-06-01'\n\n    model_update_frequency: '6month'\n    training_label_timespans: ['6month']\n    training_as_of_date_frequencies: '6month'\n\n    test_durations: '0d'\n    test_label_timespans: ['6month']\n    test_as_of_date_frequencies: '6month'\n\n    max_training_histories: '5y'\n</code></pre> <p>We can visualize the time splitting using the function <code>show-timechop</code> (See A deeper look into triage for more information)</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntriage experiment experiments/inspections_baseline.yaml --show-timechop\n</code></pre> <p> Figure. Temporal blocks for the inspections baseline experiment</p> <p>We need to specify our labels. For this first experiment we will use the label <code>failed</code>, using the same query from the <code>simple_skeleton_experiment.yaml</code></p> <pre><code>label_config:\n  query: |\n    select\n    entity_id,\n    bool_or(result = 'fail')::integer as outcome\n    from semantic.events\n    where '{as_of_date}'::timestamp &lt;= date\n    and date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\n    group by entity_id\n  name: 'failed_inspections'\n</code></pre> <p>It should be obvious, but let's state it anyway: We are only training in facilities that were inspected, but we will test our model in all the facilities in our cohort<sup>4</sup>. So, in the train matrices we will have only <code>0</code> and <code>1</code> as possible labels, but in the test matrices we will found <code>0</code>, <code>1</code> and <code>NULL</code>.</p> <p>I want to learn more about this\u2026</p> <p>In the section regarding to Early Warning Systems we will learn how to incorporate all the facilities of the cohort in the train matrices.</p> <p>We just want to include active facilities in our matrices, so we tell <code>triage</code> to take that in account:</p> <pre><code>cohort_config:\n  query: |\n    select e.entity_id\n    from semantic.entities as e\n    where\n    daterange(start_time, end_time, '[]') @&gt; '{as_of_date}'::date\n  name: 'active_facilities'\n</code></pre> <p><code>Triage</code> will generate the features for us, but we need to tell it which features we want in the section <code>feature_aggregations</code>. Here, each entry describes a <code>collate.SpacetimeAggregation</code> object and the arguments needed to create it<sup>5</sup>. For this experiment, we will use only one feature (number of inspections). <code>DummyClassifier</code> don't use any feature to do the \"prediction\", so we won't expend compute cycles doing the feature/matrix creation:</p> <pre><code>feature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates_imputation:\n      count:\n        type: 'zero_noflag'\n\n    aggregates:\n       -\n        quantity:\n          total: \"*\"\n        metrics:\n          - 'count'\n\n    intervals: ['all']\n\nfeature_group_definition:\n   prefix:\n     - 'inspections'\n\nfeature_group_strategies: ['all']\n</code></pre> <p>If we observe the image generated from the <code>temporal_config</code> section, each particular date is the beginning of the rectangles that describes the rows in the matrix. In that date (<code>as_of_date</code> in <code>timechop</code> parlance) we will calculate the feature, and we will repeat that for every other rectangle in that image.</p> <p>Now, let's discuss how we will specify the models to try (remember that the model is specified by the algorithm, the hyperparameters, and the subset of features to use). In <code>triage</code> you need to specify in the <code>grid_config</code> section a list of machine learning algorithms that you want to train and a list of hyperparameters. You can use any algorithm that you want; the only requirement is that it respects the <code>sklearn</code> API.</p> <pre><code>grid_config:\n    'sklearn.dummy.DummyClassifier':\n        strategy: [uniform]\n</code></pre> <p>Finally, we should define wich metrics we care about for evaluating our model. Here we will concentrate only in <code>precision</code> and <code>recall</code> at an specific value k <sup>6</sup>.</p> <p>In this setting k represents the resource\u2019s constraint: It is the number of inspections that the city could do in a month given all the inspectors available.</p> <pre><code>scoring:\n    testing_metric_groups:\n        -\n          metrics: [precision@, recall@, 'false negatives@', 'false positives@', 'true positives@', 'true negatives@']\n          thresholds:\n            percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n            top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]\n        -\n          metrics: [auc, accuracy]\n\n    training_metric_groups:\n      -\n        metrics: [auc, accuracy]\n      -\n        metrics: [precision@, recall@]\n        thresholds:\n          percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n          top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]\n</code></pre> <p>You should be warned that precision and recall at k in this setting is kind of ill-defined (because you will end with a lot of <code>NULL</code> labels, remember, only a few of facilities are inspected in each period)<sup>7</sup>.</p> <p>We will want a list of facilities to be inspected. The length of our list is constrained by our inspection resources, i.e. the answer to the question How many facilities can I inspect in a month? In this experiment we are assuming that the maximum capacity is 10% but we are evaluating for a larger space of possibilities (see <code>top_n</code>, <code>percentiles</code> above).</p> <p>The execution of the experiments can take a long time, so it is a good practice to validate the configuration file before running the model. You don't want to wait for hours (or days) and then discover that something went wrong.</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntriage experiment experiments/inspections_baseline.yaml  --validate-only\n</code></pre> <p>If everything was ok, you should see an <code>Experiment validation ran to completion with no errors</code>.</p> <p>You can execute the experiment as<sup>8</sup></p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntime triage experiment experiments/inspections_baseline.yaml\n</code></pre> <p>Protip</p> <p>We are including the command <code>time</code> in order to get the total running time of the experiment. You can remove it, if you like.</p> <p>Don\u2019t be scared!</p> <p>This will print a lot of output! It is not an error!</p> <p>We can query the table <code>experiments</code> to see the quantity of work that <code>triage</code> needs to do</p> <pre><code>select\n    substring(experiment_hash, 1,4) as experiment,\n    config -&gt; 'user_metadata' -&gt;&gt; 'description'  as description,\n    total_features,\n    matrices_needed,\n    models_needed\nfrom triage_metadata.experiments;\n</code></pre> experiment description total_features matrices_needed models_needed e912 \"Baseline calculation\\n\" 1 10 5 <p>If everything is correct, <code>triage</code>  will create 10 matrices (5 for training, 5 for testing) in <code>triage/matrices</code> and every matrix will be represented by two files, one with the metadata of the matrix (a <code>yaml</code> file) and one with the actual matrix (the <code>gz</code> file).</p> <pre><code># We will use some bash magic\n\nls matrices | awk -F . '{print $NF}' | sort | uniq -c\n</code></pre> <p><code>Triage</code> also will store 5 trained models in <code>triage/trained_models</code>:</p> <pre><code>ls trained_models | wc -l\n</code></pre> <p>And it will populate the <code>results</code> schema in the database. As mentioned, we will get 1 model groups:</p> <pre><code>select\n    model_group_id,\n    model_type,\n    hyperparameters\nfrom\n    triage_metadata.model_groups\nwhere\n    model_config -&gt;&gt; 'experiment_type' ~ 'inspection'\n</code></pre> model_group_id model_type hyperparameters 1 sklearn.dummy.DummyClassifier {\"strategy\": \"prior\"} <p>And 5 models:</p> <pre><code>select\n    model_group_id,\n    array_agg(model_id) as models,\n    array_agg(train_end_time::date) as train_end_times\nfrom\n    triage_metadata.models\nwhere\n    model_comment ~ 'inspection'\ngroup by\n    model_group_id\norder by\n    model_group_id;\n</code></pre> model_group_id models train_end_times 1 {1,2,3,4,5} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} <p>From that last query, you should note that the order in which <code>triage</code> trains the models is from oldest to newest <code>train_end_time</code> and <code>model_group</code> , also in ascending order. It will not go to the next block until all the models groups are trained.</p> <p>You can check on which matrix each models were trained:</p> <pre><code>select\n    model_group_id,\n    model_id, train_end_time::date,\n    substring(model_hash,1,5) as model_hash,\n    substring(train_matrix_uuid,1,5) as train_matrix_uuid,\n    ma.num_observations as observations,\n    ma.lookback_duration as feature_lookback_duration,  ma.feature_start_time\nfrom\n    triage_metadata.models as mo\n    join\n    triage_metadata.matrices as ma\n    on train_matrix_uuid = matrix_uuid\nwhere\n    mo.model_comment ~ 'inspection'\norder by\n    model_group_id,\n    train_end_time asc;\n</code></pre> model_group_id model_id train_end_time model_hash train_matrix_uuid observations feature_lookback_duration feature_start_time 1 1 2015-12-01 743a6 1c266 5790 5 years 2010-01-04 00:00:00 1 2 2016-06-01 5b656 755c6 11502 5 years 2010-01-04 00:00:00 1 3 2016-12-01 5c170 2aea8 17832 5 years 2010-01-04 00:00:00 1 4 2017-06-01 55c3e 3efdc 23503 5 years 2010-01-04 00:00:00 1 5 2017-12-01 ac993 a3762 29112 5 years 2010-01-04 00:00:00 <p>Each model was trained with the matrix indicated in the column <code>train_matrix_uuid</code>. This <code>uuid</code> is the file name of the stored matrix. The model itself was stored under the file named with the <code>model_hash</code>.</p> <p>If you want to see in which matrix the model was tested you need to run the following query</p> <pre><code>select distinct\n    model_id,\n    model_group_id, train_end_time::date,\n    substring(model_hash,1,5) as model_hash,\n    substring(ev.matrix_uuid,1,5) as test_matrix_uuid,\n    ma.num_observations as observations\nfrom\n    triage_metadata.models as mo\n    join\n    test_results.evaluations as ev using (model_id)\n    join\n    triage_metadata.matrices as ma on ev.matrix_uuid = ma.matrix_uuid\nwhere\n    mo.model_comment ~ 'inspection'\norder by\n    model_group_id, train_end_time asc;\n</code></pre> model_id model_group_id train_end_time model_hash test_matrix_uuid observations 1 1 2015-12-01 743a6 4a0ea 18719 2 1 2016-06-01 5b656 f908e 19117 3 1 2016-12-01 5c170 00a88 19354 4 1 2017-06-01 55c3e 8f3cf 19796 5 1 2017-12-01 ac993 417f0 20159 <p>All the models were stored in <code>/triage/trained_models/{model_hash}</code> using the standard serialization of sklearn models. Every model was trained with the matrix <code>train_matrix_uuid</code> stored in the directory <code>/triage/matrices</code>.</p> <p>What's the performance of this model groups?</p> <pre><code>\\set k 0.10    -- This defines a variable, \"k = 0.10\"\n\nselect distinct\n    model_group_id,\n    model_id,\n    ma.feature_start_time::date,\n    train_end_time::date,\n    ev.evaluation_start_time::date,\n    ev.evaluation_end_time::date,\n    to_char(ma.num_observations, '999,999') as observations,\n    to_char(ev.num_labeled_examples, '999,999') as \"total labeled examples\",\n    to_char(ev.num_positive_labels, '999,999') as \"total positive labels\",\n    to_char(ev.num_labeled_above_threshold, '999,999') as \"labeled examples@k%\",\n    to_char(:k * ma.num_observations, '999,999') as \"predicted positive (PP)\",\n    ARRAY[to_char(ev.best_value * ev.num_labeled_above_threshold,'999,999'),\n          to_char(ev.worst_value * ev.num_labeled_above_threshold,'999,999'),\n          to_char(ev.stochastic_value * ev.num_labeled_above_threshold, '999,999')]\n    as \"true positive (TP)@k% (best,worst,stochastic)\",\n    ARRAY[ to_char(ev.best_value, '0.999'), to_char(ev.worst_value, '0.999'),\n    to_char(ev.stochastic_value, '0.999')]  as \"precision@k% (best,worst,stochastic)\",\n    to_char(ev.num_positive_labels*1.0 / ev.num_labeled_examples, '0.999') as baserate,\n    :k * 100 as \"k%\"\nfrom\n    triage_metadata.models as mo\n    join\n    test_results.evaluations as ev using (model_id)\n    join\n    triage_metadata.matrices as ma on ev.matrix_uuid = ma.matrix_uuid\nwhere\n    ev.metric || ev.parameter = 'precision@15_pct'\n    and\n    mo.model_comment ~ 'inspection'\norder by\n    model_id, train_end_time asc;\norder by\n    model_id, train_end_time asc;\n</code></pre> model_group_id model_id feature_start_time train_end_time evaluation_start_time evaluation_end_time observations total labeled examples total positive labels labeled examples@k% predicted positive (PP) true positive (TP)@k% (best,worst,stochastic) precision@k% (best,worst,stochastic) baserate k% 21 121 2010-01-04 2015-12-01 2015-12-01 2015-12-01 18,719 5,712 1,509 0 2,808 {\"       0\",\"       0\",\"       0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.264 15.00 21 122 2010-01-04 2016-06-01 2016-06-01 2016-06-01 19,117 6,330 1,742 0 2,868 {\"       0\",\"       0\",\"       0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.275 15.00 21 123 2010-01-04 2016-12-01 2016-12-01 2016-12-01 19,354 5,671 1,494 0 2,903 {\"       0\",\"       0\",\"       0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.263 15.00 21 124 2010-01-04 2017-06-01 2017-06-01 2017-06-01 19,796 5,609 1,474 0 2,969 {\"       0\",\"       0\",\"       0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.263 15.00 21 125 2010-01-04 2017-12-01 2017-12-01 2017-12-01 20,159 4,729 1,260 0 3,024 {\"       0\",\"       0\",\"       0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.266 15.00 <p>The columns <code>num_labeled_examples, num_labeled_above_threshold, num_positive_labels</code> represent the number of selected entities on the prediction date that are labeled, the number of entities with a positive label above the threshold, and the number of entities with positive labels among all the labeled entities respectively.</p> <p>We added some extra columns: <code>baserate</code>, <code>predicted positive (PP)</code> and <code>true positive (TP)</code>. Baserate represents the proportion of the all the facilities that were inspected that failed the inspection, i.e. P(V|I). The <code>PP</code> and <code>TP</code> are approximate since it were calculated using the value of k or the precision value. But you could get the exact value of those from the <code>test_results.predictions</code> table.</p> <p>Also note that in the <code>precision@k%</code> column we are showing three numbers: best, worst, stochastic.</p> <p>They try to answer the question How do you break ties in the prediction score? This is important because it will affect the calculation of your metrics. The <code>Triage</code> proposed solution to this is calculate the metric in the best case scenario (score descending, all the true labels are at the top), and then do it in the worst case scenario (score descending, all the true labels are at the bottom) and then calculate the metric several times (n=30) with the labels randomly shuffled (a.k.a. stochastic scenario), so you get the mean metric, plus some confidence intervals.</p> <p>This problem is not specific of an inspection problem, is more related to simple models like a shallow <code>Decision Tree</code> or a <code>Dummy Classifier</code> when score ties likely will occur.</p> <p>Note how in this model, the stochastic value is close to the baserate, since we are selecting at random using the prior.</p> <p>Check this!</p> <p>Note that the baserate should be equal to the precision@100%, if is not there is something wrong \u2026</p>"},{"location":"dirtyduck/inspections/#creating-a-simple-experiment","title":"Creating a simple experiment","text":"<p>We will try two of the simplest machine learning algorithms: a Decision Tree Classifier (DT) and a  Scaled Logistic Regression (SLR)<sup>12</sup> as a second experiment. The rationale of this is that the DT is very fast to train (so it will help us to verify that the experiment configuration is correct without waiting for a long time) and it helps you to understand the structure of your data.</p> <p>The config file for this first experiment is located in <code>/triage/experiments/inspections_dt.yaml</code></p> <p>Note that we don't modify the <code>temporal_config</code> section neither the <code>feature_aggregations</code>, <code>cohort_config</code> or <code>label_config</code>. Triage is smart enough to use the previous tables and matrices instead of generating them from scratch.</p> <pre><code>config_version: 'v8'\n\nmodel_comment: 'inspections: basic ML'\n\nuser_metadata:\n  label_definition: 'failed'\n  experiment_type: 'inspections prioritization'\n  file_name: 'inspections_dt.yaml'\n  description: |\n    DT and SLR\n  purpose: 'data mining'\n  org: 'DSaPP'\n  team: 'Tutorial'\n  author: 'Your name here'\n  etl_date: '2019-02-21'\n</code></pre> <p>Note that we don't modify the <code>temporal_config</code> section neither the <code>cohort_config</code> or <code>label_config</code>. Triage is smart enough to use the previous tables and matrices instead of generating them from scratch.</p> <p>For this experiment, we will add the following features:</p> <ul> <li> <p>Number of different types of inspections the facility had in the last year (calculated for an as-of-date).</p> </li> <li> <p>Number of different types of inspections that happened in the zip code in the last year from a particular day.</p> </li> <li> <p>Number of inspections</p> </li> <li> <p>Number/proportion of inspections by result type</p> </li> <li> <p>Number/proportion of times that a facility was classify with particular risk level</p> </li> </ul> <p>In all of them we will do the aggregation in the last month, 3 months, 6 months, 1 year and historically. Remember that all this refers to events in the past, i.e. How many times the facility was marked with high risk in the previous 3 Months?, What is the proportion of failed inspections in the previous year?</p> <pre><code>feature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates_imputation:\n      count:\n        type: 'zero_noflag'\n\n    aggregates:\n      -\n        quantity:\n          total: \"*\"\n        metrics:\n          - 'count'\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n\n  -\n    prefix: 'risks'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    categoricals_imputation:\n      sum:\n        type: 'zero'\n      avg:\n        type: 'zero'\n\n    categoricals:\n      -\n        column: 'risk'\n        choices: ['low', 'medium', 'high']\n        metrics:\n          - 'sum'\n          - 'avg'\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n\n  -\n    prefix: 'results'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    categoricals_imputation:\n      all:\n        type: 'zero'\n\n    categoricals:\n      -\n        column: 'result'\n        choice_query: 'select distinct result from semantic.events'\n        metrics:\n          - 'sum'\n          - 'avg'\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n\n  -\n    prefix: 'inspection_types'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    categoricals_imputation:\n      sum:\n        type: 'zero_noflag'\n\n    categoricals:\n      -\n        column: 'type'\n        choice_query: 'select distinct type from semantic.events where type is not null'\n        metrics:\n          - 'sum'\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n</code></pre> <p>And as stated, we will train some Decision Trees, in particular we are interested in some shallow trees, and in a full grown tree. These trees will show you the structure of your data. We also will train some Scaled Logistic Regression, this will show us how \"linear\" is the data (or how the assumptions of the Logistic Regression holds in this data)</p> <pre><code>grid_config:\n  'sklearn.tree.DecisionTreeClassifier':\n        criterion: ['gini']\n        max_features: ['sqrt']\n        max_depth: [1, 2, 5,~]\n        min_samples_split: [2,10,50]\n\n  'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression':\n        penalty: ['l1','l2']\n        C: [0.000001, 0.0001, 0.01,  1.0]\n</code></pre> <p>About <code>yaml</code> and <code>sklearn</code></p> <p>Some of the parameters in <code>sklearn</code> are <code>None</code>. If you want to try those you need to indicate it with <code>yaml</code>'s <code>null</code> or <code>~</code> keyword.</p> <p>Besides the algorithm and the hyperparameters, you should specify which subset of features use. First, in the section <code>feature_group_definition</code> you specify how to group the features (you can use the <code>table name</code> or the <code>prefix</code> from the section <code>feature_aggregation</code>) and then a strategy for choosing the subsets: <code>all</code> (all the subsets at once), <code>leave-one-out</code> (try all the subsets except one, do that for all the combinations), or <code>leave-one-in</code> (just try subset at the time).</p> <pre><code>feature_group_definition:\n   prefix:\n     - 'inspections'\n     - 'results'\n     - 'risks'\n     - 'inspection_types'\n\nfeature_group_strategies: ['all']\n</code></pre> <p>Finally we will leave the <code>scoring</code> section as before.</p> <p>In this experiment we will end with 6 model groups (number of algorithms [1] \\times number of hyperparameter combinations [2 \\times 3 = 5] \\times number of feature groups strategies [1]]). Also, we will create 18 models (3 per model group) given that we have 3 temporal blocks (one model per temporal group).</p> <p>Before running the experiment, remember to validate that the configuration is correct:</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntriage experiment experiments/inspections_dt.yaml  --validate-only\n</code></pre> <p>and check the temporal cross validation:</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntriage experiment experiments/inspections_dt.yaml --show-timechop\n</code></pre> <p> Temporal blocks for inspections experiment. The label is a failed inspection in the next year.</p> <p>You can execute the experiment like this:</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntime triage experiment experiments/inspections_dt.yaml\n</code></pre> <p>Again, we can run the following <code>sql</code> to see which things <code>triage</code> needs to run:</p> <pre><code>select\n    substring(experiment_hash, 1,4) as experiment,\n    config -&gt; 'user_metadata' -&gt;&gt; 'description'  as description,\n    total_features,\n    matrices_needed,\n    models_needed\nfrom triage_metadata.experiments;\n</code></pre> experiment description total_features matrices_needed models_needed e912 Baseline calculation 1 10 5 b535 DT and SLR 201 10 100 <p>You can compare our two experiments and there are several differences, mainly in the order of magnitude. Like the number of features (1 vs 201) and models built (5 vs 100).</p> <p>The After the experiment finishes, you will get 19 new <code>model_groups</code> (1 per combination in <code>grid_config</code>)</p> <pre><code>select\n    model_group_id,\n    model_type,\n    hyperparameters\nfrom\n    triage_metadata.model_groups\nwhere\n    model_group_id not in (1);\n</code></pre> model_group_id model_type hyperparameters 2 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 3 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 4 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 5 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 6 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 7 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 8 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 9 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 10 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 11 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 12 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 13 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 14 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l1\"} 15 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l2\"} 16 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l1\"} 17 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l2\"} 18 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l1\"} 19 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l2\"} 20 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l1\"} 21 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l2\"} <p>and 100 models (as stated before)</p> <pre><code>select\n    model_group_id,\n    array_agg(model_id) as models,\n    array_agg(train_end_time) as train_end_times\nfrom\n    triage_metadata.models\nwhere\n    model_group_id not in (1)\ngroup by\n    model_group_id\norder by\n    model_group_id;\n</code></pre> model_group_id models train_end_times 2 {6,26,46,66,86} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 3 {7,27,47,67,87} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 4 {8,28,48,68,88} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 5 {9,29,49,69,89} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 6 {10,30,50,70,90} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 7 {11,31,51,71,91} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 8 {12,32,52,72,92} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 9 {13,33,53,73,93} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 10 {14,34,54,74,94} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 11 {15,35,55,75,95} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 12 {16,36,56,76,96} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 13 {17,37,57,77,97} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 14 {18,38,58,78,98} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 15 {19,39,59,79,99} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 16 {20,40,60,80,100} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 17 {21,41,61,81,101} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 18 {22,42,62,82,102} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 19 {23,43,63,83,103} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 20 {24,44,64,84,104} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 21 {25,45,65,85,105} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} <p>Let's see the performance over time of the models so far:</p> <pre><code>select\n    model_group_id,\n    array_agg(model_id order by ev.evaluation_start_time asc) as models,\n    array_agg(ev.evaluation_start_time::date order by ev.evaluation_start_time asc) as evaluation_start_time,\n    array_agg(ev.evaluation_end_time::date order by ev.evaluation_start_time asc) as evaluation_end_time,\n    array_agg(to_char(ev.num_labeled_examples, '999,999') order by ev.evaluation_start_time asc) as labeled_examples,\n    array_agg(to_char(ev.num_labeled_above_threshold, '999,999') order by ev.evaluation_start_time asc) as labeled_above_threshold,\n    array_agg(to_char(ev.num_positive_labels, '999,999') order by ev.evaluation_start_time asc) as total_positive_labels,\n    array_agg(to_char(ev.stochastic_value, '0.999') order by ev.evaluation_start_time asc) as \"precision@15%\"\nfrom\n    triage_metadata.models as mo\n    inner join\n    triage_metadata.model_groups as mg using(model_group_id)\n    inner join\n    test_results.evaluations  as ev using(model_id)\nwhere\n     mg.model_config -&gt;&gt; 'experiment_type' ~ 'inspection'\n     and\n     ev.metric||ev.parameter = 'precision@15_pct'\n     and model_group_id between 2 and 21\ngroup by\n    model_group_id\n</code></pre> model_group_id models evaluation_start_time evaluation_end_time labeled_examples labeled_above_threshold total_positive_labels precision@15% 1 {1,2,3,4,5} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"       0\",\"       0\",\"       0\",\"       0\",\"       0\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 2 {6,26,46,66,86} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"       0\",\"     578\",\"     730\",\"     574\",\"       0\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.000\",\" 0.352\",\" 0.316\",\" 0.315\",\" 0.000\"} 3 {7,27,47,67,87} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     161\",\"     622\",\"       0\",\"     433\",\"       0\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.283\",\" 0.203\",\" 0.000\",\" 0.282\",\" 0.000\"} 4 {8,28,48,68,88} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     161\",\"   1,171\",\"       0\",\"       0\",\"     995\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.285\",\" 0.348\",\" 0.000\",\" 0.000\",\" 0.306\"} 5 {9,29,49,69,89} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     726\",\"   1,318\",\"     362\",\"     489\",\"     291\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.306\",\" 0.360\",\" 0.213\",\" 0.294\",\" 0.241\"} 6 {10,30,50,70,90} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     992\",\"     730\",\"       0\",\"     176\",\"     290\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.294\",\" 0.349\",\" 0.000\",\" 0.324\",\" 0.334\"} 7 {11,31,51,71,91} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"   1,023\",\"     325\",\"     329\",\"     176\",\"   1,033\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.323\",\" 0.400\",\" 0.347\",\" 0.323\",\" 0.315\"} 8 {12,32,52,72,92} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"   1,250\",\"   1,013\",\"     737\",\"   1,104\",\"     939\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.331\",\" 0.280\",\" 0.327\",\" 0.335\",\" 0.365\"} 9 {13,33,53,73,93} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     595\",\"     649\",\"     547\",\"   1,000\",\"     841\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.281\",\" 0.250\",\" 0.309\",\" 0.350\",\" 0.356\"} 10 {14,34,54,74,94} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"   1,012\",\"     887\",\"     856\",\"     387\",\"     851\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.345\",\" 0.339\",\" 0.342\",\" 0.248\",\" 0.327\"} 11 {15,35,55,75,95} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"       0\",\"       0\",\"       0\",\"       0\",\"       0\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 12 {16,36,56,76,96} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"   1,094\",\"   1,033\",\"     878\",\"     880\",\"     842\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.278\",\" 0.332\",\" 0.285\",\" 0.311\",\" 0.299\"} 13 {17,37,57,77,97} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     995\",\"   1,117\",\"     987\",\"     623\",\"     651\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.313\",\" 0.324\",\" 0.320\",\" 0.318\",\" 0.299\"} 14 {18,38,58,78,98} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"       0\",\"       0\",\"       0\",\"       0\",\"       0\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 15 {19,39,59,79,99} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     771\",\"     582\",\"     845\",\"     570\",\"     625\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.246\",\" 0.227\",\" 0.243\",\" 0.244\",\" 0.232\"} 16 {20,40,60,80,100} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"       0\",\"       0\",\"       0\",\"       0\",\"       0\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 17 {21,41,61,81,101} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     783\",\"     587\",\"     813\",\"     586\",\"     570\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.250\",\" 0.235\",\" 0.253\",\" 0.259\",\" 0.253\"} 18 {22,42,62,82,102} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     551\",\"     649\",\"     588\",\"     552\",\"     444\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.310\",\" 0.336\",\" 0.355\",\" 0.330\",\" 0.372\"} 19 {23,43,63,83,103} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"   1,007\",\"     776\",\"     818\",\"     784\",\"     725\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.343\",\" 0.409\",\" 0.373\",\" 0.366\",\" 0.421\"} 20 {24,44,64,84,104} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     797\",\"     971\",\"     887\",\"     770\",\"     745\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.311\",\" 0.355\",\" 0.347\",\" 0.395\",\" 0.431\"} 21 {25,45,65,85,105} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\"   5,712\",\"   6,330\",\"   5,671\",\"   5,609\",\"   4,729\"} {\"     943\",\"     953\",\"     837\",\"     730\",\"     699\"} {\"   1,509\",\"   1,742\",\"   1,494\",\"   1,474\",\"   1,260\"} {\" 0.320\",\" 0.363\",\" 0.349\",\" 0.397\",\" 0.429\"} <p>Which model in production (model selection) is something that we will review later, with <code>Audition</code>, but for now, let's choose the model group <code>3</code> and see the <code>predictions</code> table:</p> <pre><code>select\n    model_id,\n    entity_id,\n    as_of_date::date,\n    round(score,2),\n    label_value as label\nfrom\n    test_results.predictions\nwhere\n    model_id = 11\norder by as_of_date asc, score desc\nlimit 20\n</code></pre> model_id entity_id as_of_date round label 11 26873 2015-06-01 0.49 11 26186 2015-06-01 0.49 11 25885 2015-06-01 0.49 11 24831 2015-06-01 0.49 11 24688 2015-06-01 0.49 11 21485 2015-06-01 0.49 11 20644 2015-06-01 0.49 11 20528 2015-06-01 0.49 11 19531 2015-06-01 0.49 11 18279 2015-06-01 0.49 11 17853 2015-06-01 0.49 11 17642 2015-06-01 0.49 11 16360 2015-06-01 0.49 11 15899 2015-06-01 0.49 11 15764 2015-06-01 0.49 11 15381 2015-06-01 0.49 11 15303 2015-06-01 0.49 11 14296 2015-06-01 0.49 11 14016 2015-06-01 0.49 11 27627 2015-06-01 0.49 <p>NOTE: Given that this is a shallow tree, there will be a lot of entities with the same score,you probably will get a different set of entities, since <code>postgresql</code> will sort them at random.</p> <p>It is important to know\u2026</p> <p><code>Triage</code> sorted the predictions at random using the <code>random_seed</code> from the experiment\u2019s config file. If you want the predictions being sorted in a different way add</p> <pre><code>prediction:\n    randk_tiebreaker: \"worst\" # or \"best\" or \"random\"\n</code></pre> <p>Note that at the top of the list (sorted by <code>as_of_date</code>, and then by <code>score</code>), the labels are <code>NULL</code>. This means that the facilities that you are classifying as high risk, actually weren't inspected in that as of date. So, you actually don't know if this is a correct prediction or not.</p> <p>This is a characteristic of all the resource optimization problems: You do not have all the information about the elements in your system<sup>9</sup>.</p> <p>So, how the precision/recall is calculated? The number that is show in the <code>evaluations</code> table is calculated using only the rows that have a non-null label. You could argue that this is fine, if you assume that the distribution of the label in the non-observed facilities is the same that the ones that were inspected that month<sup>10</sup>. We will come back to this problem in the Early Warning Systems.</p>"},{"location":"dirtyduck/inspections/#a-more-advanced-experiment","title":"A more advanced experiment","text":"<p>Ok, let's add a more complete experiment. First the usual generalities.</p> <pre><code>config_version: 'v8'\n\nmodel_comment: 'inspections: advanced'\n\nuser_metadata:\n  label_definition: 'failed'\n  experiment_type: 'inspections prioritization'\n  description: |\n    Using Ensamble methods\n  purpose: 'trying ensamble algorithms'\n  org: 'DSaPP'\n  team: 'Tutorial'\n  author: 'Your name here'\n  etl_date: '2019-02-21'\n</code></pre> <p>We won't change anything related to features, cohort and label definition neither to temporal configuration.</p> <p>As before, we can check the temporal structure of our crossvalidation:</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntriage experiment experiments/inspections_label_failed_01.yaml --show-timechop\n</code></pre> <p> Figure. Temporal blocks for inspections experiment. The label is a failed inspection in the next month.</p> <p>We want to use all the features groups (<code>feature_group_definition</code>). The training will be made on matrices with <code>all</code> the feature groups, then leaving one feature group out at a time, <code>leave-one-out</code> (i.e. one model with <code>inspections</code> and <code>results</code>, another with <code>inspections</code> and <code>risks</code>, and another with <code>results</code> and <code>risks), and finally leaving one feature group in at a time (i.e. a model with</code>inspections<code>only, another with</code>results<code>only, and a third with</code>risks` only).</p> <pre><code>feature_group_definition:\n   prefix:\n     - 'inspections'\n     - 'results'\n     - 'risks'\n     - 'inspection_types'\n\nfeature_group_strategies: ['all', 'leave-one-in', 'leave-one-out']\n</code></pre> <p>Finally, we will try some <code>RandomForestClassifier</code>:</p> <p><pre><code>grid_config:\n   'sklearn.ensemble.RandomForestClassifier':\n     n_estimators: [10000]\n     criterion: ['gini']\n     max_depth: [2, 5, 10]\n     max_features: ['sqrt']\n     min_samples_split: [2, 10, 50]\n     n_jobs: [-1]\n\n   'sklearn.ensemble.ExtraTreesClassifier':\n     n_estimators: [10000]\n     criterion: ['gini']\n     max_depth: [2, 5, 10]\n     max_features: ['sqrt']\n     min_samples_split: [2, 10, 50]\n     n_jobs: [-1]\n\nscoring:\n    testing_metric_groups:\n        -\n          metrics: [precision@, recall@]\n          thresholds:\n            percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n            top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]\n\n    training_metric_groups:\n      -\n        metrics: [accuracy]\n      -\n        metrics: [precision@, recall@]\n        thresholds:\n          percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n          top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]\n</code></pre> Before running, let's verify the configuration file</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntriage experiment experiments/inspections_label_failed_01.yaml  --validate-only\n</code></pre> <p>You can execute the experiment with</p> <pre><code># Remember to run this in bastion  NOT in your laptop shell!\ntime triage experiment experiments/inspections_label_failed_01.yaml\n</code></pre> <p>This will take a looooong time to run. The reason for that is easy to understand: We are computing a lot of models: 6 time splits, 18 model groups and 9 features sets (one for <code>all</code>, four for <code>leave_one_in</code> and four for <code>leave_one_out</code>), so 6 \\times 18 \\times 9 = 486 extra models.</p> <p>Well, now we have a lot of models. How can you pick the best one? You could try the following query:</p> <pre><code>with features_groups as (\nselect\n    model_group_id,\n    split_part(unnest(feature_list), '_', 1) as feature_groups\nfrom\n    triage_metadata.model_groups\n),\n\nfeatures_arrays as (\nselect\n    model_group_id,\n    array_agg(distinct feature_groups) as feature_groups\nfrom\n    features_groups\ngroup by\n    model_group_id\n)\n\nselect\n    model_group_id,\n    model_type,\n    hyperparameters,\n    feature_groups,\n     array_agg(to_char(stochastic_value, '0.999') order by train_end_time asc) filter (where metric = 'precision@') as \"precision@15%\",\n    array_agg(to_char(stochastic_value, '0.999') order by train_end_time asc) filter (where metric = 'recall@') as \"recall@15%\"\nfrom\n    triage_metadata.models\n    join\n    features_arrays using(model_group_id)\n    join\n    test_results.evaluations using(model_id)\nwhere\n    model_comment ~ 'inspection'\nand\n    parameter = '15_pct'\ngroup by\n    model_group_id,\n    model_type,\n    hyperparameters,\n    feature_groups\norder by\n    model_group_id;\n</code></pre> This is a long table \u2026 model_group_id model_type hyperparameters feature_groups precision@15% recall@15% 1 sklearn.dummy.DummyClassifier {\"strategy\": \"prior\"} {inspections} {\" 0.339\",\" 0.366\",\" 0.378\"} {\" 0.153\",\" 0.151\",\" 0.149\"} 2 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 2, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.347\",\" 0.394\",\" 0.466\"} {\" 0.155\",\" 0.153\",\" 0.180\"} 3 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 2, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.349\",\" 0.397\",\" 0.468\"} {\" 0.156\",\" 0.154\",\" 0.181\"} 4 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 10, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.409\",\" 0.407\",\" 0.470\"} {\" 0.179\",\" 0.163\",\" 0.178\"} 5 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 10, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.416\",\" 0.409\",\" 0.454\"} {\" 0.183\",\" 0.160\",\" 0.169\"} 6 sklearn.tree.DecisionTreeClassifier {\"max_depth\": null, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.368\",\" 0.394\",\" 0.413\"} {\" 0.165\",\" 0.161\",\" 0.160\"} 7 sklearn.tree.DecisionTreeClassifier {\"max_depth\": null, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.386\",\" 0.397\",\" 0.417\"} {\" 0.171\",\" 0.161\",\" 0.162\"} 8 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.441\",\" 0.471\",\" 0.513\"} {\" 0.190\",\" 0.187\",\" 0.193\"} 9 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.470\",\" 0.478\",\" 0.532\"} {\" 0.200\",\" 0.189\",\" 0.200\"} 10 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,results,risks} {\" 0.481\",\" 0.479\",\" 0.513\"} {\" 0.204\",\" 0.189\",\" 0.193\"} 11 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,results,risks} {\" 0.474\",\" 0.472\",\" 0.535\"} {\" 0.202\",\" 0.183\",\" 0.199\"} 12 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspections} {\" 0.428\",\" 0.417\",\" 0.389\"} {\" 0.179\",\" 0.149\",\" 0.148\"} 13 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspections} {\" 0.428\",\" 0.417\",\" 0.390\"} {\" 0.180\",\" 0.149\",\" 0.148\"} 14 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspections} {\" 0.427\",\" 0.417\",\" 0.376\"} {\" 0.179\",\" 0.149\",\" 0.140\"} 15 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspections} {\" 0.428\",\" 0.417\",\" 0.380\"} {\" 0.179\",\" 0.149\",\" 0.143\"} 16 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {results} {\" 0.415\",\" 0.398\",\" 0.407\"} {\" 0.180\",\" 0.157\",\" 0.157\"} 17 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {results} {\" 0.393\",\" 0.401\",\" 0.404\"} {\" 0.171\",\" 0.158\",\" 0.155\"} 18 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {results} {\" 0.436\",\" 0.425\",\" 0.447\"} {\" 0.191\",\" 0.169\",\" 0.171\"} 19 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {results} {\" 0.432\",\" 0.423\",\" 0.438\"} {\" 0.188\",\" 0.168\",\" 0.167\"} 20 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {risks} {\" 0.413\",\" 0.409\",\" 0.431\"} {\" 0.184\",\" 0.170\",\" 0.166\"} 21 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {risks} {\" 0.407\",\" 0.391\",\" 0.459\"} {\" 0.180\",\" 0.159\",\" 0.179\"} 22 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {risks} {\" 0.418\",\" 0.432\",\" 0.469\"} {\" 0.184\",\" 0.176\",\" 0.181\"} 23 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {risks} {\" 0.427\",\" 0.431\",\" 0.476\"} {\" 0.187\",\" 0.176\",\" 0.183\"} 24 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection} {\" 0.435\",\" 0.483\",\" 0.483\"} {\" 0.193\",\" 0.194\",\" 0.186\"} 25 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection} {\" 0.448\",\" 0.465\",\" 0.518\"} {\" 0.196\",\" 0.188\",\" 0.202\"} 26 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection} {\" 0.446\",\" 0.446\",\" 0.508\"} {\" 0.189\",\" 0.179\",\" 0.193\"} 27 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection} {\" 0.459\",\" 0.444\",\" 0.513\"} {\" 0.198\",\" 0.176\",\" 0.198\"} 28 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,results,risks} {\" 0.472\",\" 0.479\",\" 0.506\"} {\" 0.202\",\" 0.191\",\" 0.190\"} 29 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,results,risks} {\" 0.476\",\" 0.486\",\" 0.532\"} {\" 0.202\",\" 0.191\",\" 0.199\"} 30 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,results,risks} {\" 0.485\",\" 0.454\",\" 0.535\"} {\" 0.203\",\" 0.180\",\" 0.204\"} 31 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,results,risks} {\" 0.479\",\" 0.497\",\" 0.521\"} {\" 0.205\",\" 0.193\",\" 0.196\"} 32 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,risks} {\" 0.437\",\" 0.432\",\" 0.474\"} {\" 0.191\",\" 0.178\",\" 0.181\"} 33 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,risks} {\" 0.459\",\" 0.468\",\" 0.501\"} {\" 0.202\",\" 0.191\",\" 0.197\"} 34 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,risks} {\" 0.461\",\" 0.448\",\" 0.482\"} {\" 0.201\",\" 0.181\",\" 0.187\"} 35 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,risks} {\" 0.463\",\" 0.445\",\" 0.497\"} {\" 0.200\",\" 0.180\",\" 0.189\"} 36 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,results} {\" 0.462\",\" 0.448\",\" 0.513\"} {\" 0.199\",\" 0.177\",\" 0.191\"} 37 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,results} {\" 0.465\",\" 0.491\",\" 0.537\"} {\" 0.197\",\" 0.190\",\" 0.203\"} 38 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,results} {\" 0.459\",\" 0.481\",\" 0.522\"} {\" 0.193\",\" 0.187\",\" 0.198\"} 39 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,results} {\" 0.474\",\" 0.479\",\" 0.536\"} {\" 0.203\",\" 0.188\",\" 0.201\"} 40 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspections,results,risks} {\" 0.436\",\" 0.429\",\" 0.490\"} {\" 0.189\",\" 0.174\",\" 0.185\"} 41 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspections,results,risks} {\" 0.441\",\" 0.448\",\" 0.515\"} {\" 0.190\",\" 0.180\",\" 0.194\"} 42 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspections,results,risks} {\" 0.460\",\" 0.475\",\" 0.481\"} {\" 0.198\",\" 0.189\",\" 0.178\"} 43 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspections,results,risks} {\" 0.465\",\" 0.446\",\" 0.496\"} {\" 0.199\",\" 0.179\",\" 0.187\"} <p>This table summarizes all our experiments, but it is very difficult to use if you want to choose the best combination of hyperparameters and algorithm (i.e. the model group). In the next section will solve this dilemma with the support of <code>audition</code>.</p>"},{"location":"dirtyduck/inspections/#selecting-the-best-model","title":"Selecting the best model","text":"<p>43 model groups! How to pick the best one and use it for making predictions with new data? What do you mean by \u201cthe best\u201d? This is not as easy as it sounds, due to several factors:</p> <ul> <li>You can try to pick the best using a metric specified in the     config file (<code>precision@</code> and <code>recall@</code>), but at what point of     time? Maybe different model groups are best at different     prediction times.</li> <li>You can just use the one that performs best on the last test set.</li> <li>You can value a model group that provides consistent results over     time. It might not be the best on any test set, but you can feel     more confident that it will continue to perform similarly.</li> <li>If there are several model groups that perform similarly and their     lists are more or less similar, maybe it doesn't really matter     which you pick.</li> </ul> <p>Remember\u2026</p> <p>Before move on, remember the two main caveats for the value of the metric in this kind of ML problems:</p> <ul> <li>Could be many entities with the same predicted risk score (ties)</li> <li>Could be a lot of entities without a label (Weren't inspected, so we don\u2019t know)</li> </ul> <p>We included a simple configuration file in <code>/triage/audition/inspection_audition_config.yaml</code> with some rules:</p> <pre><code># CHOOSE MODEL GROUPS\nmodel_groups:\n    query: |\n        select distinct(model_group_id)\n        from triage_metadata.model_groups\n        where model_config -&gt;&gt; 'experiment_type' ~ 'inspection'\n# CHOOSE TIMESTAMPS/TRAIN END TIMES\ntime_stamps:\n    query: |\n        select distinct train_end_time\n        from triage_metadata.models\n        where model_group_id in ({})\n        and extract(day from train_end_time) in (1)\n        and train_end_time &gt;= '2014-01-01'\n# FILTER\nfilter:\n    metric: 'precision@' # metric of interest\n    parameter: '10_pct' # parameter of interest\n    max_from_best: 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time.\n    threshold_value: 0.0 # The worst absolute value that the given metric should be.\n    distance_table: 'inspections_distance_table' # name of the distance table\n    models_table: 'models' # name of the models table\n\n# RULES\nrules:\n    -\n        shared_parameters:\n            -\n                metric: 'precision@'\n                parameter: '10_pct'\n\n        selection_rules:\n            -\n                name: 'best_current_value' # Pick the model group with the best current metric value\n                n: 3\n            -\n                name: 'best_average_value' # Pick the model with the highest average metric value\n                n: 3\n            -\n                name: 'lowest_metric_variance' # Pick the model with the lowest metric variance\n                n: 3\n            -\n                name: 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case`\n                dist_from_best_case: [0.05]\n                n: 3\n</code></pre> <p><code>Audition</code> will have each rule give you the best n model groups based on the metric and parameter following that rule for the most recent time period (in all the rules shown n = 3).</p> <p>We can run the simulation of the rules against the experiment as:</p> <pre><code># Run this in bastion\u2026\ntriage --tb audition -c inspection_audition_config.yaml --directory audition/inspections\n</code></pre> <p><code>Audition</code> will create several plots that will help you to sort out which is the best model group to use (like in a production setting or just to generate your predictions list).</p>"},{"location":"dirtyduck/inspections/#filtering-model-groups","title":"Filtering model groups","text":"<p>Most of the time, <code>audition</code> should be used in a iterative fashion, the result of each iteration will be a reduced set of models groups and a best rule for selecting model groups.</p> <p>If you look again at the audition configuration file above you can filter the number of models to consider using the parameters <code>max_from_best</code> and <code>threshold_value</code>. The former will filter out models groups with models which performance in the <code>metric</code> is farther than the <code>max_from_best</code> (In this case we are allowing all the models, since <code>max_from_best = 1.0</code>, if you want less models you could choose <code>0.1</code> for example, and you will remove the <code>DummyClassifier</code> and some <code>DecisionTreeClassifiers</code>). <code>threshold_value</code> filter out all the models groups with models performing below that the specified value. This could be important if you don\u2019t find acceptable models with metrics that are that low.</p> <p><code>Audition</code> will generate two plots that are meant to be used together: model performance over time and distance from best.</p> <p> Figure. Model group performance over time. In this case the metric show is <code>precision@10%</code>. We didn\u2019t filter out any model group, so the 45 model groups are shown. See discussion above to learn how to plot less model groups. The black dashed line represents the (theoretical) system's performance if we select the best performant model in a every evaluation date. The colored lines represents different model groups. All the model groups that share an algorithm will be colored the same.</p> <p>Next figure shows the proportion of models that are behind the best model. The distance is measured in percentual points. You could use this plot to filter out more model groups, changing the value of <code>max_from_best</code> in the configuration file. This plot is hard to read, but is very helpful since it shows you the consistency of the model group: How consistently are the model group in a specific range, let's say 20 points, from the best?</p> <p> Figure. Proportion of *models in a model group that are separated from the best model. The distance is measured in percentual points, i.e. How much less precision at 10 percent of the population compared to the best model in that date.*</p> <p>In the figure, you can see that the ~60% of the <code>DummyClassifier</code> models are ~18 percentual points below of the best.</p>"},{"location":"dirtyduck/inspections/#selecting-the-best-rule-or-strategy-for-choosing-model-groups","title":"Selecting the best rule or strategy for choosing model groups","text":"<p>In this phase of the audition, you will see what will happen in the next time if you choose your model group with an specific strategy or rule. We call this the regret of the strategies.</p> <p>We define regret as</p> <p>Regret Is the difference in performance between the model group you picked and the best one in the next time period.</p> <p>The next plot show the best model group selected by the strategies specified in the configuration file:</p> <p> Figure. Given a strategy for selecting model groups (in the figure, 4 are shown), What will be the performace of the model group chosen by that strategy in the next evaluation date?</p> <p>It seems that the strategies best average and best current value select the same model group.</p> <p> Figure. Given a strategy for selecting model groups (in the plot 4 are shown). What will be the distance (*regret) to the best theoretical model in the following evaluation date?*</p> <p>Obviously, you don\u2019t know the future, but with the available data, if you stick to an a particular strategy, How much you will regret about that decision?</p> <p> Figure. Expected regret for the strategies. The less the better.</p> <p>The best 3 model groups per strategy will be stored in the file <code>[[file:audition/inspections/results_model_group_ids.json][results_model_group_ids.json]]</code>:</p> <pre><code>{\n    \"best_current_value_precision@_10_pct\": [39, 30, 9],\n    \"best_average_value_precision@_10_pct\": [39, 9, 29],\n    \"lowest_metric_variance_precision@_10_pct\": [1, 5, 19],\n    \"most_frequent_best_dist_precision@_10_pct_0.05\": [8, 9, 10]\n}\n</code></pre> <p>The analysis suggests that the best strategies are</p> <ul> <li>select the model groups (<code>39,9,29</code>) which have the best average precision@10% value or,</li> <li>select the best model group (<code>39,30,9</code>) using precision@10% today and use it for the next time period.</li> </ul> <p>You will note that both strategies share two models groups and differ in one. In the next two sections, we will investigate further those four model groups selected by <code>audition</code>, using the Postmodeling tool set.</p>"},{"location":"dirtyduck/inspections/#postmodeling-inspecting-the-best-models-closely","title":"Postmodeling: Inspecting the best models closely","text":"<p>Postmodeling will help you to understand the behaviour orf your selected models (from audition)</p> <p>As in <code>Audition</code>, we will split the postmodeling process in two parts. The first one is about exploring the model groups filtered by audition, with the objective of select one. The second part is about learning about models in the model group that was selected.</p> <p>We will setup some parameters in the postmodeling configuration file located at <code>/triage/postmodeling/inspection_postmodeling_config.yaml</code>, mainly where is the audition\u2019s output file located.</p> <pre><code># Postmodeling Configuration File\n\nproject_path: '/triage' # Project path defined in triage with matrices and models\nmodel_group_id:\n  - 39\n  - 9\n  - 29\n  - 30\n\nthresholds: # Thresholds for defining positive predictions\n  rank_abs: [50, 100, 250]\n  rank_pct: [5, 10, 25]\n\nbaseline_query: | # SQL query for defining a baseline for comparison in plots. It needs a metric and parameter\n      select g.model_group_id,\n             m.model_id,\n             extract('year' from m.evaluation_end_time) as as_of_date_year,\n             m.metric,\n             m.parameter,\n             m.value,\n             m.num_labeled_examples,\n             m.num_labeled_above_threshold,\n             m.num_positive_labels\n       from test_results.evaluations m\n       left join triage_metadata.models g\n       using(model_id)\n       where g.model_group_id = 1\n             and metric = 'precision@'\n             and parameter = '10_pct'\n\nmax_depth_error_tree: 5 # For error trees, how depth the decision trees should go?\nn_features_plots: 10 # Number of features for importances\nfigsize: [12, 12] # Default size for plots\nfontsize: 20 # Default fontsize for plots\n</code></pre> <p>Compared to the previous sections, postmodeling is not an automated process (yet). Hence, to do the following part of the tutorial, you need to run <code>jupyter</code> inside <code>bastion</code> as follows:</p> <pre><code>jupyter-notebook \u2013-ip=0.0.0.0  --port=56406 --allow-root\n</code></pre> <p>And then in your browser type<sup>11</sup>: http://0.0.0.0:56406</p> <p>Now that you are in a jupyter notebook, type the following:</p> <pre><code>%matplotlib inline\nimport matplotlib\n#matplotlib.use('Agg')\n\nimport triage\nimport pandas as pd\nimport numpy as np\n\nfrom collections import OrderedDict\nfrom triage.component.postmodeling.contrast.utils.aux_funcs import create_pgconn, get_models_ids\nfrom triage.component.catwalk.storage import ProjectStorage, ModelStorageEngine, MatrixStorageEngine\nfrom triage.component.postmodeling.contrast.parameters import PostmodelParameters\nfrom triage.component.postmodeling.contrast.model_evaluator import ModelEvaluator\nfrom triage.component.postmodeling.contrast. model_group_evaluator import ModelGroupEvaluator\n</code></pre> <p>After importing, we need to create an <code>sqlalchemy engine</code> for connecting to the database, and read the configuration file.</p> <pre><code>params = PostmodelParameters('inspection_postmodeling_config.yaml')\nengine = create_pgconn('database.yaml')\n</code></pre> <p>Postmodeling provides the object <code>ModelGroupEvaluator</code> to compare different model groups.</p> <pre><code>audited_models_class = ModelGroupEvaluator(tuple(params.model_group_id), engine)\n</code></pre>"},{"location":"dirtyduck/inspections/#comparing-the-audited-model-groups","title":"Comparing the audited model groups","text":"<p>First we will compare the performance of the audited model groups and the baseline over time. First, we will plot <code>precision@10_pct</code></p> <pre><code>audited_models_class.plot_prec_across_time(param_type='rank_pct',\n                                           param=10,\n                                           baseline=True,\n                                           baseline_query=params.baseline_query,\n                                           metric='precision@',\n                                               figsize=params.figsize)\n</code></pre> <p> Figure. Precision@10% over time from the best performing model groups selected by Audition</p> <p>and now the <code>recall@10_pct</code></p> <pre><code>audited_models_class.plot_prec_across_time(param_type='rank_pct',\n                                           param=10,\n                                           metric='recall@',\n                                           figsize=params.figsize)\n</code></pre> <p> Figure. Recall@10% over time from the best performing model groups selected by Audition</p> <p>All the selected model groups have a very similar performance. Let\u2019s see if they are predicting similar lists of facilities that are at risk of fail an inspection.</p> <pre><code>audited_models_class.plot_jaccard_preds(param_type='rank_pct',\n                                        param=10,\n                                        temporal_comparison=True)\n</code></pre> <p> Figure. How similar are the model groups\u2019 generated list? We use Jaccard similarity on the predicted lists (length of list 10%) to asses the overlap between lists.</p> <p>The plot will shows the overlap of the predicted list containing the 10% of the facilities between model groups at each as of date. The lists are at least 50% similar.</p> <p>Tip</p> <p>Why the models are not learning the same? You should investigate why is that so. This could lead you to defining new features or some another conclusion about your data, but for this tutorial we will move on.</p>"},{"location":"dirtyduck/inspections/#going-deeper-with-a-model","title":"Going deeper with a model","text":"<p>Imagine that after a deeper analysis, you decide to choose model group 39</p> <pre><code>select\n    mg.model_group_id,\n    mg.model_type,\n    mg.hyperparameters,\n    array_agg(model_id order by train_end_time) as models\nfrom\n    triage_metadata.model_groups as mg\n    inner join\n    triage_metadata.models\n    using (model_group_id)\nwhere model_group_id = 39\ngroup by 1,2,3\n</code></pre> model_group_id model_type hyperparameters models 39 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {53,89,125} <p>We will investigate what the particular models are doing. Postmodeling created a <code>ModelEvaluator</code> (similar to the <code>ModelGroupEvaluator</code>) to do this exploration:</p> <pre><code>models_39 = { f'{model}': ModelEvaluator(39, model, engine) for model in [53,89,125] }\n</code></pre> <p>In this tutorial, we will just show some parts of the analysis in the most recent model, but feel free of exploring the behavior of all the models in this model group, and check if you can detect any pattern.</p> <ul> <li> <p>Feature importances</p> <pre><code>models_39['125'].plot_feature_importances(path=params.project_path,\n                                          n_features_plots=params.n_features_plots,\n                                          figsize=params.figsize)\n</code></pre> <p> Figure. Top 10 feature importances for de model group 11 at 2016-06-01 (i.e. model 125).</p> <pre><code>models_39['125'].plot_feature_group_aggregate_importances()\n</code></pre> <p> Figure. Feature group \u201cimportance\u201d (we are basically taking the max of all the feature importances in a feature group) for the model group 39, model 125.</p> </li> <li> <p>Our Policy menu</p> <p>The following plot depicts the behavior of the metrics if you change the length of the facilities predicted at risk (i.e. the k). This plot is important from the decision making point of view, since it could be used as a policy menu.</p> <pre><code>models_39['125'].plot_precision_recall_n()\n</code></pre> <p> Figure. Plot of Precision and Recall over the proportion of the facilities. This plot is used as a \"policy menu\" since allows you to see how much you will gain if you invest more resources or how much you will sacrifice if you reduce your budget for resources. This is also known as \u201cRayid plot\u201d at DSaPP.</p> <p>We selected this model group because it was the best at precision at 10% (i.e. the model group consistently chose facilities will fail inspections at the top 10% of the risk). With the plot above you could decide to double your resources (maybe hiring more inspectors so you could inspect 20% of the facilities) and with this model you will double the detection of facilities that will fail inspections (from ~18% to ~30% in recall) with only a few percent points less of precision ~45% to ~40% (this means that 6 in 10 facilities that the inspectors visit will pass the inspection). You could also go the other way around: if you reduce the length of the list from 10% to 5%, well you will gain a little of precision, but your recall will be ~5%.</p> </li> </ul>"},{"location":"dirtyduck/inspections/#where-to-go-from-here","title":"Where to go from here","text":"<p>Ready to get started with your own data? Check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project.</p> <p>Want to work through another example? Take a look at our early warning system case study</p> <ol> <li> <p>If you assume a uniform distribution, it will make sense to select facilities at random.\u00a0\u21a9</p> </li> <li> <p>The underlying assumption here is that the City of Chicago is currently doing random selection for the inspections. This is not true (and probably unfair). In a real project, you will setup a real baseline and you will compare your models against it. This baseline could be a rule or a model.\u00a0\u21a9</p> </li> <li> <p>You need to check this! Fortunately, <code>triage</code> allows you to try several options here, so, if you think that this is too high or too low you can change that and fit your needs.\u00a0\u21a9</p> </li> <li> <p>Think about it: we can\u2019t learn the relationship between the features and the label if we don't know the label.\u00a0\u21a9</p> </li> <li> <p>Confused? Check A deeper look at triage for more details.\u00a0\u21a9</p> </li> <li> <p>The formulas are, for <code>precision@k</code>, is the proportion of facilities correctly identified in the top-k facilities ranked by risk:</p> <p>$$   precision@k = \\frac{TP \\in k}{k}   $$</p> <p>This is a measure about how efficiently are your system using your resources.</p> <p><code>recall@k</code>, in the other hand is the proportion of all the facilities that are risk found in the top-k</p> <p>$$   recall@k = \\frac{TP \\in k}{TP}   $$</p> <p>recall is a measure about the coverage of your system, i.e. how good is identifying in the top-k the facilities at risk.</p> <p>One possible variation of this is to only include in the   denominator the labeled rows in k. This is the approach used by <code>triage</code>.\u00a0\u21a9</p> </li> <li> <p>We will explore how to one way to tackle this in the advance part of this tutorial.\u00a0\u21a9</p> </li> <li> <p>The flags <code>-no-save-predictions</code> and <code>profile</code> are not necessary but useful. The first one configure triage to not store the predictions (at this stage you don't need them, and you can always could recreate them from the model and the matrix). This will save you execution time. The flag <code>profile</code> stores the execution profile times in a file, so you can check which models or matrices are taking a lot of time on been built.\u00a0\u21a9</p> </li> <li> <p>From a more mathematical point of view: Your data actually reflects the empirical probability: P(violation|inspected), i.e. the probability of find a violation given that the facility is inspected. But the probability that you want is P(violation) (yes, I know that there are no such things as unconditional probabilities, please bare with me),i.e. the probability that the facility is in violation.\u00a0\u21a9</p> </li> <li> <p>You should see that this assumption is very dangerous in other settings, for example, crime prediction.\u00a0\u21a9</p> </li> <li> <p>This assumes that you are in a GNU/Linux machine, if not (you should reconsider what are you doing with your life) you should change the ip address (<code>0.0.0.0</code>) and use the one from the docker virtual machine.\u00a0\u21a9</p> </li> <li> <p>For some reason, <code>sklearn</code> doesn\u2019t scale the inputs to the Logistic Regression, so we (DSaPP) developed a version that does that.\u00a0\u21a9</p> </li> </ol>"},{"location":"dirtyduck/ml_governance/","title":"Machine learning governance","text":"<p>When <code>triage</code> executes the experiment, it creates a series of new schemas for storing the copious output of the experiment. The schemas are <code>test_results, train_results</code>, and <code>triage_metadata</code>. These schemas store the metadata of the trained models, features, parameters, and hyperparameters used in their training. It also stores the predictions and evaluations of the models on the test sets.</p> <p>The schema <code>triage_metadata</code> is composed by the tables:</p> <pre><code>\\dt triage_metadata.*\n</code></pre> List of relations Schema Name Type Owner model<sub>metadata</sub> experiment<sub>matrices</sub> table food<sub>user</sub> model<sub>metadata</sub> experiment<sub>models</sub> table food<sub>user</sub> model<sub>metadata</sub> experiments table food<sub>user</sub> model<sub>metadata</sub> list<sub>predictions</sub> table food<sub>user</sub> model<sub>metadata</sub> matrices table food<sub>user</sub> model<sub>metadata</sub> model<sub>groups</sub> table food<sub>user</sub> model<sub>metadata</sub> models table food<sub>user</sub> <p>The tables contained in <code>test_results</code> are:</p> <pre><code>\\dt test_results.*\n</code></pre> List of relations Schema Name Type Owner test<sub>results</sub> aequitas table food<sub>user</sub> test<sub>results</sub> evaluations table food<sub>user</sub> test<sub>results</sub> individual<sub>importances</sub> table food<sub>user</sub> test<sub>results</sub> predictions table food<sub>user</sub> <p>Lastly, if you have interest in how the model performed in the training data sets you could consult <code>train_results</code></p> <pre><code>\\dt train_results.*\n</code></pre> List of relations Schema Name Type Owner train<sub>results</sub> aequitas table food<sub>user</sub> train<sub>results</sub> evaluations table food<sub>user</sub> train<sub>results</sub> feature<sub>importances</sub> table food<sub>user</sub> train<sub>results</sub> predictions table food<sub>user</sub>"},{"location":"dirtyduck/ml_governance/#what-are-all-the-results-tables-about","title":"What are all the results tables about?","text":"<p><code>model_groups</code> stores the algorithm (<code>model_type</code>), the hyperparameters (<code>hyperparameters</code>), and the features shared by a particular set of models. <code>models</code> contains data specific to a model: the <code>model_group</code> (you can use <code>model_group_id</code> for linking the model to a model group), temporal information (like <code>train_end_time</code>), and the train matrix UUID (<code>train_matrix_uuid</code>). This UUID is important because it's the name of the file in which the matrix is stored.</p> <p>Lastly, <code>{train, test}_results.predictions</code> contains all the scores generated by every model for every entity. <code>{train_test}_results.evaluation</code> stores the value of all the metrics for every model, which were specified in the <code>scoring</code> section in the config file.</p>"},{"location":"dirtyduck/ml_governance/#triage_metadataexperiments","title":"<code>triage_metadata.experiments</code>","text":"<p>This table has the two columns: <code>experiment_hash</code> and <code>config</code></p> <pre><code>\\d triage_metadata.experiments\n</code></pre> Table \"model<sub>metadata.experiments</sub>\" Column Type Collation Nullable Default experiment<sub>hash</sub> character varying not null config jsonb Indexes: \"experiments<sub>pkey</sub>\" PRIMARY KEY, btree (experiment<sub>hash</sub>) Referenced by: TABLE \"model<sub>metadata.experiment</sub><sub>matrices</sub>\" CONSTRAINT \"experiment<sub>matrices</sub><sub>experiment</sub><sub>hash</sub><sub>fkey</sub>\" FOREIGN KEY (experiment<sub>hash</sub>) REFERENCES model<sub>metadata.experiments</sub>(experiment<sub>hash</sub>) TABLE \"model<sub>metadata.experiment</sub><sub>models</sub>\" CONSTRAINT \"experiment<sub>models</sub><sub>experiment</sub><sub>hash</sub><sub>fkey</sub>\" FOREIGN KEY (experiment<sub>hash</sub>) REFERENCES model<sub>metadata.experiments</sub>(experiment<sub>hash</sub>) TABLE \"model<sub>metadata.matrices</sub>\" CONSTRAINT \"matrices<sub>built</sub><sub>by</sub><sub>experiment</sub><sub>fkey</sub>\" FOREIGN KEY (built<sub>by</sub><sub>experiment</sub>) REFERENCES model<sub>metadata.experiments</sub>(experiment<sub>hash</sub>) TABLE \"model<sub>metadata.models</sub>\" CONSTRAINT \"models<sub>experiment</sub><sub>hash</sub><sub>fkey</sub>\" FOREIGN KEY (built<sub>by</sub><sub>experiment</sub>) REFERENCES model<sub>metadata.experiments</sub>(experiment<sub>hash</sub>) <p><code>experiment_hash</code> contains the hash of the configuration file that we used for our <code>triage</code> run.<sup>1</sup> <code>config</code> that contains the configuration experiment file that we used for our <code>triage</code> run, stored as <code>jsonb</code>.</p> <pre><code>select experiment_hash,\nconfig -&gt;  'user_metadata' as user_metadata\nfrom triage_metadata.experiments;\n</code></pre> experiment<sub>hash</sub> user<sub>metadata</sub> 67a1d564d31811b9c20ca63672c25abd {\"org\": \"DSaPP\", \"team\": \"Tutorial\", \"author\": \"Adolfo De Unanue\", \"etl<sub>date</sub>\": \"2019-02-21\", \"experiment<sub>type</sub>\": \"test\", \"label<sub>definition</sub>\": \"failed<sub>inspection</sub>\"} <p>We could use the following advice: If we are interested in all models that resulted from a certain config, we could lookup that config in <code>triage_metadata.experiments</code> and then use its <code>experiment_hash</code> on other tables to find all the models that resulted from that configuration.</p>"},{"location":"dirtyduck/ml_governance/#metadata_modelmodel_groups","title":"<code>metadata_model.model_groups</code>","text":"<p>Do you remember how we defined in <code>grid_config</code> the different classifiers that we want <code>triage</code> to train? For example, we could use in a configuration file the following:</p> <pre><code>    'sklearn.tree.DecisionTreeClassifier':\n        criterion: ['entropy']\n        max_depth: [1, 2, 5, 10]\n        random_state: [2193]\n</code></pre> <p>By doing so, we are saying that we want to train 4 decision trees (<code>max_depth</code> is one of <code>1, 2, 5, 10</code>). However, remember that we are using temporal crossvalidation to build our models, so we are going to have different temporal slices that we are training models on, e.g., 2010-2011, 2011-2012, etc.</p> <p>Therefore, we are going to train our four decision trees on each temporal slice. Therefore, the trained model (or the instance of that model) will change across temporal splits, but the configuration will remain the same. This table lets us keep track of the different configurations (<code>model_groups</code>) and gives us an <code>id</code> for each configuration (<code>model_group_id</code>). We can leverage the <code>model_group_id</code> to find all the models that were trained using the same config but on different slices of time.</p> <p>In our simple test configuration file we have:</p> <pre><code>    'sklearn.dummy.DummyClassifier':\n        strategy: [most_frequent]\n</code></pre> <p>Therefore, if we run the following</p> <pre><code>select\n    model_group_id,\n    model_type,\n    hyperparameters,\n    model_config -&gt; 'feature_groups' as feature_groups,\n    model_config -&gt; 'cohort_name' as cohort,\n    model_config -&gt; 'label_name' as label,\n    model_config -&gt; 'label_definition' as label_definition,\n    model_config -&gt; 'experiment_type' as experiment_type,\n    model_config -&gt; 'etl_date' as etl_date\nfrom\n    triage_metadata.model_groups;\n</code></pre> model<sub>group</sub><sub>id</sub> model<sub>type</sub> hyperparameters feature<sub>groups</sub> cohort label label<sub>definition</sub> experiment<sub>type</sub> etl<sub>date</sub> 1 sklearn.dummy.DummyClassifier {\"strategy\": \"most<sub>frequent</sub>\"} [\"prefix: results\", \"prefix: risks\", \"prefix: inspections\"] \"test<sub>facilities</sub>\" \"failed<sub>inspections</sub>\" \"failed<sub>inspection</sub>\" \"test\" \"2019-02-21\" <p>You can see that a model group is defined by the classifier (<code>model_type</code>), its hyperparameters (<code>hyperparameters</code>), the features (<code>feature_list</code>) (not shown), and the <code>model_config</code>.</p> <p>The field <code>model_config</code> is created using information from the block <code>model_group_keys</code>. In our test configuration file the block is:</p> <pre><code>model_group_keys:\n  - 'class_path'\n  - 'parameters'\n  - 'feature_names'\n  - 'feature_groups'\n  - 'cohort_name'\n  - 'state'\n  - 'label_name'\n  - 'label_timespan'\n  - 'training_as_of_date_frequency'\n  - 'max_training_history'\n  - 'label_definition'\n  - 'experiment_type'\n  - 'org'\n  - 'team'\n  - 'author'\n  - 'etl_date'\n</code></pre> <p>What can we learn from that? For example, if we add a new feature and rerun <code>triage</code>, <code>triage</code> will create a new <code>model_group</code> even if the classifier and the <code>hyperparameters</code> are the same as before.</p>"},{"location":"dirtyduck/ml_governance/#triage_metadatamodels","title":"<code>triage_metadata.models</code>","text":"<p>This table stores the information about our actual models, i.e., instances of our classifiers trained on specific temporal slices.</p> <pre><code>\\d triage_metadata.models\n</code></pre> Table \"model<sub>metadata.models</sub>\" Column Type Collation Nullable Default model<sub>id</sub> integer not null nextval('model<sub>metadata.models</sub><sub>model</sub><sub>id</sub><sub>seq</sub>'::regclass) model<sub>group</sub><sub>id</sub> integer model<sub>hash</sub> character varying run<sub>time</sub> timestamp without time zone batch<sub>run</sub><sub>time</sub> timestamp without time zone model<sub>type</sub> character varying hyperparameters jsonb model<sub>comment</sub> text batch<sub>comment</sub> text config json built<sub>by</sub><sub>experiment</sub> character varying train<sub>end</sub><sub>time</sub> timestamp without time zone test boolean train<sub>matrix</sub><sub>uuid</sub> text training<sub>label</sub><sub>timespan</sub> interval model<sub>size</sub> real Indexes: \"models<sub>pkey</sub>\" PRIMARY KEY, btree (model<sub>id</sub>) \"ix<sub>results</sub><sub>models</sub><sub>model</sub><sub>hash</sub>\" UNIQUE, btree (model<sub>hash</sub>) Foreign-key constraints: \"matrix<sub>uuid</sub><sub>for</sub><sub>models</sub>\" FOREIGN KEY (train<sub>matrix</sub><sub>uuid</sub>) REFERENCES model<sub>metadata.matrices</sub>(matrix<sub>uuid</sub>) \"models<sub>experiment</sub><sub>hash</sub><sub>fkey</sub>\" FOREIGN KEY (built<sub>by</sub><sub>experiment</sub>) REFERENCES model<sub>metadata.experiments</sub>(experiment<sub>hash</sub>) \"models<sub>model</sub><sub>group</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>group</sub><sub>id</sub>) REFERENCES model<sub>metadata.model</sub><sub>groups</sub>(model<sub>group</sub><sub>id</sub>) Referenced by: TABLE \"test<sub>results.evaluations</sub>\" CONSTRAINT \"evaluations<sub>model</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>id</sub>) REFERENCES model<sub>metadata.models</sub>(model<sub>id</sub>) TABLE \"train<sub>results.feature</sub><sub>importances</sub>\" CONSTRAINT \"feature<sub>importances</sub><sub>model</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>id</sub>) REFERENCES model<sub>metadata.models</sub>(model<sub>id</sub>) TABLE \"test<sub>results.individual</sub><sub>importances</sub>\" CONSTRAINT \"individual<sub>importances</sub><sub>model</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>id</sub>) REFERENCES model<sub>metadata.models</sub>(model<sub>id</sub>) TABLE \"model<sub>metadata.list</sub><sub>predictions</sub>\" CONSTRAINT \"list<sub>predictions</sub><sub>model</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>id</sub>) REFERENCES model<sub>metadata.models</sub>(model<sub>id</sub>) TABLE \"test<sub>results.predictions</sub>\" CONSTRAINT \"predictions<sub>model</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>id</sub>) REFERENCES model<sub>metadata.models</sub>(model<sub>id</sub>) TABLE \"train<sub>results.evaluations</sub>\" CONSTRAINT \"train<sub>evaluations</sub><sub>model</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>id</sub>) REFERENCES model<sub>metadata.models</sub>(model<sub>id</sub>) TABLE \"train<sub>results.predictions</sub>\" CONSTRAINT \"train<sub>predictions</sub><sub>model</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>id</sub>) REFERENCES model<sub>metadata.models</sub>(model<sub>id</sub>) <p>Noteworthy columns are:</p> <ul> <li><code>model_id</code>: The id of the model (i.e., instance\u2026). We will use this ID to trace a model evaluation to a <code>model_group</code> and vice versa.</li> <li><code>model_group_id</code>: The id of the models model group we encountered above.</li> <li><code>model_hash</code>: The hash of our model. We can use the hash to load the actual model. It gets stored under <code>TRIAGE_OUTPUT_PATH/trained_models/{model_hash}</code>. We are going to this later to look at a trained decision tree.</li> <li><code>run_time</code>: Time when the model was trained.</li> <li><code>model_type</code>: The algorithm used for training.</li> <li><code>model_comment</code>: Literally the text in the <code>model_comment</code> block in the configuration file</li> <li><code>hyperparameters</code>: Hyperparameters used for the model configuration.</li> <li><code>built_by_experiment</code>: The hash of our experiment. We encountered this value in the <code>results.experiments</code> table before.</li> <li><code>train_end_time</code>: When building the training matrix, we included training samples up to this date.</li> <li><code>train_matrix_uuid</code>: The hash of the matrix that we used to train this model. The matrix gets stored as <code>csv</code> under <code>TRIAGE_OUTPUT_PATH/matrices/{train_matrix_uuid}.csv</code>. This is helpful when trying to inspect the matrix and features that were used for training.</li> <li><code>train_label_timespan</code>: How big was our window to get the labels for our training matrix? For example, a <code>train_label_window</code> of 1 year would mean that we look one year from a given date in the training matrix into the future to find the label for that training sample.</li> </ul>"},{"location":"dirtyduck/ml_governance/#triage_metadatamatrices","title":"<code>triage_metadata.matrices</code>","text":"<p>This schema contains information about the matrices used in the model's training. You could use this information to debug your models. Important columns are <code>matrix_uuid</code> (The matrix gets stored as <code>TRIAGE_OUTPUT_PATH/matrices/{train_matrix_uuid}.csv</code>), <code>matrix_type</code> (indicated if the matrix was used for training models or testing them), <code>lookback_duration</code> and <code>feature_starttime</code> (give information about the temporal setting of the features) and <code>num_observations</code> (size of the matrices).</p> <pre><code>\\d triage_metadata.matrices\n</code></pre> Table \"model<sub>metadata.matrices</sub>\" Column Type Collation Nullable Default matrix<sub>id</sub> character varying matrix<sub>uuid</sub> character varying not null matrix<sub>type</sub> character varying labeling<sub>window</sub> interval num<sub>observations</sub> integer creation<sub>time</sub> timestamp with time zone now() lookback<sub>duration</sub> interval feature<sub>start</sub><sub>time</sub> timestamp without time zone matrix<sub>metadata</sub> jsonb built<sub>by</sub><sub>experiment</sub> character varying Indexes: \"matrices<sub>pkey</sub>\" PRIMARY KEY, btree (matrix<sub>uuid</sub>) \"ix<sub>model</sub><sub>metadata</sub><sub>matrices</sub><sub>matrix</sub><sub>uuid</sub>\" UNIQUE, btree (matrix<sub>uuid</sub>) Foreign-key constraints: \"matrices<sub>built</sub><sub>by</sub><sub>experiment</sub><sub>fkey</sub>\" FOREIGN KEY (built<sub>by</sub><sub>experiment</sub>) REFERENCES model<sub>metadata.experiments</sub>(experiment<sub>hash</sub>) Referenced by: TABLE \"test<sub>results.evaluations</sub>\" CONSTRAINT \"evaluations<sub>matrix</sub><sub>uuid</sub><sub>fkey</sub>\" FOREIGN KEY (matrix<sub>uuid</sub>) REFERENCES model<sub>metadata.matrices</sub>(matrix<sub>uuid</sub>) TABLE \"train<sub>results.evaluations</sub>\" CONSTRAINT \"evaluations<sub>matrix</sub><sub>uuid</sub><sub>fkey</sub>\" FOREIGN KEY (matrix<sub>uuid</sub>) REFERENCES model<sub>metadata.matrices</sub>(matrix<sub>uuid</sub>) TABLE \"model<sub>metadata.models</sub>\" CONSTRAINT \"matrix<sub>uuid</sub><sub>for</sub><sub>models</sub>\" FOREIGN KEY (train<sub>matrix</sub><sub>uuid</sub>) REFERENCES model<sub>metadata.matrices</sub>(matrix<sub>uuid</sub>) TABLE \"test<sub>results.predictions</sub>\" CONSTRAINT \"matrix<sub>uuid</sub><sub>for</sub><sub>testpred</sub>\" FOREIGN KEY (matrix<sub>uuid</sub>) REFERENCES model<sub>metadata.matrices</sub>(matrix<sub>uuid</sub>) TABLE \"train<sub>results.predictions</sub>\" CONSTRAINT \"matrix<sub>uuid</sub><sub>for</sub><sub>trainpred</sub>\" FOREIGN KEY (matrix<sub>uuid</sub>) REFERENCES model<sub>metadata.matrices</sub>(matrix<sub>uuid</sub>) TABLE \"train<sub>results.predictions</sub>\" CONSTRAINT \"train<sub>predictions</sub><sub>matrix</sub><sub>uuid</sub><sub>fkey</sub>\" FOREIGN KEY (matrix<sub>uuid</sub>) REFERENCES model<sub>metadata.matrices</sub>(matrix<sub>uuid</sub>)"},{"location":"dirtyduck/ml_governance/#test-train_resultsevaluations","title":"<code>{test, train}_results.evaluations</code>","text":"<p>These tables lets us analyze how well our models are doing. Based on the config that we used for our <code>triage</code> run, <code>triage</code> is calculating metrics and storing them in this table, e.g., our model's precision in top 10%.</p> <pre><code>\\d test_results.evaluations\n</code></pre> Table \"test<sub>results.evaluations</sub>\" Column Type Collation Nullable Default model<sub>id</sub> integer not null evaluation<sub>start</sub><sub>time</sub> timestamp without time zone not null evaluation<sub>end</sub><sub>time</sub> timestamp without time zone not null as<sub>of</sub><sub>date</sub><sub>frequency</sub> interval not null metric character varying not null parameter character varying not null value numeric num<sub>labeled</sub><sub>examples</sub> integer num<sub>labeled</sub><sub>above</sub><sub>threshold</sub> integer num<sub>positive</sub><sub>labels</sub> integer sort<sub>seed</sub> integer matrix<sub>uuid</sub> text Indexes: \"evaluations<sub>pkey</sub>\" PRIMARY KEY, btree (model<sub>id</sub>, evaluation<sub>start</sub><sub>time</sub>, evaluation<sub>end</sub><sub>time</sub>, as<sub>of</sub><sub>date</sub><sub>frequency</sub>, metric, parameter) Foreign-key constraints: \"evaluations<sub>matrix</sub><sub>uuid</sub><sub>fkey</sub>\" FOREIGN KEY (matrix<sub>uuid</sub>) REFERENCES model<sub>metadata.matrices</sub>(matrix<sub>uuid</sub>) \"evaluations<sub>model</sub><sub>id</sub><sub>fkey</sub>\" FOREIGN KEY (model<sub>id</sub>) REFERENCES model<sub>metadata.models</sub>(model<sub>id</sub>) <p>Its columns are:</p> <ul> <li><code>model_id</code>: Our beloved <code>model_id</code> that we have encountered before.</li> <li><code>evaluation_start_time</code>: After training the model, we evaluate it on a test matrix. This column tells us the earliest time that an example in our test matrix could have.</li> <li><code>evaluation_end_time</code>: After training the model, we evaluate it on a test matrix. This column tells us the latest time that an example in our test matrix could have.</li> <li><code>metric</code>: Indicates which metric we are evaluating, e.g., <code>precision@</code>.</li> <li><code>parameter</code> ::Indicates at which parameter we are evaluating our metric, e.g., a metric of precision@ and a parameter of <code>100.0_pct</code> shows us the <code>precision@100pct</code>.</li> <li><code>value</code>: The value observed for our metric@parameter.</li> <li><code>num_labeled_examples</code>: The number of labeled examples in our test matrix. Why does it matter? It could be the case that we have entities that have no label for the test timeframe (for example, not all facilities will have an inspection). We still want to make predictions for these entities but can't include them when calculating performance metrics.</li> <li><code>num_labeled_above_threshold</code>: How many examples above our threshold were labeled?</li> <li><code>num_positive_labels</code>: The number of rows that had true positive labels.</li> </ul> <p>A look at the table shows that we have multiple rows for each model, each showing a different performance metric.</p> <pre><code>select\n    evaluation_end_time,\n    model_id,\n    metric || parameter as metric,\n    value,\n    num_labeled_examples,\n    num_labeled_above_threshold,\n    num_positive_labels\nfrom\n    test_results.evaluations\nwhere\n    parameter = '100.0_pct';\n</code></pre> evaluation<sub>end</sub><sub>time</sub> model<sub>id</sub> metric value num<sub>labeled</sub><sub>examples</sub> num<sub>labeled</sub><sub>above</sub><sub>threshold</sub> num<sub>positive</sub><sub>labels</sub> 2016-01-01 00:00:00 1 precision@100.0<sub>pct</sub> 0.6666666666666666 3 3 2 2016-01-01 00:00:00 1 recall@100.0<sub>pct</sub> 1.0 3 3 2 2017-01-01 00:00:00 2 precision@100.0<sub>pct</sub> 0.3333333333333333 3 3 1 2017-01-01 00:00:00 2 recall@100.0<sub>pct</sub> 1.0 3 3 1 <p>Remember that at 100%, the <code>recall</code> should be 1, and the <code>precision</code> is equal to the baserate. If these two things don't match, there are problems in your data, pipeline, etl. You must get this correct!</p> <p>What does this query tell us?</p> <p>We can now see how the different instances (trained on different temporal slices, but with the same model params) of a model group performs over time. Note how we only included the models that belong to our model group <code>1</code>.</p>"},{"location":"dirtyduck/ml_governance/#test_train_resultsaequitas","title":"<code>{test_train}_results.aequitas</code>","text":"<p>Standard evaluation metrics don't tell us the entire story: what are the biases in our models? what is the fairest model? Given the <code>bias_audit_config</code> in the experiment config in which we defined what protected attributes we care about (e.g. ethnicity) and the specific thresholds our model is going to be used, Triage uses Aequitas to generate a bias report on each model and matrix, similar to standard evaluation metrics. The <code>aequitas</code> tables will have a row for each combination of: - model_id - subset_hash - tie_breaker (e.g. best, worst) - evaluation_start_time - evaluation_end_time - parameter (e.g. <code>25_abs</code>, similar to evaluation metric thresholds) - attribute_name (e.g. 'facility_type') - attribute_value (e.g. 'kids_facility', 'restaurant')</p> <p>For each row Aequitas calculates the following group metrics:</p> Metric Formula Description Predicted Positive The number of entities within a group where the decision is positive, i.e.,   Total Predictive Positive The total number of entities predicted positive across groups defined by  Predicted Negative The number of entities within a group which decision is negative, i.e.,   Predicted Prevalence The fraction of entities within a group which were predicted as positive. Predicted Positive Rate The fraction of the entities predicted as positive that belong to a certain group. False Positive The number of entities of the group with  and  False Negative The number of entities of the group with  and  True Positive The number of entities of the group with  and  True Negative The number of entities of the group with  and  False Discovery Rate The fraction of false positives of a group within the predicted positive of the group. False Omission Rate The fraction of false negatives of a group within the predicted negative of the group. False Positive Rate The fraction of false positives of a group within the labeled negative of the group. False Negative Rate The fraction of false negatives of a group within the labeled positives of the group. <p>In the context of public policy and social good we want to avoid providing less benefits to specific groups of entities, if the intervention is assistive, as well as, avoid hurting more specific groups, if the intervention is punitive. Therefore we define bias as a disparity measure of group metric values of a given group when compared with a reference group. This reference can be selected using different criteria. For instance, one could use the majority group (with larger size) across the groups defined by A, or the group with minimum group metric value, or the traditional approach of fixing a historically favored group (e.g ethnicity:caucasian).</p> <p>Each disparity metric  for a given group  is calculated as follows: </p> <p>To read about the bias metrics saved in this table, look at the Aequitas documentation.</p> Table \"test_results.aequitas\" Column Type Collation Nullable Default model_id integer not null subset_hash character varying not null tie_breaker character varying not null evaluation_start_time timestamp without time zone not null evaluation_end_time timestamp without time zone not null matrix_uuid text parameter character varying not null attribute_name character varying not null attribute_value character varying not null total_entities integer group_label_pos integer group_label_neg integer group_size integer group_size_pct numeric prev numeric pp integer pn integer fp integer fn integer tn integer tp integer ppr numeric pprev numeric tpr numeric tnr numeric for numeric fdr numeric fpr numeric fnr numeric npv numeric precision numeric ppr_disparity numeric ppr_ref_group_value character varying pprev_disparity numeric pprev_ref_group_value character varying precision_disparity numeric precision_ref_group_value character varying fdr_disparity numeric fdr_ref_group_value character varying for_disparity numeric for_ref_group_value character varying fpr_disparity numeric fpr_ref_group_value character varying fnr_disparity numeric fnr_ref_group_value character varying tpr_disparity numeric tpr_ref_group_value character varying tnr_disparity numeric tnr_ref_group_value character varying npv_disparity numeric npv_ref_group_value character varying Statistical_Parity boolean Impact_Parity boolean FDR_Parity boolean FPR_Parity boolean FOR_Parity boolean FNR_Parity boolean TypeI_Parity boolean TypeII_Parity boolean Equalized_Odds boolean Unsupervised_Fairness boolean Supervised_Fairness boolean"},{"location":"dirtyduck/ml_governance/#test-train_resultspredictions","title":"<code>{test, train}_results.predictions</code>","text":"<p>You can think of the previous table <code>{test, train}_results.{test, train}_predictions</code> as a summary of individuals predictions that our model is making. But where can you find the individual predictions that our model is making? (So you can generate a list from here). And where can we find the test matrix that the predictions are based on? Let us introduce you to the <code>results.predictions</code> table.</p> <p>Here is what its first row looks like:</p> <pre><code>select\n    model_id,\n    entity_id,\n    as_of_date,\n    score,\n    label_value,\n    matrix_uuid\nfrom\n    test_results.predictions\nwhere\n    model_id = 1\norder by score desc;\n</code></pre> model<sub>id</sub> entity<sub>id</sub> as<sub>of</sub><sub>date</sub> score label<sub>value</sub> matrix<sub>uuid</sub> 1 229 2016-01-01 00:00:00 1.0 1 cd0ae68d6ace43033b49ee0390c3583e 1 355 2016-01-01 00:00:00 1.0 1 cd0ae68d6ace43033b49ee0390c3583e 1 840 2016-01-01 00:00:00 1.0 0 cd0ae68d6ace43033b49ee0390c3583e <p>As you can see, the table contains our models' predictions for a given entity and date.</p> <p>And do you notice the field <code>matrix_uuid</code>? Doesn't it look similar to the fields from above that gave us the names of our training matrices? In fact, it is the same. You can find the test matrix that was used to make this prediction under <code>TRIAGE_OUTPUT_PATH/matrices/{matrix_uuid}.csv</code>.</p>"},{"location":"dirtyduck/ml_governance/#test-train_resultsfeature_importances","title":"<code>{test, train}_results.feature_importances</code>","text":"<p>This tables store the feature importance of all the models.</p>"},{"location":"dirtyduck/ml_governance/#footnotes","title":"Footnotes","text":"<p><sup>1</sup> Literally from the configuration file. If you modify something it will generate a new hash. Handle with care!</p>"},{"location":"dirtyduck/problem_description/","title":"Description of the problem","text":"<p>This tutorial aims to introduce the reader to Triage, a machine learning modeling tool built by the Center for Data Science and Public Policy. We will use the well-known Chicago Food Inspections dataset<sup>1</sup>.</p> <p>We will present the two problems that <code>triage</code> was built to model<sup>2</sup>:</p> <ol> <li>Resource Prioritization Systems (also known as an inspections problem)<sup>3</sup> and</li> <li>Early Warning Systems<sup>4</sup>.</li> </ol>"},{"location":"dirtyduck/problem_description/#resource-prioritization-systems","title":"Resource Prioritization Systems","text":"<p>In an ideal world, inspectors would visit every food facility, every day<sup>5</sup> to ensure it meets safety standards. But the real world doesn't have enough inspectors for that to happen, so the city needs to decide how to allocate its limited inspection workforce to find and remediate as many establishments with food hazards as possible. Assuming the city can inspect n facilities in the next X period of time, they can define the problem as:</p> <p>Which n facilities will have a food violation in the following X period of time?</p> <p>If our inspection workforce is really limited, we should probably just target the most serious violations. Then we'd define the problem as:</p> <p>Which n facilities will have a critical or serious violation in the following X period of time?</p> <p>The answer to this question is a list of length n with the facilities at high risk of found a violation if they are inspected in the following X period of time.</p> <p>If you want to continue to this case studie click here</p>"},{"location":"dirtyduck/problem_description/#early-warning-systems","title":"Early Warning Systems","text":"<p>Using the same dataset (Chicago Food Inspections dataset), facility owners or managers would pose the machine learning (ML) problem as an early warning problem. They'd like to know whether an inspector is going to visit their facility so they can prepare for it. They can define the problem as:</p> <p>Will my facility be inspected in the next X period of time?</p> <p>If you want to continue to this case studie click here</p> <p>Important</p> <p>Note that in both case studies, resource prioritization and early warning systems we are defining a period of time in which the event will potentially happen. This makes this ML problem a prediction problem instead of a classification problem.</p>"},{"location":"dirtyduck/problem_description/#what-do-they-have-in-common","title":"What do they have in common?","text":"<p>For either problem, X could be a day, a week, month, a quarter, a year, 56 days, or some other time period.</p> <p>Without going into much detail, both problems use data where each row describes an event in which an entity was involved, and each event has a specific outcome or result.</p> <p>The entity for both inspection prioritizations and early warnings in this tutorial is a food facility, and the event is an inspection. But the outcome differs. For inspections the outcome is whether the inspection failed or major violation was found, while for early warning the outcome is whether the facility was inspected.</p>"},{"location":"dirtyduck/problem_description/#how-do-they-differ","title":"How do they differ?","text":"<p>Besides the obvious (e.g. the label), these ML's problem formulations have very different internal structure:</p> <p>Fot the EIS problem all of the entities of interest in a given period of time have a label. The Inspections problem does not have that luxury. Given all of the existing entities of interest only a fraction are inspected which means that only the inspected facilities will have a label (<code>True/False</code>) since these are the only entities with a known outcome (e.g a major violation was discovered during the inspection), but all of the remaining ones will not have a label. This will be reflected, in the training matrices since you only train on the facilities that were inspected (so you will have less rows in them). Another impact will be in the metrics. You will need to be very careful about interpreting the metrics in an inspections problem. Finally, when you are designing the field validation of your model, you need to take in account selection bias. If not, you will be inspecting the same facilities over and over and never inspect any facilities you have not inspected before.</p>"},{"location":"dirtyduck/problem_description/#whats-next","title":"What's next?","text":"<ul> <li>Learn more about early warning systems</li> <li>Learn more about resource prioritization systems</li> </ul> <ol> <li> <p>Several examples use this dataset, such as City of Chicago Food Inspection Forecasting, PyCon 2016 keynote: Built in Super Heroes, and PyData 2016: Forecasting critical food violations at restaurants using open data.\u00a0\u21a9</p> </li> <li> <p>It is also possible to do \"visit-level prediction\" type of ML problem.\u00a0\u21a9</p> </li> <li> <p>Examples include Predictive Enforcement of Hazardous Waste Regulations and Targeting Proactive Inspections for Lead Hazards.\u00a0\u21a9</p> </li> <li> <p>Examples include Increasing High School Graduation Rates: Early Warnings and Predictive Systems, Building Data-Driven Early Intervention Systems for Police Officers, and Data-Driven Justice Initiative: Identifying Frequent Users of Multiple Public Systems for More Effective Early Assistance.\u00a0\u21a9</p> </li> <li> <p>Defined as \"bakery, banquet hall, candy store, caterer, coffee shop, day care center (for ages less than 2), day care center (for ages 2 \u2013 6), day care center (combo, for ages less than 2 and 2 \u2013 6 combined), gas station, Golden Diner, grocery store, hospital, long term care center(nursing home), liquor store, mobile food dispenser, restaurant, paleteria, school, shelter, tavern, social club, wholesaler, or Wrigley Field Rooftop\" (source).\u00a0\u21a9</p> </li> </ol>"},{"location":"dirtyduck/triage_intro/","title":"Triage","text":"<p>Predictive analytics projects require coordinating many tasks, such as feature generation, classifier training, evaluation, and list generation. Each of these tasks is complicated in its own right, but it also needs to be combined with the other tasks throughout the course of the project.</p> <p>DSaPP built <code>triage</code> to facilitate the creation of supervised learning models, in particular binary classification models with a strong temporal component in the data.</p> <p>The dataset's temporal component mainly affects two modeling steps: feature creation (you need to be careful to avoid leaking information from the future through your features) and hyperparameter selection. <code>triage</code> solves both by splitting the data into temporal blocks and automating temporal cross-validation (TCC) and the feature generation.</p> <p><code>triage</code> uses the concept of an experiment. An experiment consists of a series of steps that aim to generate a good model for predicting the label of an entity in the data set. The steps are data time-splitting, label generation, feature generation, matrix creation, model training, predictions, and model evaluation. In each of these steps, <code>triage</code> will handle the temporal nuances of the data.</p> <p>Nowadays <code>triage</code> will help you to select the best model (model selection) and it allows you to explore and understand the behavior of your models using post-modeling techniques.</p> <p>You need to specify (via a configuration file) how you want to split your data temporally, which combination of machine learning algorithms and their hyperparameters you'd like to use, which kinds of features you want to generate, which subsets of those features you want to try in each model, and which metrics you'd like to use to evaluate performance and provide some criteria to select the best model.</p> <p>An experiment run consists in fitting every combination of algorithm, hyperparameters, and feature subsets to the temporally split data and evaluating their predictive performance on future data splits using the user's metrics.</p> <p><code>triage</code> calls a unique combination of algorithm, hyperparameters, and feature subsets a <code>model_group</code> and a model group fit to a specific data matrix a <code>model</code>. Our data typically span multiple time periods, so triage fits multiple models for each model group.</p> <p><code>triage</code> is simple to use, but it contains a lot of complex concepts that we will try to clarify in this section. First we will explain how to run <code>triage</code>, and then we will create a toy experiment that  helps explain triage's main concepts.</p>"},{"location":"dirtyduck/triage_intro/#triage-interface","title":"Triage interface","text":"<p>To run a <code>triage</code> experiment, you need the following:</p> <ul> <li> <p>A database with the data that you want to model.</p> <ul> <li>In this tutorial, the credentials are part of the <code>DATABASE_URL</code> environment variable</li> </ul> </li> <li> <p><code>triage</code> installed in your environment. You can verify that <code>triage</code> is indeed installed if you type in <code>bastion</code>:</p> </li> </ul> <pre><code>triage -h\n</code></pre> <ul> <li>An experiment config file. This is where the magic happens. We will discuss this file at length in this section of the tutorial.</li> </ul> <p>We are providing a <code>docker</code> container, <code>bastion</code>, that executes <code>triage</code> experiments. You already had the database (you were working on it the last two sections of this tutorial, remember?). So, like a real project, you just need to worry about the experiment configuration file.</p> <p>In the following section of the tutorial we will use a small experiment configuration file located at &lt;../triage/experiments/simple_test_skeleton.yaml&gt;.</p> <p>We will show you how to setup the experiment while explaining the inner workings of <code>triage</code>. We will modify the configuration file to show the effects of the configuration parameters. If you want to follow along, we suggest you copy that file and modify by yourself.</p> <p>You can run that experiment with:</p> <pre><code># Remember to run this in bastion NOT in your laptop!\ntriage experiment experiments/simple_test_skeleton.yaml\n</code></pre> <p>Every time you modify the configuration file and see the effects, you should execute the experiment again using the previous command.</p>"},{"location":"dirtyduck/triage_intro/#a-simple-triage-experiment","title":"A simple <code>triage</code> experiment","text":""},{"location":"dirtyduck/triage_intro/#a-brief-recap-of-machine-learning","title":"A brief recap of Machine Learning","text":"<p>Triage helps you to run a Machine learning experiment. An experiment in this context means the use of Machine Learning to explore a dynamic system in order to do some predictions about it.</p> <p>Before execute the any ML experiment, you need to define some boundaries:</p> <ul> <li>Which are the entities that you want to study?</li> <li>What will you want to know about them?</li> </ul> <p>In DSaPP, we build ML systems that aim to have social impact, i.e. help government offices, NGOs or other agents to do their job better. \"Do their job better\" means increase their reach (e.g. identify correctly more entities with some characteristics) using more efficiently their (scarce) resources (e.g. inspectors, medics, money, etc).</p> <p>With this optic, the boundaries are:</p> <ul> <li>Cohort: Which are the entities that you want to reach?</li> <li>Label: What will you want to know about them?</li> <li>Label timespan: In what time period?</li> <li>Update frequency: How frequently do you want to intervene?</li> <li>List size: How many resources do you have to intervene?</li> </ul> <p>Triage's experiment configuration file structures this information.</p>"},{"location":"dirtyduck/triage_intro/#cohorts-labels-event-dates-and-as-of-dates","title":"Cohorts, labels, event dates and as of dates","text":"<p>We will use the inspections prioritization as a narrative to help clarify these concepts:</p> <ul> <li>Which are the entities that you want to reach?*: Active facilities, i.e. facilities that exists at the day of the *planning inspections. We don't want to waste city resources (inspectors time) going to facilities that are out of business.</li> <li>What will you want to know about them?: Will those facilities fail the inspection?</li> <li>In what time period?: Will those facilities fail the inspection in the following month?</li> <li>How frequently do you want to intervene?: Every month.</li> <li>How many resources do you have to intervene?: We only have one inspector, so, one inspection per month</li> </ul> <p>To exemplify and explain the inner workings of <code>triage</code> in this scenario, we will use a subset of the <code>semantic.events</code> table with the following facilities (i.e. imagine that Chicago only has this three facilities):</p> <pre><code>select\n    entity_id,\n    license_num,\n    facility_aka,\n    facility_type,\n    activity_period\nfrom\n    semantic.entities\nwhere\n    license_num in (1596210, 1874347, 1142451)\norder by\n    entity_id asc;\n</code></pre> entity<sub>id</sub> license<sub>num</sub> facility<sub>aka</sub> facility<sub>type</sub> activity<sub>period</sub> 229 1596210 food 4 less grocery store [2010-01-08,) 355 1874347 mcdonalds restaurant [2010-01-12,2017-11-09) 840 1142451 jewel foodstore # 3345 grocery store [2010-01-26,) <p>The first thing <code>triage</code> does when executes the experiment, is split the time that the data covers in blocks considering the time horizon for the label ( Which facilities will fail an inspection in the following month? in this scenario of inspection prioritization<sup>1</sup>) . This time horizon is calculated from a set of specific dates (<code>as_of_date</code> in triage parlance) that divide the blocks in past (for training the model) and future (for testing the model). The set of <code>as_of_dates</code> is (mainly) calculated from the label timespan and the update frequency<sup>2</sup>. The as of date is not the event date. The event date occurred when the facility was inspected. The as of date is when the planning of the future facilities to be inspected happens.</p> <p><code>triage</code> will create those labels using information about the outcome of the event<sup>3</sup>, taking into account the temporal structure of the data. In our example: if a facility is inspected is an event, and whether it fails the inspection (outcome true) or not (outcome false).</p> <p>For a given entity on a given as of date, <code>triage</code> asks whether there's an outcome in the future time horizon. If so, <code>triage</code> will generate a label for that specific entity on that as of date.</p> <p>For this example, the label will be if given an as of date (e.g. January first, 2014), the facility will have a failed inspection in the following year.</p> <p>The following example hopefully will clarify the difference between outcome and label. We will focus on events (inspections) that happened in the year of 2014.</p> <pre><code>select\n    date,\n    entity_id,\n    (result = 'fail') as outcome\nfrom\n    semantic.events\nwhere\n    '[2014-01-01, 2015-01-01]'::daterange @&gt; date\n    and\n    entity_id in (229,355,840)\norder by\n    date asc;\n</code></pre> date entity<sub>id</sub> outcome 2014-01-14 840 f 2014-02-04 229 f 2014-02-24 840 t 2014-03-05 840 f 2014-04-10 355 t 2014-04-15 229 f 2014-04-18 355 f 2014-05-06 840 f 2014-08-28 355 f 2014-09-19 229 f 2014-09-30 355 t 2014-10-10 355 f 2014-10-31 840 f <p>We can observe that the facilities had several inspections, but in that timeframe <code>362</code> y <code>859</code> had failed inspections.</p> <p>Continuing the narrative, from the perspective of the day of <code>2014-01-01</code> (as of date), those facilities will have positive label.</p> <p>We can express that in a query and getting the labels for that as of date :</p> <pre><code>select\n    '2014-01-01' as as_of_date,\n    entity_id,\n    bool_or(result = 'fail')::integer as label\nfrom\n    semantic.events\nwhere\n    '2014-01-01'::timestamp &lt;= date\n    and date &lt; '2014-01-01'::timestamp + interval '1 year'\n    and entity_id in (229,355,840)\ngroup by\n    entity_id;\n</code></pre> as<sub>of</sub><sub>date</sub> entity<sub>id</sub> label 2014-01-01 229 0 2014-01-01 355 1 2014-01-01 840 1 <p>Note that ee transform the label to integer, since the machine learning algorithms only work with numeric data.</p> <p>We also need a way to store the state of each entity. We can group entities in cohorts defined by the state. The cohort can be used to decide which facilities you want to predict on (i.e. include in the ML train/test matrices). The rationale of this comes from the need to only predict for entities in a particular state: Is the restaurant new? Is this facility on this zip code? Is the facility \"active\"?<sup>4</sup></p> <p>We will consider a facility as active if a given as of date is in the interval defined by the <code>start_date</code> and <code>end_date</code>.</p> <pre><code>select\n    '2018-01-01'::date as as_of_date,\n    entity_id,\n    activity_period,\ncase when\nactivity_period @&gt; '2018-01-01'::date -- 2018-01-01 is as of date\nthen 'active'::text\nelse 'inactive'::text\nend as state\nfrom\n    semantic.entities\nwhere\n    entity_id in (229,355,840);\n</code></pre> as<sub>of</sub><sub>date</sub> entity<sub>id</sub> activity<sub>period</sub> state 2018-01-01 229 [2010-01-08,) active 2018-01-01 355 [2010-01-12,2017-11-09) inactive 2018-01-01 840 [2010-01-26,) active <p><code>Triage</code> will use a simple modification of the queries that we just discussed for automate the generation of the cohorts and labels for our experiment.</p>"},{"location":"dirtyduck/triage_intro/#experiment-configuration-file","title":"Experiment configuration file","text":"<p>The experiment configuration file is used to create the <code>experiment</code> object. Here, you will specify the temporal configuration, the features to be generated, the labels to learn, and the models that you want to train in your data.</p> <p>The configuration file is a <code>yaml</code> file with the following main sections:</p> <ul> <li> <p><code>temporal_config</code>: Temporal specification of the data, used for creating the blocks for temporal crossvalidation.</p> </li> <li> <p><code>cohort_config</code>: Using the state of the entities, define (using <code>sql</code>) cohorts to filter out objects that shouldn't be included in the training and prediction stages. This will generate a table call <code>cohort_{experiment_hash}</code></p> </li> <li> <p><code>label_config</code>: Specify (using <code>sql</code>) how to generate labels from the event's outcome. A table named <code>labels_{experiment_hash}</code> will be created.</p> </li> <li> <p><code>feature_aggregation</code>: Which spatio-temporal aggregations of the columns in the data set do you want to generate as features for the models?</p> </li> <li> <p><code>model_group_keys</code>: How do you want to identify the <code>model_group</code> in the database (so you can run analysis on them)?</p> </li> <li> <p><code>grid_config</code>: Which combination of hyperparameters and algorithms will be trained and evaluated in the data set?</p> </li> <li> <p><code>scoring</code>: Which metrics will be calculated?</p> </li> </ul> <p>Two of the more important (and potentially confusing) sections are <code>temporal_config</code> and <code>feature_generation</code>. We will explain them in detail in the next sections.</p> <p></p>"},{"location":"dirtyduck/triage_intro/#temporal-crossvalidation","title":"Temporal crossvalidation","text":"<p>Cross validation is a common technique to select a model that generalizes well to new data. Standard cross validation randomly splits the training data into subsets, fits models on all but one, and calculates the metric of interest (e.g. precision/recall) on the one left out, rotating through the subsets and leaving each out once. You select the model that performed best across the left-out sets, and then retrain it on the complete training data.</p> <p>Unfortunately, standard cross validation is inappropriate for most real-world data science problems. If your data have temporal correlations, standard cross validation lets the model peek into the future, training on some future observations and testing on past observations. To avoid this problem, you should design your training and testing to mimic how your model will be used, making predictions only using the data that would be available at that time (i.e. from the past).</p> <p>In temporal crossvalidation, rather than randomly splitting the dataset into training and test splits, temporal cross validation splits the data by time.</p> <p><code>triage</code> uses the <code>timechop</code> library for this purpose. <code>Timechop</code> will \"chop\" the data set in several temporal blocks. These blocks are then used for creating the features and matrices for training and evaluation of the machine learning models.</p> <p>Assume we want to select which restaurant (of two in our example dataset) we should inspect next year based on its higher risk of violating some condition. Also assume that the process of picking which facility is repeated every year on January 1<sup>st</sup><sup>5</sup></p> <p>Following the problem description template given in Section Description of the problem to solve, the question that we'll attempt to answer is:</p> <p>Which facility ( n=1 ) is likely to violate some inspected condition in the following year ( X=1 )?</p> <p>The traditional approach in machine learning is splitting the data in training and test datasets. Train or fit the algorithm on the training data set to generate a train model and test or evaluate the model on the test data set. We will do the same here, but, with the help of <code>timechop</code> we will take in account the time:</p> <p>We will fit models on training set up to 2014-01-01 and see how well those models would have predicted 2015; fit more models on training set up to 2015-01-01 and see how well those models would have predicted 2016; and so on. That way, we choose models that have historically performed best at our task, forecasting. It\u2019s why this approach is sometimes called evaluation on a rolling forecast origin because the origin at which the prediction is made rolls forward in time. <sup>6</sup></p> <p></p> <p>The data at which the model will do the predictions is denominated as as of date in <code>triage</code> (as of date = January first in our example). The length of the prediction time window (1 year) is called label span. Training and predicting with a new model as of date (every year) is the model update frequency.</p> <p>Because it's inefficient to calculate by hand all the as of dates or prediction points, <code>timechop</code> will take care of that for us. To do so, we need to specify some more constraints besides the label span  and the model update frequency:</p> <ul> <li>What is the date range covered by our data?</li> <li>What is the date range in which we have information about labels?</li> <li>How frequently do you receive information about your entities?</li> <li>How far in the future you want to predict?</li> <li>How much of the past data do you want to use?</li> </ul> <p>With this information, <code>timechop</code> will calculate as-of train and test dates from the last date in which you have label data, using the label span in both test and train sets, plus the constraints just mentioned.</p> <p>In total <code>timechop</code> uses 11 configuration parameters<sup>7</sup>.</p> <ul> <li> <p>There are parameters related to the boundaries of the available data set:</p> <ul> <li><code>feature_start_time</code>: data aggregated into features begins at this point (earliest date included in features)</li> <li><code>feature_end_time</code>: data aggregated into features is from before this point (latest date included in features)</li> <li><code>label_start_time</code>: data aggregated into labels begins at this point (earliest event date included in any label (event date &gt;= label<sub>start</sub><sub>time</sub>)</li> <li><code>label_end_time</code>: data aggregated is from before this point (event date &lt; label<sub>end</sub><sub>time</sub> to be included in any label)</li> </ul> </li> <li> <p>Parameters that control the labels' time horizon on the train and test sets:</p> <ul> <li> <p><code>training_label_timespans</code>: how much time is covered by training labels (e.g., outcomes in the next 3 days? 2 months? 1 year?) (training prediction span)</p> </li> <li> <p><code>test_label_timespans</code>: how much time is covered by test prediction (e.g., outcomes in the next 3 days? 2 months? 1 year?) (test prediction span)</p> </li> </ul> <p>These parameters will be used with the outcomes table to generate the labels. In an early warning setting, they will often have the same value. For inspections prioritization, this value typically equals <code>test_durations</code> and <code>model_update_frequency</code>.</p> </li> <li> <p>Parameters related about how much data we want to use, both in the future and in the past relative to the as-of date:</p> <ul> <li> <p><code>test_durations</code>: how far into the future should a model be used to make predictions (test span)</p> <p>NOTE: in the typical case of wanting a single prediction set immediately after model training, this should be set to 0 days</p> </li> </ul> <p>For early warning problems, <code>test_durations</code> should equal <code>model_update_frequency</code>. For inspection prioritization, organizational process determines the value: how far out are you scheduling for?</p> <p>The equivalent of <code>test_durations</code> for the training matrices is <code>max_training_histories</code>:</p> <ul> <li><code>max_training_histories</code>: the maximum amount of history     for each entity to train on (early matrices may contain less     than this time if it goes past label/feature start times). If     patterns have changed significantly, models trained on recent     data may outperform models trained on a much lengthier     history.</li> </ul> </li> <li> <p>Finally, we should specify how many rows per <code>entity_id</code> in the train and test matrix:</p> <ul> <li> <p><code>training_as_of_date_frequencies</code>: how much time between     rows for a single entity in a training matrix (list time     between rows for same entity in train matrix).</p> </li> <li> <p><code>test_as_of_date_frequencies</code>: how much time between rows     for a single entity in a test matrix (time between rows for     same entity in test matrix).</p> </li> </ul> </li> </ul> <p>The following images (we will show how to generate them later) shows the time blocks created by several temporal configurations. We will change a parameter at a time so you could see how it affects the  resulting blocks.</p> <p>If you want to try the modifications (or your own) and generate the temporal blocks images run the following (they'll be generated in &lt;./images/&gt;):</p> <pre><code># Remember to run this in bastion NOT in laptop's shell!\ntriage experiment experiments/simple_test_skeleton.yaml --show-timechop\n</code></pre> <ul> <li> <p><code>{feature, label}_{end, start}_time</code></p> <p>The image below shows these <code>{feature, label}_start_time</code> are equal, as are the <code>{feature, label}_end_time</code>. These parameters show in the image as dashed vertical black lines. This setup will be our baseline example.</p> <p>The plot is divided in two horizontal lines (\"Block 0\" and \"Block 1\"). Each line is divided by vertical dashed lines \u2013 the grey lines outline the boundaries of the data for features and data for labels, which in this image coincide. The black dash lines represent the beginning and the end of the test set. In \"Block 0\" those lines correspond to <code>2017</code> and <code>2018</code>, and in \"Block 1\" they correspond to <code>2016</code> and <code>2017</code>.</p> <p></p> <p>The shaded areas (in this image there is just one per block, but you will see other examples below) represents the span of the as of dates. They start with the oldest as of date and end with the latest. Each line inside that area represents the label span. Those lines begin at the as of date. At each as of date, timechop generates each entity's features (from the past) and labels (from the future). So in the image, we will have two sets of train/test datasets. Each facility will have 13 rows in \"Block 0\" and 12 rows in \"Block 1\". The trained models will predict the label using the features calculated for that test set as of date. The single line represents the label's time horizon in testing.</p> <p>This is the temporal configuration that generated the previous image:</p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '0d'\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '1y'\n</code></pre> <p>In that configuration the date ranges of features and labels are equal, but they can be different (maybe you have more data for features that data for labels) as is shown in the following image and in their configuration parameters.</p> <p></p> <pre><code>temporal_config:\n    feature_start_time: '2010-01-01'   # &lt;------- The change happened here!\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '0d'\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '1y'\n</code></pre> </li> <li> <p><code>model_update_frequency</code></p> <p>From our baseline <code>temporal_config</code> example (102), we will change how often we want a new model, which generates more time blocks (if there are time-constrained data, obviously).</p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '6month' # &lt;------- The change happened here!\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '0d'\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '1y'\n</code></pre> <p></p> </li> <li> <p><code>max_training_histories</code></p> <p>With this parameter you could get a growing window for training (depicted in 110) or as in all the other examples, fixed training windows.</p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '0d'\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '10y'  # &lt;------- The change happened here!\n</code></pre> <p></p> </li> <li> <p><code>_as_of_date_frequencies</code> and <code>test_durations</code></p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '3month' # &lt;------- The change happened here!\n\n    test_durations: '0d'\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '10y'\n</code></pre> <p></p> <p>Now, change <code>test_as_of_date_frequencies</code>:</p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '0d'\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '3month'&lt;------- The change happened here!\n\n    max_training_histories: '10y'\n</code></pre> <p></p> <p>Nothing changed because the test set doesn't have \"space\" to allow more spans. The \"space\" is controlled by <code>test_durations</code>, so let's change it to <code>6month</code>:</p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '6month' &lt;------- The change happened here!\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '10y'\n</code></pre> <p></p> <p>So, now we will move both parameters: <code>test_durations</code>, <code>test_as_of_date_frequencies</code></p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '6month' &lt;------- The change happened here!\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '3month' &lt;------- and also here!\n\n    max_training_histories: '10y'\n</code></pre> <p></p> </li> <li> <p><code>_label_timespans</code></p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1y']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '0d'\n    test_label_timespans: ['3month']  &lt;------- The change happened here!\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '10y'\n</code></pre> <p></p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2018-01-01'\n    label_start_time: '2014-01-02'\n    label_end_time: '2018-01-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['3month'] &lt;------- The change happened here!\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '0d'\n    test_label_timespans: ['1y']\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '10y'\n</code></pre> <p></p> <p>That's it! Now you have the power to bend time!<sup>8</sup></p> <p>With the time blocks defined, <code>triage</code> will create the labels and then the features for our train and test sets. We will discuss features in the following section.</p> </li> </ul> <p></p>"},{"location":"dirtyduck/triage_intro/#feature-generation","title":"Feature generation","text":"<p>We will show how to create features using the experiments config file. <code>triage</code> uses <code>collate</code> for this.<sup>9</sup> The <code>collate</code> library controls the generation of features (including the imputation rules for each feature generated) using the time blocks generated by <code>timechop</code>. <code>Collate</code> helps the modeler create features based on spatio-temporal aggregations into the as of date. <code>Collate</code> generates <code>SQL</code> queries that will create features per each as of date.</p> <p>As before, we will try to mimic what <code>triage</code> does behind the scenario. <code>Collate</code> will help you to create features based on the following template:</p> <p>For a given as of date, how the aggregation function operates into a column taking into account a previous time interval and some attributes.</p> <p>Two possible features could be framed as:</p> <pre><code>As of 2016-01-01, how many inspections\nhas each facility had in the previous 6 months?\n</code></pre> <p>and</p> <pre><code>As of 2016-01-01, how many \"high risk\" findings has the\nfacility had in the previous 6 months?\n</code></pre> <p>In our data, that date range (between 2016-01-01 and 2015-07-01) looks like:</p> <pre><code>select\n    event_id,\n    date,\n    entity_id,\n    risk\nfrom\n    semantic.events\nwhere\n    date &lt;@ daterange(('2016-01-01'::date - interval '6 months')::date, '2016-01-01')\n    and entity_id in (229,355,840)\norder by\n    date asc;\n</code></pre> event<sub>id</sub> date entity<sub>id</sub> risk 1561324 2015-07-17 840 high 1561517 2015-07-24 840 high 1562122 2015-08-12 840 high 1547403 2015-08-20 229 high 1547420 2015-08-28 229 high 1547448 2015-09-14 355 medium 1547462 2015-09-21 355 medium 1547504 2015-10-09 355 medium 1547515 2015-10-16 355 medium 1583249 2015-10-21 840 high 1583577 2015-10-28 840 high 1583932 2015-11-04 840 high <p>We can transform those data to two features: <code>number_of_inspections</code> and <code>flagged_as_high_risk</code>:</p> <pre><code>select\n    entity_id,\n    '2016-01-01' as as_of_date,\n    count(event_id) as inspections,\n    count(event_id) filter (where risk='high') as flagged_as_high_risk\nfrom\n    semantic.events\nwhere\n    date &lt;@ daterange(('2016-01-01'::date - interval '6 months')::date, '2016-01-01')\n    and entity_id in (229,355,840)\ngroup by\n    grouping sets(entity_id);\n</code></pre> entity<sub>id</sub> as<sub>of</sub><sub>date</sub> inspections flagged<sub>as</sub><sub>high</sub><sub>risk</sub> 229 2016-01-01 2 2 355 2016-01-01 4 0 840 2016-01-01 6 6 <p>This query is making an aggregation. Note that the previous <code>SQL</code> query has five parts:</p> <ul> <li>The filter ((<code>risk = 'high')::int</code>)</li> <li>The aggregation function (<code>count()</code>)</li> <li>The name of the resulting transformation (<code>flagged_as_high_risk</code>)</li> <li>The context in which it is aggregated (by <code>entity_id</code>)</li> <li>The date range (between 2016-01-01 and 6 months before)</li> </ul> <p>What about if we want to add proportions and totals of failed and passed inspections?</p> <pre><code>select\n    entity_id,\n    '2016-01-01' as as_of_date,\n    count(event_id) as inspections,\n    count(event_id) filter (where risk='high') as flagged_as_high_risk,\n    count(event_id) filter (where result='pass') as passed_inspections,\n    round(avg((result='pass')::int), 2) as proportion_of_passed_inspections,\n    count(event_id) filter (where result='fail') as failed_inspections,\n    round(avg((result='fail')::int), 2) as proportion_of_failed_inspections\nfrom\n    semantic.events\nwhere\n    date &lt;@ daterange(('2016-01-01'::date - interval '6 months')::date, '2016-01-01')\n    and entity_id in (229,355,840)\ngroup by\n    grouping sets(entity_id)\n</code></pre> entity<sub>id</sub> as<sub>of</sub><sub>date</sub> inspections flagged<sub>as</sub><sub>high</sub><sub>risk</sub> passed<sub>inspections</sub> proportion<sub>of</sub><sub>passed</sub><sub>inspections</sub> failed<sub>inspections</sub> proportion<sub>of</sub><sub>failed</sub><sub>inspections</sub> 229 2016-01-01 2 2 1 0.50 1 0.50 355 2016-01-01 4 0 1 0.25 2 0.50 840 2016-01-01 6 6 4 0.67 2 0.33 <p>But what if we want to also add features for \"medium\" and \"low\" risk? And what would the query look like if we want to use several time intervals, like 3 months, 5 years, etc? What if we want to contextualize this by location? Plus we need to calculate all these features for several as of dates and manage the imputation strategy for all of them!!!</p> <p>You will realize that even with this simple set of features we will require very complex <code>SQL</code> to be constructed.</p> <p>But fear not. <code>triage</code> will automate that for us!</p> <p>The following blocks of code represent a snippet of <code>triage</code>'s configuration file related to feature aggregation. It shows the <code>triage</code> syntax for the <code>inspections</code> feature constructed above:</p> <pre><code>feature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates:\n      -\n        quantity:\n          total: \"*\"\n        imputation:\n           count:\n              type: 'zero_noflag'\n        metrics:\n          - 'count'\n\n    intervals: ['6month']\n</code></pre> <p><code>feature_aggregations</code> is a <code>yaml</code> list<sup>10</sup> of feature groups construction specification or just feature group. A feature group is a way of grouping several features that share <code>intervals</code> and source data (<code>from_obj</code>). <code>triage</code> requires the following configuration parameter for every feature group:</p> <ul> <li><code>prefix</code>: This will be used for name of the feature created</li> <li><code>from_obj</code>: Represents a <code>TABLE</code> object in <code>PostgreSQL</code>. You can pass a table like in the example above (<code>semantic.events</code>) or a <code>SQL</code> query that returns a table. We will see an example of this later. <code>triage</code> will use it like the <code>FROM</code> clause in the <code>SQL</code> query.</li> <li><code>knowlege_date_column</code>: Column that indicates the date of the event.</li> <li><code>intervals</code>: A <code>yaml</code> list. <code>triage</code> will create one feature per interval listed.</li> </ul> <p>The last section to discuss is <code>imputation</code>. Imputation is very important step in the modeling, and you should carefully think about how you will impute the missing values in the feature. After deciding the best way of impute each feature, you should avoid leakage (For example, imagine that you want to impute with the mean one feature. You could have leakage if you take all the values of the column, including ones of the future to calculate the imputation). We will return to this later in this tutorial.</p> <p><code>Collate</code> is in charge of creating the <code>SQL</code> agregation queries. Another way of thinking about it is that <code>collate</code> encapsulates the <code>FROM</code> part of the query (<code>from_obj</code>) as well as the <code>GROUP BY</code> aggregation at the <code>entity_id</code> level.</p> <p><code>triage</code> (<code>collate</code>) supports two types of objects to be aggregated: <code>aggregates</code> and <code>categoricals</code> (more on this one later)<sup>11</sup>. The <code>aggregates</code> subsection represents a <code>yaml</code> list of features to be created. Each element on this represents a column (<code>quantity</code>, in the example, the whole row <code>*</code>) and an alias (<code>total</code>), defines the <code>imputation</code> strategy for <code>NULLs</code>, and the <code>metric</code> refers to the <code>aggregation function</code> to be applied to the <code>quantity</code> (<code>count</code>).</p> <p><code>triage</code> will generate the following (or a very similar one), one per each combination of <code>interval</code> \u00d7 <code>quantity</code>:</p> <pre><code>select\n  metric(quantity) as alias\nfrom\n  from_obj\nwhere\n  as_of_date &lt;@ (as_of_date - interval, as_of_date)\ngroup by\n  grouping sets(entity_id)\n</code></pre> <p>With the previous configuration <code>triage</code> will generate 1 feature with the following name:<sup>12</sup></p> <ul> <li><code>inspections_entity_id_6month_total_count</code></li> </ul> <p>All the features of that feature group (in this case only 1) will be stored in the table.</p> <ul> <li><code>features.inspections_aggregation_imputed</code></li> </ul> <p>In general the names of the generated tables are constructed as follows:</p> <pre><code>schema.prefix_group_aggregation_imputed\n</code></pre> <p>NOTE: the outputs are stored in the <code>features</code> schema.</p> <p>Inside each of those new tables, the feature name will follow this pattern:</p> <pre><code>prefix_group_interval_alias_aggregation_operation\n</code></pre> <p>If we complicate a little the above configuration adding new intervals:</p> <pre><code>feature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates:\n      - # number of inspections\n        quantity:\n          total: \"*\"\n\n        imputation:\n          count:\n            type: 'zero_noflag'\n\n        metrics: ['count']\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n</code></pre> <p>You will end with 5 new features, one for each interval (5) \u00d7 the only aggregate definition we have. Note the weird <code>all</code> in the <code>intervals</code> definition. <code>all</code> is the time interval between the <code>feature_start_time</code> and the <code>as_of_date</code>.</p> <p><code>triage</code> also supports <code>categorical</code> objects. The following code adds a feature for the <code>risk</code> flag.</p> <pre><code>feature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates:\n      - # number of inspections\n        quantity:\n          total: \"*\"\n\n        imputation:\n          count:\n            type: 'zero_noflag'\n\n        metrics: ['count']\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n\n  -\n    prefix: 'risks'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    categoricals_imputation:\n      sum:\n        type: 'zero'\n\n    categoricals:\n      -\n        column: 'risk'\n        choice_query: 'select distinct risk from semantic.events'\n          metrics:\n            - 'sum'\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n</code></pre> <p>There are several changes. First, the imputation strategy in this new feature group is for every categorical features in that feature group (in that example only one). The next change is the type: instead of <code>aggregates</code>, it's <code>categoricals</code>. <code>categoricals</code> define a <code>yaml</code> list too. Each <code>categorical</code> feature needs to define a <code>column</code> to be aggregated and the query to get all the distinct values.</p> <p>With this configuration, <code>triage</code> will generate two tables, one per feature group. The new table will be <code>features.risks_aggregation_imputed</code>. This table will have more columns: <code>intervals</code> (5) \u00d7 <code>metric</code> (1) \u00d7 features (1) \u00d7 number of choices returned by the query.</p> <p>The query:</p> <pre><code>select distinct risk from semantic.events;\n</code></pre> risk \u00a4 medium high low <p>returns 4 possible values (including <code>NULL</code>). When dealing with categorical aggregations you need to be careful. Could be the case that in some period of time, in your data, you don't have all the possible values of the categorical variable. This could cause problems down the road. Triage allows you to specify the possible values (choices) of the variable. Instead of using <code>choice_query</code>, you could use <code>choices</code> as follows:</p> <pre><code>feature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates:\n      - # number of inspections\n        quantity:\n          total: \"*\"\n\n        imputation:\n          count:\n            type: 'mean'\n\n        metrics: ['count']\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n\n  -\n    prefix: 'risks'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    categoricals_imputation:\n      sum:\n        type: 'zero'\n\n    categoricals:\n      -\n        column: 'risk'\n        choices: ['low', 'medium', 'high']\n          metrics:\n            - 'sum'\n\n    intervals: ['1month', '3month', '6month', '1y', 'all']\n</code></pre> <p>In both cases <code>triage</code> will generate <code>20</code> new features, as expected.</p> <p>The features generated from categorical objects will have the following pattern:</p> <pre><code>prefix_group_interval_column_choice_aggregation_operation\n</code></pre> <p>So, <code>risks_entity_id_1month_risk_medium_sum</code> will be among our new features in the last example.</p> <p><code>Triage</code> will also add several imputation flag (binary) columns per feature. Those columns convey information about if that particular value was imputed or not. The section below describes how imputation can be specified in <code>triage</code> in more detail.</p>"},{"location":"dirtyduck/triage_intro/#imputation","title":"Imputation","text":"<p><code>Triage</code> currently supports the following imputation strategies:</p> <ul> <li> <p><code>mean</code>: The mean value of the feature.</p> </li> <li> <p><code>constant</code>: Fill with a constant (you need to provide the constant value).</p> </li> <li> <p><code>zero</code>: Same that the previous one, but the constant is zero.</p> </li> <li> <p><code>zero_noflag</code>: Sometimes, the absence (i.e. a NULL)     doesn't mean that the value is missing, that actually means that     the event didn't happen to that entity. For example a <code>NULL</code> in     the <code>inspections_entity_id_1month_total_count</code> column in     <code>features.inspections_aggreagtion_imputed</code> doesn't mean that the     value is missing, it means that zero inspections happen to that     facility in the last month. Henceforth, the flag column is not     needed.</p> </li> </ul> <p>Only for aggregates:</p> <ul> <li><code>binary_mode</code>: Takes the mode of a binary feature</li> </ul> <p>Only for categoricals:</p> <ul> <li><code>null_category</code>: Just flag null values with the null category column</li> </ul> <p>and finally, if you are sure that is not possible to have NULLS:</p> <ul> <li><code>error</code>: Raise an exception if ant null values are encountered.</li> </ul>"},{"location":"dirtyduck/triage_intro/#feature-groups-strategies","title":"Feature groups strategies","text":"<p>Another interesting thing that <code>triage</code> controls is how many feature groups are used in the machine learning grid. This would help you to understand the effect of using different groups in the final performance of the models.</p> <p>In <code>simple_test_skeleton.yaml</code> you will find the following blocks:</p> <pre><code>feature_group_definition:\n  prefix:\n    - 'results'\n    - 'risks'\n    - 'inspections'\n\nfeature_group_strategies: ['all']\n</code></pre> <p>This configuration adds to the number of model groups to be created.</p> <p>The possible feature group strategies are:</p> <ul> <li><code>all</code>: All the features groups are used.</li> <li><code>leave-one-out</code>: All the combinations of: \"All the feature groups except one are used\".</li> <li><code>leave-one-in</code>: All the combinations of \"One feature group except the rest is used\"</li> <li><code>all-combinations</code>: All the combinations of feature groups</li> </ul> <p>In order to clarify these concepts, let's use <code>simple_test_skeleton.yaml</code> configuration file. In it there are three feature groups: <code>inspections</code>, <code>results</code>, <code>risks</code>.</p> <p>Using <code>all</code> will create just one set containg all the features of the three feature groups:</p> <ul> <li><code>{inspections, results, risks}</code></li> </ul> <p>If you modify <code>feature_group_strategies</code> to <code>['leave-one-out']</code>: the following sets will be created:</p> <ul> <li><code>{inspections, results}, {inspections, risks}, {results, risks}</code></li> </ul> <p>Using the <code>leave-one-in</code> strategy:</p> <ul> <li><code>{inspections}, {results}, {risks}</code></li> </ul> <p>Finally choosing <code>all-combinations</code>:</p> <ul> <li><code>{inspections}, {results}, {risks}, {inspections, results}</code>, <code>{inspections, risks}, {results, risks}, {inspections, results, risks}</code></li> </ul>"},{"location":"dirtyduck/triage_intro/#controlling-the-size-of-the-tables","title":"Controlling the size of the tables","text":"<p>This section is a little technical, you can skip it if you fell like it.</p> <p>By default, <code>triage</code> will use the biggest column type for the features table (<code>integer</code>, <code>numeric</code>, etc). This could lead to humongous tables, with sizes several hundred of gigabytes. <code>Triage</code> took that decision, because it doesn't know anything about the possible values of your data (e.g. Is it possible to have millions of inspections in one month? or just a few dozens?).</p> <p>If you are facing this difficulty, you can force <code>triage</code> to cast the column in the features table. Just add <code>coltype</code> to the <code>aggregate/categorical</code> block:</p> <pre><code> aggregates:\n   -\n    quantity:\n      total: \"*\"\n    metrics: ['count']\n    coltype: 'smallint'\n</code></pre> <p></p>"},{"location":"dirtyduck/triage_intro/#the-grid","title":"The Grid","text":"<p>Before applying Machine Learning to your dataset you don't know which combination of algorithm and hyperparameters will be the best given a specific matrix.</p> <p><code>Triage</code> approaches this problem exploring a algorithm + hyperparameters + feature groups grid. At this time, this exploration is a exhaustive one, i.e. it covers the complete grid, so you would get (number of algorithms) \\times (number of hyperparameters) \\times (number of feature group strategies) models groups. The number of models trained is (number of model groups) \\times (number of time splits).</p> <p>In our simple experiment the grid is very simple:</p> <pre><code>grid_config:\n    'sklearn.dummy.DummyClassifier':\n        strategy: [most_frequent]\n</code></pre> <p>Just one algorithm and one hyperparameter (also we have only one feature group strategy: <code>all</code>), and two time splits. So we will get 2 models, 1 model group.</p> <p>Keep in mind that the grid is providing more than way to select a model. You can use the tables generated by the grid (see section Machine Learning Governance ) and analyze and understand your data. In other words, analyzing the results (evaluations, predictions, hyperparameter space, etc.) is like applying Data mining concepts to your data using Machine learning. We will return to this when we apply post modeling to our models.</p>"},{"location":"dirtyduck/triage_intro/#audition","title":"Audition","text":"<p>Audition is a tool for helping you select a subset of trained classifiers from a triage experiment. Often, production-scale experiments will come up with thousands of trained models, and sifting through all of those results can be time-consuming even after calculating the usual basic metrics like precision and recall.</p> <p>You will be facing questions as:</p> <ul> <li>Which metrics matter most?</li> <li>Should you prioritize the best metric value over time or treat recent data as most important?</li> <li>Is low metric variance important?</li> </ul> <p>The answers to questions like these may not be obvious. Audition introduces a structured, semi-automated way of filtering models based on what you consider important.</p>"},{"location":"dirtyduck/triage_intro/#post-modeling","title":"Post-modeling","text":"<p>As the name indicates, postmodeling occurs after you have modeled (potentially) thousands of models (different hyperparameters, different time windows, different algorithms, etc), and using <code>audition</code> you pre selected a small number of models.</p> <p>Now, with the postmodeling tools you will be able to select your final model for production use.</p> <p>Triage's postmodeling capabilities include:</p> <ul> <li>Show the score distribution</li> <li>Compare the list generated by a set of models</li> <li>Compare the feature importance between a set of models</li> <li>Diplay the probability calibration curves</li> <li>Analyze the errors using a decision tree trained on the errors of the model.</li> <li>Cross-tab analysis</li> <li>Bias analysis</li> </ul> <p>If you want to see Audition and Postmodeling in action, we will use them after Inspections and EIS modeling.</p>"},{"location":"dirtyduck/triage_intro/#final-cleaning","title":"Final cleaning","text":"<p>In the next section we will start modeling, so it is a good idea to clean the <code>{test, train}_results</code> schemas and have a fresh start:</p> <pre><code>select nuke_triage();\n</code></pre> nuke<sub>triage</sub> triage was send to the oblivion. Long live to triage! <p><code>triage</code> also creates a lot of files (we will see why in the next section). Let's remove them too.</p> <pre><code>rm -r /triage/matrices/*\n</code></pre> <pre><code>rm -r /triage/trained_models/*\n</code></pre>"},{"location":"dirtyduck/triage_intro/#where-to-go-from-here","title":"Where to go from here...","text":"<p>If you haven't done so already, our dirty duck tutorial is a good way to geet up and running with some sample data.</p> <p>If you're ready to get started with your own data, check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project.</p>"},{"location":"dirtyduck/triage_intro/#footnotes","title":"Footnotes","text":"<p><sup>1</sup> Would be my restaurant inspected in the following month? in the case of an early warning case.</p> <p><sup>2</sup> It's a little more complicated than that as we will see.</p> <p><sup>3</sup> All events produce some outcome. In theory every event of interest in stored in a database. These events are immutable: you can't (shouldn't) change them (they already happen).</p> <p><sup>4</sup> We could consider different states, for example: we can use the column <code>risk</code> as an state. Another possibility is define a new state called <code>failed</code> that indicates if the facility failed in the last time it was inspected. One more: you could create cohorts based on the <code>facility_type.</code></p> <p><sup>5</sup> The city in this toy example has very low resources.</p> <p><sup>6</sup> See for example: https://robjhyndman.com/hyndsight/tscv/</p> <p><sup>7</sup> I know, I know. And in order to cover all the cases, we are still missing one or two parameters, but we are working on it.</p> <p><sup>8</sup> Obscure reference to the \"Avatar: The Last Airbender\" cartoon series. I'm sorry.</p> <p><sup>9</sup> <code>collate</code> is to feature generation what <code>timechop</code> is to date temporal splitting</p> <p><sup>10</sup> <code>triage</code> uses a lot of <code>yaml</code>, this guide could be handy</p> <p><sup>11</sup> Note that the name <code>categoricals</code> is confusing here: The original variable (i.e. a column) is categorical, the aggregate of that column is not. The same with the <code>aggregates</code>: The original column could be a categorical or a numeric (to be fare most of the time is a numeric column, but see the example: we are counting), and then <code>triage</code> applies an aggregate that will be numeric. That is how triage named things, and yes, I know is confusing.</p> <p><sup>12</sup> <code>triage</code> will generate also a new binary column that indicates if the value of the feature was imputed (<code>1</code>) or not (<code>0</code>): <code>inspections_entity_id_6month_total_count_imp</code>.</p>"},{"location":"dirtyduck/who_is_this_tutorial_for/","title":"Who is this tutorial for?","text":"<p>We created this tutorial with two roles in mind:</p> <ul> <li> <p>A data scientist/ML practitioner who wants to focus in the problem at his/her hands, not in the nitty-gritty detail about how to configure and setup a Machine learning pipeline, Model governance, Model selection, etc.</p> </li> <li> <p>A policy maker with a little of technical background that wants to   learn how to pose his/her policy problem as a Machine Learning   problem.</p> </li> </ul>"},{"location":"experiments/algorithm/","title":"Experiment Algorithm Deep Dive","text":"<p>This guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some experience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation phase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their needs, this should help.</p>"},{"location":"experiments/algorithm/#1-temporal-validation-setup","title":"1. Temporal Validation Setup","text":"<p>First, the given <code>temporal_config</code> section in the experiment definition is transformed into train and test splits, including <code>as_of_times</code> for each matrix.</p> <p>We create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time at the rate of the given <code>model_update_frequency</code>, until we get to the earliest reasonable split time.</p> <p>For each split, we create <code>as_of_times</code> by moving either backwards from the split time towards the <code>max_training_history</code> (for train matrices) or forwards from the split time towards the <code>test_duration</code> (for test matrices) at the provided <code>data_frequency</code>.</p> <p>Many of these configured values may be lists, in which case we generate the cross-product of all the possible values and generate more splits.</p> <p>For a more detailed look at the temporal validation logic, see Temporal Validation Deep Dive.</p> <p>The train and test splits themselves are not used until the Building Matrices section, but a flat list of all computed <code>as_of_times</code> for all matrices needed in the experiment is used in the next section, Transforming Data.</p>"},{"location":"experiments/algorithm/#2-transforming-data","title":"2. Transforming Data","text":"<p>With all of the <code>as_of_times</code> for this Experiment now computed, it's now possible to transform the input data into features and labels as of all the required times.</p>"},{"location":"experiments/algorithm/#labels","title":"Labels","text":"<p>The Experiment populates a 'labels' table using the following input: 1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given as_of_date and label_timespan.</p> <ol> <li>Each as_of_date and label_timespan defined in temporal config</li> </ol> <p>For instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching inspections) would look like:</p> <pre><code>    select\n    events.entity_id,\n    bool_or(outcome::bool)::integer as outcome\n    from events\n    where '{as_of_date}' &lt;= outcome_date\n        and outcome_date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\n        group by entity_id\n</code></pre> <p>This binary labels table is scoped to the entire Experiment, so all <code>as_of_time</code> (computed in step 1) and <code>label_timespan</code> (taken straight from <code>temporal_config</code>) combinations are present. Additionally, the 'label_name' and 'label_type' are also recorded with each row in the table.</p> <p>The name of the labels table is based on both the name of the label and a hash of the label query (e.g <code>labels_failedviolation_a0b1c2d3</code>), so any prior experiments that shared both the name and query will be able to reuse the labels table.  If the 'replace' flag was sent, for each <code>as_of_time</code> and <code>label_timespan</code>, the labels table is queried to check if any rows exist that match. If any such rows exist, the labels query for that date and timespan is not run.</p> <p>At this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix. How these rows have their labels represented is up to the configured <code>include_missing_labels_in_train_as</code> value in the experiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see 'Retrieving Data and Saving Completed Matrix')</p>"},{"location":"experiments/algorithm/#cohort-table","title":"Cohort Table","text":"<p>The Experiment keeps track of the which entities are in the cohort on any given date. Similarly to the labels table, the experiment populates a cohort table. using one of two options:</p> <ol> <li> <p>A query, provided by the user in the configuration file, that generates entity_ids for a given as_of_date. This is run for each as_of_date as generated from the temporal config.</p> </li> <li> <p>All distinct entity ids and as_of_dates in the labels table, if no query is provided by the user in the configuration file.</p> </li> </ol> <p>This cohort table is scoped to the entire Experiment, so all <code>as_of_times</code> (computed in step 1) are present. </p> <p>The name of the cohort table is based on both the name of the cohort and a hash of the cohort query (e.g <code>cohort_permitted_a0b1c2d3</code>), so any prior experiments that shared both the name and query will be able to reuse the cohort table.  If the 'replace' flag was sent, for each <code>as_of_time</code>, the cohort table is queried to check if any rows exist that match. If any such rows exist, the cohort query for that date is not run.</p>"},{"location":"experiments/algorithm/#features","title":"Features","text":"<p>Each provided <code>feature_aggregation</code> configures the creation and population of several feature tables in the 'features' schema: one for each of the groups specified in the config, one that merges the groups together into one table, and one that fills in null values from the merged table with imputed values based on imputation config.</p>"},{"location":"experiments/algorithm/#generating-aggregation-sql","title":"Generating Aggregation SQL","text":"<p>To generate the SQL that creates the pre-imputation table, the Experiment assembles building blocks from the feature aggregation config, as well as the experiment's list of <code>as_of_times</code>:</p> <ul> <li><code>from_obj</code> represents, well, the object of the FROM clause in the SQL query. Often this is just a table, but can be configured to be a subquery. This holds all the data that we want to aggregate into features</li> <li>Each <code>as_of_time</code> in the experiment and <code>interval</code> in the <code>feature_aggregation</code> is combined with the <code>knowledge_date_column</code> to create a WHERE clause representing a valid window of events to aggregate in the <code>from_obj</code>: e.g (<code>where {knowledge_date_column} &gt;= {as_of_time} - interval {interval}</code>)</li> <li>Each <code>aggregate</code>, <code>categorical</code>, or <code>array_categorical</code> represents a SELECT clause. For aggregates, the <code>quantity</code> is a column or SQL expression representing a numeric quantity present in the <code>from_obj</code>, and the <code>metrics</code> are any number of aggregate functions we want to use. The aggregate function is applied to the quantity.</li> <li>By default the query is joined with the cohort table to remove unnecessary rows. If <code>features_ignore_cohort</code> is passed to the Experiment this is not done.</li> </ul> <p>So a simplified version of a typical query would look like: <pre><code>SELECT {group}, {metric}({quantity})\nFROM {from_obj}\nJOIN {cohort_table} ON (\n    {cohort_table.entity_id} = {from_obj.entity_id}\n    AND {cohort_table.date} = {as_of_time}\n)\nWHERE {knowledge_date_column} &gt;= {as_of_time} - interval {interval}\nGROUP BY entity_id\n</code></pre></p>"},{"location":"experiments/algorithm/#writing-group-wide-feature-tables","title":"Writing Group-wide Feature Tables","text":"<p>For each <code>as_of_time</code>, the results from the generated query are written to a table whose name is prefixed with the <code>prefix</code>, and suffixed with the <code>group</code> (always <code>entity_id</code> in <code>triage</code>). The table is keyed on this grouping column plus the as_of_date.</p>"},{"location":"experiments/algorithm/#merging-into-aggregation-wide-feature-tables","title":"Merging into Aggregation-wide Feature Tables","text":"<p>Each generated group table is combined into one representing the whole aggregation with a left join to ensure all valid entity/date pairs are included (allowing for identification of nulls requiring imputation). This aggregation-level table represents all of the features in the aggregation, pre-imputation. Its output location is generally <code>{prefix}_aggregation</code></p>"},{"location":"experiments/algorithm/#imputing-values","title":"Imputing Values","text":"<p>A table that looks similar, but with imputed values is created. The cohort table from above is passed into collate as the comprehensive set of entities and dates for which output should be generated, regardless if they exist in the <code>from_obj</code>. Each feature column has an imputation rule, inherited from some level of the feature definition. The imputation rules that are based on data (e.g. <code>mean</code>) use the rows from the <code>as_of_time</code> to produce the imputed value.  In addition, each column that needs imputation has an imputation flag column created, which contains a boolean flagging which rows were imputed or not. Since the values of these columns are redundant for most aggregate functions that look at a given timespan's worth of data (they will be imputed only if zero events in their timespan are seen), only one imputation flag column per timespan is created. An exception to this are some statistical functions that require not one, but two values, like standard deviation and variance. These boolean imputation flags are not merged in with the others. Its output location is generally <code>{prefix}_aggregation_imputed</code></p>"},{"location":"experiments/algorithm/#recap","title":"Recap","text":"<p>At this point, we have at least three tables that are used to populate matrices:</p> <ul> <li><code>labels_{labelname}_{labelqueryhash}</code> with computed labels for each date</li> <li><code>cohort_{cohortname}_{cohortqueryhash}</code> with the cohort for each date</li> <li>A <code>features.{prefix}_aggregation_imputed</code> table for each feature aggregation present in the experiment config.</li> </ul>"},{"location":"experiments/algorithm/#3-building-matrices","title":"3. Building Matrices","text":"<p>At this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g. <code>s3://bucket-name/project_directory</code>)</p> <p>First we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a good start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data, like feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer of iteration we introduce for each split, that may produce many more matrices.</p> <p>What do we iterate over? * Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature group generation and mixing process, which is described more below. * Cohorts - In theory we can take in different cohorts and iterate in the same experiment.  This is not fully implemented, so in reality we just use the one cohort that is passed in the <code>cohort_config</code> * Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same experiment. Right now there is no support for multiple label names, but the label name used is configurable through the optional 'label_config'-&gt;'name' config value * Label types - In theory we can take in different label types (e.g. binary) in the same experiment. Right now this isn't done, there is one label type and it is hardcoded as 'binary'.</p>"},{"location":"experiments/algorithm/#feature-lists","title":"Feature Lists","text":"<p>How do we arrive at the feature lists? There are two pieces of config that are used: <code>feature group_definition</code> and <code>feature_group_strategies</code>. Feature group definitions are just ways to define logical blocks of features, most often features that come from the same source, or describing a particular type of event. These groups within the experiment as a list of feature names, representing some subset of all potential features for the experiment. Feature group strategies are ways to take feature groups and mix them together in various ways. The feature group strategies take these subsets of features and convert them into another list of subsets of features, which is the final list iterated over to create different matrices.</p>"},{"location":"experiments/algorithm/#feature-group-definition","title":"Feature Group Definition","text":"<p>Feature groups, at present, can be defined as either a <code>prefix</code> (the prefix of the feature name), a <code>table</code> (the feature table that the feature resides in), or <code>all</code> (all features).  Each argument is passed as a list, and each entry in the list is interpreted as a group. So, a feature group config of <code>{'table': ['complaints_aggregate_imputed', 'incidents_aggregate_imputed']}</code> would result in two feature groups: one with all the features in <code>complaints_aggregate_imputed</code>, and one with all the features in <code>incidents_aggregate_imputed</code>. Note that this requires a bit of knowledge on the user's part of how the feature table names will be constructed.</p> <p><code>prefix</code> works on the prefix of the feature name as it exists in the database. So this also requires some knowledge of how these get created. The general format is: <code>{aggregation_prefix}_{group}_{timeperiod}_{quantity}</code>, so with some knowledge the user can create groups with the aggregation's configured prefix (common), or the aggregations configured prefix + group (in case they want to compare, for instance, zip-code level features versus entity level features).</p> <p><code>all</code>, with a single value of <code>True</code>, will include a feature group with all defined features. If no feature group definition is sent, this is the default.</p> <p>Either way, at the end of this process the experiment will be aware of some list of feature groups, even if the list is just length 1 with all features as one group.</p>"},{"location":"experiments/algorithm/#feature-group-mixing","title":"Feature Group Mixing","text":"<p>A few basic feature group mixing strategies are implemented: <code>leave-one-in</code>, <code>leave-one-out</code>, and <code>all</code>. These are sent in the experiment definition as a list, so different strategies can be tried in the same experiment. Each included strategy will be applied to the list of feature groups from the previous step, to convert them into</p> <p>For instance, 'leave-one-in' will cycle through each feature group, and for each one create a list of features that just represents that feature group, so for some matrices we would only use features from that particular group. <code>leave-one-out</code> does the opposite, for each feature group creating a list of features that includes all other feature groups but that one. <code>all</code> just creates a list of features that represents all feature groups together.</p>"},{"location":"experiments/algorithm/#iteration-and-matrix-creation","title":"Iteration and Matrix Creation","text":"<p>At this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups, state definitions), grabbing the data corresponding to each from the database, and assembling that data into a design matrix that is saved along with the metadata that defines it.</p> <p>As an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3 feature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18 matrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train and test matrix.</p>"},{"location":"experiments/algorithm/#associating-matrices-with-experiment","title":"Associating Matrices with Experiment","text":"<p>After all matrices for the Experiment are defined but before any are built, the Experiment is associated with each Matrix in the database through the <code>triage_metadata.experiment_matrices</code> table. This means that whether or not the Experiment has to end up building a matrix, after the fact a user can query the database to see if it used said matrix.</p>"},{"location":"experiments/algorithm/#retrieving-data-and-saving-completed-matrix","title":"Retrieving Data and Saving Completed Matrix","text":"<p>Each matrix that has to be built (i.e. has not been built by some prior experiment) is built by retrieving its data out of the database.</p> <p>How do we get the data for an individual matrix out of the database?</p> <ol> <li> <p>Create an entity-date table for this specific matrix. There is some logic applied to decide what rows show up. There are two possible sets of rows that could show up.</p> </li> <li> <p><code>all valid entity dates</code>. These dates come from the entity-date-state table for the experiment (populated using the rules defined in the 'cohort_config'), filtered down to the entity-date pairs that match both the state filter and the list of as-of-dates for this matrix.</p> </li> <li> <p><code>all labeled entity dates</code>. These dates consist of all the valid entity dates from above, that also have an entry in the labels table.</p> </li> </ol> <p>If the matrix is a test matrix, all valid entity dates will be present.</p> <p>If the matrix is a train matrix, whether or not valid but unlabeled examples show up is decided by the <code>include_missing_labels_in_train_as</code> configuration value. If it is present in any form, these labels will be in the matrix. Otherwise, they will be filtered out.</p> <ol> <li> <p>Write features data from tables to disk in CSV format using a COPY command, table by table. Each table is joined with the matrix-specific entity-date table to only include the desired rows.</p> </li> <li> <p>Write labels data to disk in CSV format using a COPY command. These labels will consist of the rows in the matrix-specific entity-date table left joined to the labels table. Rows not present in the labels table will have their label filled in (either True or False) based on the value of the <code>include_missing_labels_in_train_as</code> configuration key.</p> </li> <li> <p>Merge the features and labels CSV files horizontally, in pandas. They are expected to be of the same shape, which is enforced by the entity-date table. The resulting matrix is indexed on <code>entity_id</code> and <code>as_of_date</code>, and then saved to disk (in CSV format, more formats to come) along with its metadata: time, feature, label, index, and state information. along with any user metadata the experiment config specified. The filename is decided by a hash of this metadata, and the metadata is saved in a YAML file with the same hash and directory. The metadata is additionally added to a database table 'matrices'.</p> </li> </ol> <p>Matrix metadata reference: - Train matrix temporal info - Test matrix temporal info - Feature, label, index, cohort, user metadata</p>"},{"location":"experiments/algorithm/#recap_1","title":"Recap","text":"<p>At this point, all finished matrices and metadata will be saved under the <code>project_path</code> supplied by the user to the Experiment constructor, in the subdirectory <code>matrices</code>.</p>"},{"location":"experiments/algorithm/#4-running-models","title":"4. Running Models","text":"<p>The last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'triage_metadata', 'train_results', and 'test_results'.</p>"},{"location":"experiments/algorithm/#associating-models-with-experiment","title":"Associating Models with Experiment","text":"<p>Every combination of training matrix + classifier + hyperparameter is considered a Model. Before any Models are trained, the Experiment is associated with each Model in the database through the <code>triage_metadata.experiment_models</code> table. This means that whether or not the Experiment has to end up training a model, after the fact a user can query the database to see if it used said model.</p>"},{"location":"experiments/algorithm/#train","title":"Train","text":"<p>Each matrix marked for training is sent through the configured grid in the experiment's <code>grid_config</code>. This works much like the scikit-learn <code>ParameterGrid</code> (and in fact uses it on the backend). It cycles through all of the classifiers and hyperparameter combinations contained herein, and calls <code>.fit()</code> with that train matrix. Any classifier that adheres to the scikit-learn <code>.fit/.transform</code> interface and is available in the Python environment will work here, whether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the calling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for comparison).  Metadata about the trained classifier is written to the <code>triage_metadata.models</code> Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below).</p>"},{"location":"experiments/algorithm/#model-groups","title":"Model Groups","text":"<p>Each model is assigned a 'model group'. A model group represents a number of trained classifiers that we want to treat as equivalent by some criteria. By default, this is aimed at defining models which are equivalent across time splits, to make analyzing model stability easier. This default is accomplished with a set of 'model group keys' that includes data about the classifier (module, hyperparameters), temporal intervals used to create the train matrix (label timespan, training history, as-of-date frequency), and metadata describing the data in the train matrix (features and feature groups, label name, cohort name). The user can override this set of <code>model_group_keys</code> in the experiment definition, with all of the default information plus other matrix metadata at their disposal (See end of 'Retrieving Data and Saving Completed Matrix' section for more about matrix metadata). This data is stored in the <code>triage_metadata.model_groups</code> table, along with a <code>model_group_id</code> that is used as a foreign key in the <code>triage_metadata.models</code> table.</p>"},{"location":"experiments/algorithm/#model-hash","title":"Model Hash","text":"<p>Each trained model is assigned a hash, for the purpose of uniquely defining and caching the model. This hash is based on the training matrix metadata, classifier path, hyperparameters (except those which concern execution and do not affect results of the classifier, such as <code>n_jobs</code>), and the given project path for the Experiment. This hash can be found in each row of the <code>triage_metadata.models</code> table. It is enforced as a unique key in the table.</p>"},{"location":"experiments/algorithm/#global-feature-importance","title":"Global Feature Importance","text":"<p>The training phase also writes global feature importances to the database, in the <code>train_results.feature_importances</code> table. A few methods are queried to attempt to compute feature importances: * The bulk of these are computed using the trained model's <code>.feature_importances_</code> attribute, if it exists. * For sklearn's <code>SVC</code> models with a linear kernel, the model's <code>.coef_.squeeze()</code> is used. * For sklearn's LogisticRegression models, <code>np.exp(model.coef_).squeeze()</code> is used. * Otherwise, no feature importances are written.</p>"},{"location":"experiments/algorithm/#test-matrix","title":"Test Matrix","text":"<p>For each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema.</p>"},{"location":"experiments/algorithm/#predictions","title":"Predictions","text":"<p>The trained model's prediction probabilities (<code>predict_proba()</code>) are computed both for the matrix it was trained on and any testing matrices. The predictions for the training matrix are saved in <code>train_results.predictions</code> and those for the testing matrices are saved in the <code>test_results.predictions</code>. More specifically, <code>predict_proba</code> returns the probabilities for each label (false and true), but in this case only the probabilities for the true label are saved in the <code>{train or test}_predictions</code> table. The <code>entity_id</code> and <code>as_of_date</code> are retrieved from the matrix's index, and stored in the database table along with the probability score, label value (if it has one), as well as other metadata.</p>"},{"location":"experiments/algorithm/#individual-feature-importance","title":"Individual Feature Importance","text":"<p>Feature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the <code>test_results.individual_importances</code> table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the <code>individual_importances</code> table.</p>"},{"location":"experiments/algorithm/#metrics","title":"Metrics","text":"<p>Triage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the <code>train_results.evaluations</code> table or the <code>test_results.evaluations</code>. Triage defines a number of Evaluation Metrics metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through.</p> <p>Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken in some way (see next paragraph), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability.</p> <p>A few different versions of tiebreaking are implemented to deal with the nuances of thresholding, and each result is written to the evaluations table for each metric score, along with some related statistics:</p> <ul> <li><code>worst_value</code> - Ordering by the label ascending. This has the effect of as many predicted negatives making it above thresholds as possible, thus producing the worst possible score.</li> <li><code>best_value</code> - Ordering by the label descending. This has the effect of as many predicted positives making it above thresholds as possible, thus producing the best possible score.</li> <li><code>stochastic_value</code> - If the <code>worst_value</code> and <code>best_value</code> are not the same (as defined by the floating point tolerance at catwalk.evaluation.RELATIVE_TOLERANCE), the sorting/thresholding/evaluation will be redone many times, and the mean of all these trials is written to this column. Otherwise, the <code>worst_value</code> is written here</li> <li><code>num_sort_trials</code> - If trials are needed to produce the <code>stochastic_value</code>, the number of trials taken is written here. Otherwise this will be 0</li> <li><code>standard_deviation</code> - If trials are needed to produce the <code>stochastic_value</code>, the standard deviation of these trials is written here. Otherwise this will be 0</li> </ul> <p>Sometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score.</p> <ul> <li><code>num_labeled_examples</code> - The number of rows in the test matrix that have labels</li> <li><code>num_labeled_above_threshold</code> - The number of rows above the configured threshold for this metric score that have labels</li> <li><code>num_positive_labels</code> - The number of positive labels in the test matrix</li> </ul> <p>Triage supports performing a bias audit using the Aequitas library, if a <code>bias_audit_config</code> is passed in configuration. This is handled first through creating a 'protected groups'table which retrieves the configured protected group information for each member of the cohort, and the time that this protected group information was first known. This table is named using a hash of the bias audit configuration, so data can be reused across experiments as long as the bias configuration does not change.</p> <p>A bias audit is performed alongside metric calculation time for each model that is built, on both the train and test matrices, and each subset. This is very similar to the evaluations table schema, in that for each slice of data that has evaluation metrics generated for it, also receives a bias audit. The change is that thresholds are not borrowed from the evaluation configuration, as aequitas audits are computationally expensive and large threshold grids are common in Triage experiments; the bias audit has its evaluation thresholds configured in the <code>bias_audit_config</code>. All data from the bias audit is saved to either the <code>train_results.aequitas</code> or <code>test_results.aequitas</code> tables.</p> <p>Triage also supports evaluating a model on a subset of the predictions made. This is done by passing a subset query in the prediction config. The model evaluator will then subset the predictions on valid entity-date pairs for the given model and will calculate metrics for the subset, re-applying thresholds as necessary to the predictions in the subset. Subset definitions are stored in the <code>triage_metadata.subsets</code> table, and the evaluations are stored in the <code>evaluations</code> tables. A hash of the subset configuration identifies subset evaluations and links the <code>subsets</code> table.</p>"},{"location":"experiments/algorithm/#recap_2","title":"Recap","text":"<p>At this point, the 'triage_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.</p>"},{"location":"experiments/architecture/","title":"Experiment Architecture","text":"<p>This document is aimed at people wishing to contribute to Triage development. It explains the design and architecture of the Experiment class.</p>"},{"location":"experiments/architecture/#dependency-graphs","title":"Dependency Graphs","text":"<p>For a general overview of how the parts of an experiment depend on each other, refer to the graphs below. </p>"},{"location":"experiments/architecture/#experiment-high-level","title":"Experiment (high-level)","text":"graph TD     TC[Timechop]     subgraph Architect     LG[Label Generator]     EDG[Entity-Date Generator]     FG[\"Feature Generator (+ feature groups)\"]     MB[Matrix Builder]     end     subgraph Catwalk, per-model     MT[Model Trainer]     PR[Predictor]     PG[Protected Group Generator]     EV[Model Evaluator]     end     TC --&gt; LG     TC --&gt; EDG     TC --&gt; FG     LG --&gt; MB     EDG --&gt; MB     FG --&gt; MB     MB --&gt; MT     MB --&gt; PR     MT --&gt; PR     EDG --&gt; PG     PG --&gt; EV     PR --&gt; EV  <p>The FeatureGenerator section above hides some details to make the overall graph flow more concise. To support feature grouping, there are more operations that happen between feature table creation and matrix building. The relevant section of the dependency graph is expanded below, along with the output that each pair of components sends between each other within the arrow</p>"},{"location":"experiments/architecture/#feature-dependency-details","title":"Feature Dependency Details","text":"graph TD         TC[Timechop]         FG[Feature Generator]         FDG[Feature Dictionary Generator]         FGC[Feature Group Creator]         FGM[Feature Group Mixer]         PL[Planner]         MB[Matrix Builder]         TC -- as-of-dates --&gt; FG         FG -- feature tables --&gt; FDG         FDG -- master feature dictionary --&gt; FGC         FGC -- feature groups --&gt; FGM         FGM -- recombined feature groups --&gt; PL         TC -- time splits --&gt; PL         FG -- feature tables --&gt; MB         PL -- matrix build tasks --&gt; MB"},{"location":"experiments/architecture/#component-list-and-inputoutput","title":"Component List and Input/Output","text":"<p>These are where the interesting data science work is done.</p> <ul> <li>Timechop (temporal cross-validation)</li> <li>Architect (design matrix creation)<ul> <li>Entity-Date_ Table Generator</li> <li>Label Generator</li> <li>Feature Generator</li> <li>Feature Dictionary Creator</li> <li>Feature Group Creator</li> <li>Feature Group Mixer</li> <li>Planner</li> <li>Matrix Builder</li> </ul> </li> <li>Catwalk (modeling)<ul> <li>Model Train/Tester</li> <li>Model Grouper</li> <li>Model Trainer</li> <li>Predictor</li> <li>Protected Group Table Generator</li> <li>Model Evaluator</li> <li>Individual Importance Calculator</li> </ul> </li> </ul>"},{"location":"experiments/architecture/#timechop","title":"Timechop","text":"<p>Timechop does the necessary temporal math to set up temporal cross-validation. It 'chops' time according to config into train-test split definitions, which other components use.</p> <p>Input</p> <ul> <li><code>temporal_config</code> in experiment config</li> </ul> <p>Output</p> <ul> <li>Time splits containing temporal cross-validation definition, including each <code>as_of_date</code> to be included in the matrices in each time split</li> </ul>"},{"location":"experiments/architecture/#entity-date-table-generator","title":"Entity-Date Table Generator","text":"<p>The <code>EntityDateTableGenerator</code> manages entity-date tables (including cohort and subset tables) by running the configured query for a number of different <code>as_of_dates</code>. Alternately, will retrieve all unique entities and dates from the labels table if no query is configured.</p> <p>Input</p> <ul> <li>All unique <code>as_of_dates</code> needed by matrices in the experiment, as provided by Timechop</li> <li>query and name from <code>cohort_config</code> or <code>subsets</code> in the <code>scoring</code> section in an experiment config</li> <li>entity-date table name that the caller wants to use</li> </ul> <p>Output</p> <ul> <li>An entity-date table in the database, consisting of entity ids and dates</li> </ul>"},{"location":"experiments/architecture/#label-generator","title":"Label Generator","text":"<p>The <code>LabelGenerator</code> manages a labels table by running the configured label query for a number of different <code>as_of_dates</code> and <code>label_timespans</code>.</p> <p>Input</p> <ul> <li>All unique <code>as_of_dates</code> and <code>label_timespans</code>, needed by matrices in the experiment, as provided by Timechop</li> <li>query and name from <code>label_config</code> in experiment config</li> </ul> <p>Output</p> <ul> <li>A labels table in the database, consisting of entity ids, dates, and boolean labels</li> </ul>"},{"location":"experiments/architecture/#feature-generator","title":"Feature Generator","text":"<p>The <code>FeatureGenerator</code> manages a number of features tables by converting the configured <code>feature_aggregations</code> into <code>collate.Spacetime</code> objects, and then running the queries generated by <code>collate</code>. For each <code>feature_aggregation</code>, it runs a few passes:</p> <ol> <li>Optionally, convert a complex from object (e.g. the <code>FROM</code> part of the configured aggregation query) into an indexed table for speed.</li> <li>Create a number of empty tables at different <code>GROUP BY</code> levels (always <code>entity_id</code> in <code>triage</code>) and run inserts individually for each <code>as_of_date</code>. These inserts are split up into individual tasks and parallelized for speed.</li> <li>Roll up the <code>GROUP BY</code> tables from step 1 to the <code>entity_id</code> level with a single <code>LEFT JOIN</code> query.</li> <li>Use the cohort table to find all members of the cohort not present in the table from step 2 and create a new table with all members of the cohort, null values filled in with values based on the rules in the <code>feature_aggregations</code> config.</li> </ol> <p>Input</p> <ul> <li>All unique <code>as_of_dates</code> needed by matrices in the experiment, and the start time for features, as provided by Timechop</li> <li>The populated cohort table, as provided by Entity-Date Table Generator</li> <li><code>feature_aggregations</code> in experiment config</li> </ul> <p>Output</p> <ul> <li>Populated feature tables in the database, one for each <code>feature_aggregation</code></li> </ul>"},{"location":"experiments/architecture/#feature-dictionary-creator","title":"Feature Dictionary Creator","text":"<p>Summarizes the feature tables created by FeatureGenerator into a dictionary more easily usable for feature grouping and serialization purposes. Does this by querying the database's <code>information_schema</code>.</p> <p>Input</p> <ul> <li>Names of feature tables and the index of each table, as provided by Feature Generator</li> </ul> <p>Output</p> <ul> <li>A master feature dictionary, consisting of each populated feature table and all of its feature column names.</li> </ul>"},{"location":"experiments/architecture/#feature-group-creator","title":"Feature Group Creator","text":"<p>Creates feature groups by taking the configured feature grouping rules and applying them to the master feature dictionary, to create a collection of smaller feature dictionaries.</p> <p>Input</p> <ul> <li>Master feature dictionary, as provided by Feature Dictionary Creator</li> <li><code>feature_group_definition</code> in experiment config</li> </ul> <p>Output</p> <ul> <li>List of feature dictionaries, each representing one feature group</li> </ul>"},{"location":"experiments/architecture/#feature-group-mixer","title":"Feature Group Mixer","text":"<p>Combines feature groups into new ones based on the configured rules (e.g. <code>leave-one-out</code>, <code>leave-one-in</code>).</p> <p>Input</p> <ul> <li>List of feature dictionaries, as provided by Feature Group Creator</li> <li><code>feature_group_strategies</code> in experiment config</li> </ul> <p>Output</p> <ul> <li>List of feature dictionaries, each representing one or more feature groups.</li> </ul>"},{"location":"experiments/architecture/#planner","title":"Planner","text":"<p>Mixes time split definitions and feature groups to create the master list of matrices that are required for modeling to proceed.</p> <p>Input</p> <ul> <li>List of feature dictionaries, as provided by Feature Group Mixer</li> <li>List of matrix split definitions, as provided by Timechop</li> <li><code>user_metadata</code>, in experiment config</li> <li><code>feature_start_time</code> from <code>temporal_config</code> in experiment config</li> <li>cohort name from <code>cohort_config</code> in experiment config</li> <li>label name from <code>cohort_config</code> in experiment config</li> </ul> <p>Output</p> <ul> <li>List of serializable matrix build tasks, consisting of everything needed to build a single matrix:<ul> <li>list of as-of-dates</li> <li>a label name</li> <li>a label type</li> <li>a feature dictionary</li> <li>matrix uuid</li> <li>matrix metadata</li> <li>matrix type (train or test)</li> </ul> </li> </ul>"},{"location":"experiments/architecture/#matrix-builder","title":"Matrix Builder","text":"<p>Takes matrix build tasks from the Planner and builds them if they don't already exist.</p> <p>Input</p> <ul> <li>A matrix build task, as provided by Planner</li> <li><code>include_missing_labels_in_train_as</code> from <code>label_config</code> in experiment config</li> <li>The experiment's MatrixStorageEngine</li> </ul> <p>Output</p> <ul> <li>The built matrix saved in the MatrixStorageEngine</li> <li>A row describing the matrix saved in the database's <code>triage_metadata.matrices</code> table.</li> </ul>"},{"location":"experiments/architecture/#modeltraintester","title":"ModelTrainTester","text":"<p>A meta-component of sorts. Encompasses all of the other catwalk components.</p> <p>Input</p> <ul> <li>One temporal split, as provided by Timechop</li> <li><code>grid_config</code> in experiment config</li> <li>Fully configured ModelTrainer, Predictor, ModelEvaluator, Individual Importance Calculator objects</li> </ul> <p>Output</p> <ul> <li>All of its components are run, resulting in trained models, predictions, evaluation metrics, and individual importances</li> </ul>"},{"location":"experiments/architecture/#modelgrouper","title":"ModelGrouper","text":"<p>Assigns a <code>model group</code> to each model based on its metadata.</p> <p>Input</p> <ul> <li><code>model_group_keys</code> in experiment config</li> <li>All the data about a particular model neded to decide a model group for the model: classifier name, hyperparameter list, and matrix metadata, as provided by ModelTrainer</li> </ul> <p>Output</p> <ul> <li>a model group id corresponding to a row in the <code>triage_metadata.model_groups</code> table, either a matching one that already existed in the table or one that it autoprovisioned.</li> </ul>"},{"location":"experiments/architecture/#modeltrainer","title":"ModelTrainer","text":"<p>Trains a model, stores it, and saves its metadata (including model group information and feature importances) to the database. Each model to be trained is expressed as a serializable task so that it can be parallelized.</p> <p>Input</p> <ul> <li>an instance of the ModelGrouper class.</li> <li>the experiment's ModelStorageEngine</li> <li>a MatrixStore object</li> <li>an importable classifier path and a set of hyperparameters</li> </ul> <p>Output - a row in the database's <code>triage_metadata.model_groups</code> table, the <code>triage_metadata.models</code> table, and rows in <code>train_results.feature_importances</code> for each feature. - the trained model persisted in the ModelStorageEngine</p>"},{"location":"experiments/architecture/#predictor","title":"Predictor","text":"<p>Generates predictions for a given model and matrix, both returning them for immediate use and saving them to the database.</p> <p>Input</p> <ul> <li>The experiment's Model Storage Engine</li> <li>A model id corresponding to a row from the database</li> <li>A MatrixStore object</li> </ul> <p>Output</p> <ul> <li>The predictions as an array</li> <li>Each prediction saved to the database, unless configured not to. The table they are stored in depends on which type of matrix it is (e.g. <code>test_results.predictions</code> or <code>train_results.predictions</code>)</li> </ul>"},{"location":"experiments/architecture/#protected-group-table-generator","title":"Protected Group Table Generator","text":"<p>Generates a table containing protected group attributes (e.g. race, sex, age).</p> <p>Input</p> <ul> <li>A cohort table name and its configuration's unique hash</li> <li>Bias audit configuration, specifically a from object (either a table or query), and column names in the from object for protected attributes, knowledge date, and entity id.</li> <li>A name for the protected groups table</li> </ul> <p>Output</p> <ul> <li>A protected groups table, containing all rows from the cohort and any protected group information present in the from object, as well as the cohort hash so multiple cohorts can live in the same table.</li> </ul>"},{"location":"experiments/architecture/#modelevaluator","title":"ModelEvaluator","text":"<p>Generates evaluation metrics for a given model and matrix over the entire matrix and for any subsets.</p> <p>Input</p> <ul> <li><code>scoring</code> in experiment config</li> <li>array of predictions</li> <li>the MatrixStore and model_id that the predictions were generated from</li> <li>the subset to be evaluated (or <code>None</code> for the whole matrix)</li> <li>the reference group and thresholding rules from <code>bias_audit_config</code> in experiment config</li> <li>the protected group generator object (for retrieving protected group data)</li> </ul> <p>Output</p> <ul> <li>A row in the database for each evaluation metric for each subset. The table they are stored in depends on which type of matrix it is (e.g. <code>test_results.evaluations</code> or <code>train_results.evaluations</code>).</li> <li>A row in the database for each Aequitas bias report. Either <code>test_results.aequitas</code> or <code>train_results.aequitas</code>.</li> </ul>"},{"location":"experiments/architecture/#individual-importance-calculator","title":"Individual Importance Calculator","text":"<p>Generates the top <code>n</code> feature importances for each entity in a given model.</p> <p>Input</p> <ul> <li><code>individual_importance_config</code> in experiment config.</li> <li>model id</li> <li>a MatrixStore object for a test matrix</li> <li>an as-of-date</li> </ul> <p>Output</p> <ul> <li>rows in the <code>test_results.individual_importances</code> table for the model, date, and matrix based on the configured method and number of top features per entity.</li> </ul>"},{"location":"experiments/architecture/#general-class-design","title":"General Class Design","text":"<p>The Experiment class is designed to have all work done by component objects that reside as attributes on the instance. The purpose of this is to maximize the reuse potential of the components outside of the Experiment, as well as avoid excessive class inheritance within the Experiment.</p> <p>The inheritance tree of the Experiment is reserved for execution concerns, such as switching between singlethreaded, multiprocess, or cluster execution. To enable these different execution contexts without excessive duplicated code, the components that cover computationally or memory-intensive work generally implement methods to generate a collection of serializable <code>tasks</code> to perform later, on either that same object or perhaps another one running in another process or machine.  The subclasses of Experiment then differentiate themselves by implementing methods to execute a collection of these <code>tasks</code> using their preferred method of execution, whether it be a simple loop, a process pool, or a cluster.</p> <p>The components are created and experiment configuration is bound to them at Experiment construction time, so that the instance methods can have concise call signatures that only cover the information passed by other components mid-experiment.</p> <p>Data reuse/replacement is handled within components. The Experiment generally just hands the <code>replace</code> flag to each component at object construction, and at runtime each component uses that and determines whether or not the needed work has already been done.</p>"},{"location":"experiments/architecture/#im-trying-to-find-some-behavior-where-does-it-reside","title":"I'm trying to find some behavior. Where does it reside?","text":"<p>If you're looking to change behavior of the Experiment,</p> <ul> <li>When possible, the logic resides in one of the components and hopefully the component list above should be helpful at finding the lines between components.</li> <li>Logic that specifically relates to parallel execution is in one of the experiment subclasses (see parallelization section below).</li> <li>Everything else is in the Experiment base class. This is where the public interface (<code>.run()</code>) resides, and follows a template method pattern to define the skeleton of the Experiment: instantating components based on experiment configuration and runtime inputs, and passing output from one component to another.</li> </ul>"},{"location":"experiments/architecture/#i-want-to-add-a-new-option-where-should-i-put-it","title":"I want to add a new option. Where should I put it?","text":"<p>Generally, the experiment configuration is where any new options go that change any data science-related functionality; in other words, if you could conceivably get better precision from the change, it should make it into experiment configuration. This is so the hashed experiment config is meaningful and the experiment can be audited by looking at the experiment configuration rather than requiring the perusal of custom code. The blind spot in this is, of course, the state of the database, which can always change results, but it's useful for database state to continue to be the only exception to this rule.</p> <p>On the other hand, new options that affect only runtime concerns (e.g. performance boosts) should go as arguments to the Experiment. For instance, changing the number of cores to use for matrix building, or telling it to skip predictions won't change the answer you're looking for; options like these just help you potentially get to the answer faster.  Once an experiment is completed, runtime flags like these should be totally safe to ignore in analysis.</p>"},{"location":"experiments/architecture/#storage-abstractions","title":"Storage Abstractions","text":"<p>Another important part of enabling different execution contexts is being able to pass large, persisted objects (e.g. matrices or models) by reference to another process or cluster. To achieve this, as well as provide the ability to configure different storage mediums (e.g. S3) and formats (e,g, HDF) without changes to the Experiment class, all references to these large objects within any components are handled through an abstraction layer.</p>"},{"location":"experiments/architecture/#matrix-storage","title":"Matrix Storage","text":"<p>All interactions with individual matrices and their bundled metadata are handled through <code>MatrixStore</code> objects.  The storage medium is handled through a base <code>Store</code> object that is an attribute of the <code>MatrixStore</code>. The storage format is handled through inheritance on the <code>MatrixStore</code>: Each subclass, such as <code>CSVMatrixStore</code> or <code>HDFMatrixStore</code>, implements the necessary methods (<code>save</code>, <code>load</code>, <code>head_of_matrix</code>) to properly persist or load a matrix from its storage.</p> <p>In addition, the <code>MatrixStore</code> provides a variety of methods to retrieve data from either the base matrix itself or its metadata. For instance (this is not meant to be a complete list):</p> <ul> <li><code>matrix</code> - the raw matrix</li> <li><code>metadata</code> - the raw metadata dictionary</li> <li><code>exists</code> - whether or not it exists in storage</li> <li><code>columns</code> - the column list</li> <li><code>labels</code> - the label column</li> <li><code>uuid</code> - the matrix's UUID</li> <li><code>as_of_dates</code> - the matrix's list of as-of-dates</li> </ul> <p>One <code>MatrixStorageEngine</code> exists at the Experiment level, and roughly corresponds with a directory wherever matrices are stored. Its only interface is to provide a <code>MatrixStore</code> object given a matrix UUID.</p>"},{"location":"experiments/architecture/#model-storage","title":"Model Storage","text":"<p>Model storage is handled similarly to matrix storage, although the interactions with it are far simpler so there is no single-model class akin to the <code>MatrixStore</code>. One <code>ModelStorageEngine</code> exists at the Experiment level, configured with the Experiment's storage medium, and through it trained models can be saved or loaded. The <code>ModelStorageEngine</code> uses joblib to save and load compressed pickles of the model.</p>"},{"location":"experiments/architecture/#miscellaneous-project-storage","title":"Miscellaneous Project Storage","text":"<p>Both the <code>ModelStorageEngine</code> and <code>MatrixStorageEngine</code> are based on a more general storage abstraction that is suitable for any other auxiliary objects (e.g. graph images) that need to be stored. That is the <code>ProjectStorage</code> object, which roughly corresponds to a directory on some storage medium where we store everything. One of these exists as an Experiment attribute, and its interface <code>.get_store</code> can be used to persist or load whatever is needed.</p>"},{"location":"experiments/architecture/#parallelizationsubclassing-details","title":"Parallelization/Subclassing Details","text":"<p>In the Class Design section above, we introduced tasks for parallelization and subclassing for execution changes. In this section, we expand on these to help provide a new guide to working with these.</p> <p>Currently there are three methods that must be implemented by subclasses of Experiment in order to be fully functional.</p>"},{"location":"experiments/architecture/#abstract-methods","title":"Abstract Methods","text":"<ul> <li><code>process_query_tasks</code> - Run feature generation queries. Receives a list of tasks. each <code>task</code> actually represents a table and is split into three lists of queries to enable the implementation to avoid deadlocks: <code>prepare</code> (table creation), <code>inserts</code> (a collection of INSERT INTO SELECT queries), and <code>finalize</code> (indexing). <code>prepare</code> needs to be run before the inserts and <code>finalize</code> is best run after the inserts, so it is advised that only the inserts are parallelized. The subclass should run each individual batch of queries by calling <code>self.feature_generator.run_commands([list of queries])</code>, which will run all of the queries serially, so the implementation can send a batch of queries to each worker instead of having each individual query be on a new worker. </li> <li><code>process_matrix_build_tasks</code> - Run matrix build tasks (that assume all the necessary label/cohort/feature tables have been built). Receives a dictionary of tasks. Each key is a matrix UUID, and each value is a dictionary that has all the necessary keyword arguments to call <code>self.matrix_builder.build_matrix</code> to build one matrix.</li> <li><code>process_train_test_batches</code> - Run model train/test task batches (that assume all matrices are built). Receives a list of <code>triage.component.catwalk.TaskBatch</code> objects, each of which has a list of tasks, a description of those tasks, and whether or not that batch is safe to run in parallel. Within this, each task is a dictionary that has all the necessary keyword arguments to call <code>self.model_train_tester.process_task</code> to train and test one model. Each task covers model training, prediction (on both test and train matrices), model evaluation (on both test and train matrices), and saving of global and individual feature importances.</li> </ul>"},{"location":"experiments/architecture/#reference-implementations","title":"Reference Implementations","text":"<ul> <li>SingleThreadedExperiment is a barebones implementation that runs everything serially.</li> <li>MultiCoreExperiment utilizes local multiprocessing to run tasks through a worker pool. Reading this is helpful to see the minimal implementation needed for some parallelization.</li> <li>RQExperiment - utilizes an RQ worker cluster to allow the tasks to be parallelized either locally or distributed to other. Does not take care of spawning a cluster or any other infrastructural concerns: it expects that the cluster is running somewhere and is reading from the same Redis instance that is passed to the <code>RQExperiment</code>. The <code>RQExperiment</code> simply enqueues tasks and waits for them to be completed. Reading this is helpful as a simple example of how to enable distributed computing.</li> </ul>"},{"location":"experiments/cohort-labels/","title":"Cohort and Label Deep Dive","text":"<p>This document is intended at providing a deep dive into the concepts of cohorts and labels as they apply to Triage. For context, reading the Triage section of the Dirty Duck tutorial may be helpful before reading this document.</p>"},{"location":"experiments/cohort-labels/#temporal-validation-refresher","title":"Temporal Validation Refresher","text":"<p>Triage uses temporal validation to select models because the real-world problems that Triage is built for tend to evolve or change over time.  Picking a date range to train on and a date range afterwards to test on ensures that we don't leak data from the future into our models that wouldn't be available in a real-world deployment scenario. Because of this, we often talk in Triage about the as-of-date: all models trained by Triage are associated with an as-of-date, which means that all the data that goes into the model is only included if it was known about before that date. The matrix used to train the model may have multiple as-of-dates, and the most recent is referred to as the model's as-of-date so it's easy to see the cutoff date for data included in the model. For more on temporal validation, see the relevant section in Dirty Duck.</p>"},{"location":"experiments/cohort-labels/#what-are-cohorts-and-labels-in-triage","title":"What are Cohorts and Labels in Triage?","text":"<p>This document assumes that the reader is familiar with the concept of a machine learning target variable and will focus on explaining what is unique to Triage.</p> <p>A cohort is the population used used for modeling on a given as-of-date. This is expressed as a list of entities. An entity is simply the object of prediction, such as a facility to inspect or a patient coming in for a visit. Early warning systems tend to include their entire population (or at least a large subset of it) in the cohort at any given date, while appointment-based problems may only include in a date's cohort the people who are scheduled for an appointment on that date.</p> <p>A label is the binary target variable for a member of the cohort at a given as-of-date and a given label timespan. For instance, in an inspection prioritization problem the question being asked may be 'what facilities are at high risk of having a failed inspection in the next 6 months?' For this problem, the <code>label_timespan</code> is 6 months. There may be multiple label timespans tested in the same experiment, in which case there could be multiple labels for an entity and date. In addition, multiple label definitions are often tested against each other, such as \"any inspection failures\" vs \"inspection failures with serious issues\".</p> <p>Both labels and cohorts are defined in Triage's experiment configuration using SQL queries, with the variables (<code>as_of_date</code>, <code>label_timespan</code>) given as placeholders. This allows the definitions to be given in a concise manner while allowing the temporal configuration defined elsewhere in the experiment to produce the actual list of dates and timespans that are calculated during the experiment.</p>"},{"location":"experiments/cohort-labels/#cohort-definition-and-examples","title":"Cohort Definition and Examples","text":"<p>The cohort is configured with a query that returns a unique list of <code>entity_id</code>s given an <code>as_of_date</code>, and it runs the query for each <code>as_of_date</code> that is produced by your temporal config. You tell Triage where to place each <code>as_of_date</code> with a placeholder surrounded by brackets: <code>{as_of_date}</code>.</p>"},{"location":"experiments/cohort-labels/#note-1","title":"Note 1","text":"<p>The as_of_date is parsed as a timestamp in the database, which Postgres defaults to midnight at the beginning of the date in question. It's important to consider how this is used for feature generation. Features are only included if they are known about before this timestamp. So features will be only included for an as_of_date if they are known about before that as_of_date. If you want to work around this (e.g for visit-level problems in which you want to intake data on the day of the visit and make predictions using that data the same day), you can move your cohort up a day. The time splitting in Triage is designed for day granularity so approaches to train up to a specific hour and test at another hour of the same day are not supported.</p>"},{"location":"experiments/cohort-labels/#note-2","title":"Note 2","text":"<p>Triage expects all entity ids to be integers.</p>"},{"location":"experiments/cohort-labels/#note-3","title":"Note 3","text":"<p>Triage expects the cohort to be a unique list of entity ids. Throughout the cohort example queries you will see <code>distinct(entity_id)</code> used to ensure this.</p>"},{"location":"experiments/cohort-labels/#example-inspections","title":"Example: Inspections","text":"<p>Let's say I am prioritizing the inspection of food service facilities such as restaurants, caterers or grocery stores. One simple definition of a cohort for facility inspection would be to include any facilities that have active permits in the last year in the cohort. Assume that these permits are contained in a table, named <code>permits</code>, with the facility's id, a start date, and an end date of the permit.</p>"},{"location":"experiments/cohort-labels/#inspections-cohort-source-table","title":"Inspections Cohort Source Table","text":"entity_id start_date end_date 25 2016-01-01 2016-02-01 44 2016-01-01 2016-02-01 25 2016-02-01 2016-03-01 <p>Triage expects the cohort query passed to it to return a unique list of <code>entity_id</code>s given an <code>as_of_date</code>, and it runs the query for each <code>as_of_date</code> that is produced by your temporal config. You tell Triage where to place each <code>as_of_date</code> with a placeholder surrounded by brackets: <code>{as_of_date}</code>. An example query that implements the 'past year' definition would be:</p> <p><code>select distinct(entity_id) from permits where tsrange(start_date, end_date, '[]') @&gt; {as_of_date}</code></p> <ul> <li>Running this query using the <code>as_of_date</code> '2017-01-15' would return both entity ids 25 and 44.</li> <li>Running it with '2017-02-15' would return only entity id 25.</li> <li>Running it with '2017-03-15' would return no rows.</li> </ul>"},{"location":"experiments/cohort-labels/#inspections-cohort-config","title":"Inspections Cohort Config","text":"<p>We recommend storing your cohort queries as files in their own directory. This allows you to more easily reuse and update cohorts in multiple experiments. The way this looks in an Experiment configuration YAML is as follows:</p> <pre><code>cohort_config:\n  filepath: 'cohorts/permits_in_last_year.sql'\n  name: 'permits_in_last_year'\n</code></pre> <p>The <code>name</code> key is optional. Part of its purpose is to help you organize different cohorts in your configuration, but it is also included in each matrix's metadata file to help you keep them straight afterwards.</p> <p>In the <code>cohorts</code> directory of your project, you would then have the <code>permits_in_last_year.sql</code> file, which would look like:</p> <pre><code>select distinct(entity_id)\nfrom permits\nwhere tsrange(start_time, end_time, '[]') @&gt; {as_of_date}\n</code></pre> <p>Alternatively, you can include the cohort query in the configuration directly like so:</p> <pre><code>cohort_config:\n  query: |\n    select distinct(entity_id)\n    from permits\n    where tsrange(start_time, end_time, '[]') @&gt; {as_of_date}\n  name: 'permits_in_last_year'\n</code></pre> <p>This is not recommended (and may not be supported in future versions of triage) because it makes keeping cohorts consistent between experiment configs more difficult and error prone. It is permitted largely to support legacy projects that wish to upgrade triage.</p>"},{"location":"experiments/cohort-labels/#example-early-intervention","title":"Example: Early Intervention","text":"<p>An example of an early intervention system is identifying people at risk of recidivism so they can receive extra support to encourage positive outcomes.</p> <p>This example defines the cohort as everybody who has been released from jail within the last three years. It does this by querying an events table for events of type 'release'.</p>"},{"location":"experiments/cohort-labels/#early-intervention-cohort-source-table","title":"Early Intervention Cohort Source Table","text":"entity_id event_type knowledge_date 25 booking 2016-02-01 44 booking 2016-02-01 25 release 2016-03-01"},{"location":"experiments/cohort-labels/#early-intervention-cohort-config","title":"Early Intervention Cohort Config","text":"<pre><code>cohort_config:\n    filepath: 'cohorts/booking_last_3_years.sql'\n    name: 'booking_last_3_years'\n</code></pre> <p>In the <code>cohorts/booking_last_3_years.sql</code> file:</p> <pre><code>SELECT distinct(entity_id)\n  FROM events\n WHERE event_type = 'release'\n   AND knowledge_date &lt;@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date)\n</code></pre>"},{"location":"experiments/cohort-labels/#example-visits","title":"Example: Visits","text":"<p>Another problem type we may want to model is visit/appointment level modeling. An example would be a health clinic that wants to figure out which patients on a given day who are most at risk for developing diabetes within some time period but don't currently have it.</p>"},{"location":"experiments/cohort-labels/#visits-cohort-source-tables","title":"Visits Cohort Source Tables","text":"<p>Here we actually define two tables: an appointments table that contains the appointment schedule, and a diabetes diagnoses table that contains positive diabetes diagnoses.</p> <p><code>appointments</code></p> entity_id appointment_date 25 2016-02-01 44 2016-02-01 25 2016-03-01 <p><code>diabetes_diagnoses</code></p> entity_id diagnosis_date 44 2015-02-01 86 2012-06-01"},{"location":"experiments/cohort-labels/#visits-cohort-config","title":"Visits Cohort Config","text":"<p>The cohort config here queries the visits table for the next day, and excludes those who have a diabetes diagnosis at some point in the past. There's a twist: a day is subtracted from the as-of-date. Why? We may be collecting useful data during the appointment about whether or not they will develop diabetes, and we may want to use this data as features. Because the as-of-date refers to the timestamp at the beginning of the day (see note 1), if the as-of-date and appointment date match up exactly we won't be able to use those features. So, appointments show up in the next day's as-of-date.</p> <p>Whether or not this is correct depends on the feasability of generating a prediction during the visit to use this data, which depends on the deployment plans for the system.  If data entry and prediction can only happen nightly, you can't expect to use data from the visit in features and would change the as-of-date to match the appointment_date.</p> <pre><code>cohort_config:\n  filepath: 'cohorts/visit_day_no_previous_diabetes.sql'\n  name: 'visit_day_no_previous_diabetes'\n</code></pre> <p>In the <code>cohorts/visit_day_no_previous_diabetes.sql</code> file:</p> <pre><code>select distinct(entity_id)\nfrom appointments\nwhere appointment_date = ('{as_of_date}'::date - interval '1 days')::date\n  and not exists(\n      select entity_id\n      from diabetes_diagnoses\n      where entity_id = appointments.entity_id\n        and as_of_date &lt; '{as_of_date}'\n      group by entity_id)\ngroup by entity_id\n</code></pre>"},{"location":"experiments/cohort-labels/#testing-cohort-configuration","title":"Testing Cohort Configuration","text":"<p>If you want to test out a cohort query without running an entire experiment, there are a few ways, and the easiest way depends on how much of the rest of the experiment you have configured.</p> <p>Option 1: You have not started writing an experiment config file yet. If you just want to test your query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the <code>EntityDateTableGenerator</code> with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config:</p> <pre><code>from triage.component.architect.entity_date_table_generators import EntityDateTableGenerator\nfrom triage import create_engine\nfrom datetime import datetime\n\nEntityDateTableGenerator(\n    query=\"select entity_id from permits where tsrange(start_time, end_time, '[]') @&gt; {as_of_date}\",\n    db_engine=create_engine(...),\n    entity_date_table_name=\"my_test_cohort_table\"\n).generate_entity_date_table([datetime(2016, 1, 1), datetime(2016, 2, 1), datetime(2016, 3, 1)])\n</code></pre> <p>Running this will generate a table with the name you gave it (<code>my_test_cohort_table</code>), populated with the cohort for that list of dates. You can inspect this table in your SQL browser of choice.</p> <p>Option 2: You have an experiment config file that includes temporal config, and want to look at the cohort in isolation in the database. If you want to actually create the cohort for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the cohort.</p> <pre><code>from triage.experiments import SingleThreadedExperiment\nfrom triage import create_engine\nimport yaml\n\nwith open('&lt;your_experiment_config.yaml&gt;') as fd:\n    experiment_config = yaml.full_load(fd)\nexperiment = SingleThreadedExperiment(\n    experiment_config=experiment_config,\n    db_engine=create_engine(...),\n    project_path='./'\n)\nexperiment.generate_cohort()\nprint(experiment.cohort_table_name)\n</code></pre> <p>This will generate the entire cohort needed for your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the <code>cohort_table_name</code> attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice.</p> <p>These options should be enough to test your cohort in isolation. How the cohort shows up in matrices is also dependent on its interaction with the labels, and later we'll show how to test that.</p>"},{"location":"experiments/cohort-labels/#label-definition-and-examples","title":"Label Definition and Examples","text":"<p>The labels table works similarly to the cohort table: you give it a filepath pointing to a SQL query with a placeholder for an as-of-date. However, the label query has one more dependency: a <code>label timespan</code> For instance, if you are inspecting buildings for violations, a label timespan of 6 months translates into a label of 'will this building have a violation in the next 6 months?'.  These label timespans are generated by your temporal configuration as well and you may have multiple in a single experiment, so what you send Triage in your label query is also a placeholder.</p> <p>Note: The label query is expected by Triage to return only one row per entity id for a given as-of-date/label timespan combination.</p>"},{"location":"experiments/cohort-labels/#missing-labels","title":"Missing Labels","text":"<p>Since the cohort has its own definition query, separate from the label query, we have to consider the possibility that not every entity in the cohort is present in the label query, and how to deal with these missing labels.  The label value in the train matrix in these cases is controlled by a flag in the label config: <code>include_missing_labels_in_train_as</code>. </p> <ul> <li>If you omit the flag, they show up as missing. This is common for inspections problems, wherein you really don't know a suitable label. The facility wasn't inspected, so you really don't know what the label is. This makes evaluation a bit more complicated, as some of the facilities with high risk scores may have no labels. But this is a common tradeoff in inspections problems.</li> <li>If you set it to True, that means that all of the rows have positive label. What does this mean? It depends on what exactly your label query is, but a common use would be to model early warning problems of dropouts, in which the absence of an event (e.g. a school enrollment event) is the positive label.</li> <li>If you set it to False, that means that all of these rows have a negative label. A common use for this would be in early warning problems of adverse events, in which the presence of an event (e.g. excessive use of force by a police officer) is the positive label.</li> </ul>"},{"location":"experiments/cohort-labels/#example-inspections_1","title":"Example: Inspections","text":""},{"location":"experiments/cohort-labels/#inspections-label-source-table","title":"Inspections Label Source Table","text":"<p>To translate this into our restaurant example above, consider a source table named 'inspections' that contains information about inspections. A simplified version of this table may look like:</p> entity_id date result 25 2016-01-05 pass 44 2016-01-04 fail 25 2016-02-04 fail <p>The entity id is the same as the cohort above: it identifies the restaurant. The date is just the date that the inspection happened, and the result is a string 'pass'/'fail' stating whether or not the restaurant passed the inspection. </p>"},{"location":"experiments/cohort-labels/#inspections-label-config","title":"Inspections Label Config","text":"<p>In constructing the label query, we have to consider the note above that we want to return only one row for a given entity id. The easiest way to do this, given that this query is run per as-of-date, is to group by the entity id and aggregate all the matched events somehow. In this case, a sensible definition is that we want any failed inspections to trigger a positive label. So if there is one pass and one fail that falls under the label timespan , the label should be True. <code>bool_or</code> is a handy Postgres aggregation function that does this.</p> <p>A query to find any failed inspections would be written in a sql file as follows:</p> <pre><code>select\n    entity_id,\n    bool_or(result = 'fail')::integer as outcome\nfrom inspections\nwhere '{as_of_date}'::timestamp &lt;= date\nand date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\ngroup by entity_id\n</code></pre> <p>And referenced in your config file as follows:</p> <pre><code>label_config:\n  filepath: 'labels/failed_inspection.sql'\n  name: 'failed_inspection'\n</code></pre> <p>As with the cohort configuration, you can use the <code>query</code> key in place of the <code>filepath</code> key, and put the query directly in the configuration:</p> <pre><code>label_config:\n  query: |\n    select\n        entity_id,\n        bool_or(result = 'fail')::integer as outcome\n    from inspections\n    where '{as_of_date}'::timestamp &lt;= date\n    and date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\n    group by entity_id\n  name: 'failed_inspection'\n</code></pre> <p>But this is discouraged for the same reasons: It becomes harder to keep multiple experiment files aligned.</p>"},{"location":"experiments/cohort-labels/#example-early-intervention_1","title":"Example: Early Intervention","text":""},{"location":"experiments/cohort-labels/#early-intervention-label-source-table","title":"Early Intervention Label Source Table","text":"<p>We reuse the generic events table used in the early intervention cohort section.</p>"},{"location":"experiments/cohort-labels/#early-intervention-label-config","title":"Early Intervention Label Config","text":"<p>We would like to assign a <code>True</code> label to everybody who is booked into jail within the label timespan. Note the <code>include_missing_labels_in_train_as</code> value: <code>False</code>. Anybody who does not show up in this query can be assumed to not have been booked into jail, so they can be assigned a <code>False</code> label.</p> <p>Your query file would look like:</p> <pre><code>SELECT entity_id,\n       bool_or(CASE WHEN event_type = 'booking' THEN TRUE END)::integer AS outcome\n  FROM events\n WHERE knowledge_date &lt;@ daterange('{as_of_date}'::date, ('{as_of_date}'::date + interval '{label_timespan}')::date)\n GROUP BY entity_id\n</code></pre> <p>And should be referenced in your config file like so:</p> <pre><code>label_config:\n    filepath: 'labels/booking.sql'\n    include_missing_labels_in_train_as: False\n    name: 'booking'\n</code></pre>"},{"location":"experiments/cohort-labels/#example-visits_1","title":"Example: Visits","text":""},{"location":"experiments/cohort-labels/#visits-label-source-table","title":"Visits Label Source Table","text":"<p>We reuse the diabetes_diagnoses table from the cohort section.</p>"},{"location":"experiments/cohort-labels/#visits-label-config","title":"Visits Label Config","text":"<p>We would like to identify people who are diagnosed with diabetes within a certain <code>label_timespan</code> after the given <code>as-of-date</code>. Note that <code>include_missing_labels_in_train_as</code> is False here as well. Any diagnoses would show up here, so the lack of any results from this query would remove all ambiguity.</p> <p>Your query file should look like:</p> <pre><code>select entity_id, 1 as outcome\nfrom diabetes_diagnoses\nwhere as_of_date &lt;@ daterange('{as_of_date}' :: date, ('{as_of_date}' :: date + interval '{label_timespan}') :: date)\ngroup by entity_id\n</code></pre> <pre><code>label_config:\n  filepath: 'labels/diabetes.sql'\n  include_missing_labels_in_train_as: False\n  name: 'diabetes'\n</code></pre> <p>Note: If you broadened the scope of this diabetes problem to concern not just diabetes diagnoses but having diabetes in general, and you had access to both positive and negative diabetes tests, you might avoid setting <code>include_missing_labels_in_train_as</code>, similar to the inspections problem, to more completely take into account the possibility that a person may or may not have diabetes.</p>"},{"location":"experiments/cohort-labels/#testing-label-configuration","title":"Testing Label Configuration","text":"<p>If you want to test out a label query without running a whole experiment, you can test it out similarly to the cohort section above.</p> <p>Option 1: You have not started writing an experiment config file yet. If you just want to test your label query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the <code>LabelGenerator</code> with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config:</p> <pre><code>from triage.component.architect.label_generators import LabelGenerator\nfrom triage import create_engine\nfrom datetime import datetime\n\nLabelGenerator(\n    query=\"select entity_id, bool_or(result='fail')::integer as outcome from inspections where '{as_of_date}'::timestamp &lt;= date and date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id\"\n    db_engine=create_engine(...),\n).generate_all_labels(\n    labels_table='test_labels',\n    as_of_dates=[datetime(2016, 1, 1), datetime(2016, 2, 1), datetime(2016, 3, 1)],\n    label_timespans=['3 month'],\n)\n</code></pre> <p>Running this will generate a table with the name you gave it (<code>test_labels</code>), populated with the labels for that list of dates. You can inspect this table in your SQL browser of choice.</p> <p>Option 2: You have an experiment config file that includes temporal config, and want to look at the labels in isolation in the database. If you want to actually create the labels for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the labels.</p> <pre><code>from triage.experiments import SingleThreadedExperiment\nfrom triage import create_engine\nimport yaml\n\nwith open('&lt;your_experiment_config.yaml&gt;') as fd:\n    experiment_config = yaml.full_load(fd)\nexperiment = SingleThreadedExperiment(\n    experiment_config=experiment_config,\n    db_engine=create_engine(...),\n    project_path='./'\n)\nexperiment.generate_labels()\nprint(experiment.labels_table_name)\n</code></pre> <p>This will generate the labels for each as-of-date in your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the <code>labels_table_name</code> attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice.</p> <p>These options should be enough to test your labels in isolation. How the labels shows up in matrices is also dependent on its interaction with the cohort, and later we'll show how to test that.</p>"},{"location":"experiments/cohort-labels/#combining-cohorts-and-labels-to-make-matrices","title":"Combining Cohorts and Labels to make Matrices","text":"<p>Looking at the cohort and labels tables in isolation doesn't quite get you the whole picture. They are combined with features to make matrices, and some of the functionality (e.g. <code>include_missing_labels_in_train_as</code>) isn't applied until the matrices are made for performance/database disk space purposes.</p> <p>How does this work? Let's look at some example cohort and label tables.</p>"},{"location":"experiments/cohort-labels/#cohort","title":"Cohort","text":"entity_id as_of_date 25 2016-01-01 44 2016-01-01 25 2016-02-01 44 2016-02-01 25 2016-03-01 44 2016-03-01 60 2016-03-01"},{"location":"experiments/cohort-labels/#label","title":"Label","text":"entity_id as_of_date label 25 2016-01-01 True 25 2016-02-01 False 44 2016-02-01 True 25 2016-03-01 False 44 2016-03-01 True 60 2016-03-01 True <p>Above we observe three total cohorts, on <code>2016-01-01</code>, <code>2016-02-01</code>, and <code>2016-03-01</code>. The first two cohorts have two entities each and the last one has a new third entity. For the first cohort, only one of the entities has an explicitly defined label (meaning the label query didn't return anything for them on that date).</p> <p>For simplicity's sake, we are going to assume only one matrix is created that includes all of these cohorts. Depending on the experiment's temporal configuration, there may be one, many, or all dates in a matrix, but the details here are outside of the scope of this document.</p> <p>In general, the index of the matrix is created using a left join in SQL: The cohort table is the left table, and the labels table is the right table, and they are joined on entity id/as of date. So all of the rows that are in the cohort but not the labels table (in this case, just entity 44/date 2016-01-01) will initially have a null label.</p> <p>The final contents of the matrix, however, depend on the <code>include_missing_labels_in_train_as</code> setting.</p>"},{"location":"experiments/cohort-labels/#inspections-style-preserve-missing-labels-as-null","title":"Inspections-Style (preserve missing labels as null)","text":"<p>If <code>include_missing_labels_in_train_as</code> is not set, Triage treats it as a truly missing label. The final matrix will look like:</p> entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... null 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True"},{"location":"experiments/cohort-labels/#early-warning-style-missing-means-false","title":"Early Warning Style (missing means False)","text":"<p>If <code>include_missing_labels_in_train_as</code> is set to False, Triage treats the absence of a label row as a False label. The final matrix will look like:</p> entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... False 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True"},{"location":"experiments/cohort-labels/#dropout-style-missing-means-true","title":"Dropout Style (missing means True)","text":"<p>If <code>include_missing_labels_in_train_as</code> is set to True, Triage treats the absence of a label row as a True label. The final matrix will look like:</p> entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... True 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True <p>If you would like to test how your cohort and label combine to make matrices, you can tell Triage to generate matrices and then inspect the matrices. To do this, we assume that you have your cohort and label defined in sql files referenced in an experiment config file, as well as temporal config. For convenience, we show a config using the <code>query</code> keys for cohort and label configs, but we recommend using the <code>filepath</code> keys as shown in other examples in your actual project. The last piece needed to make matrices is some kind of features. Of course, the features aren't our main focus here, so let's use a placeholder feature that should create very quickly.</p> <pre><code>config_version: 'v8'\n\nrandom_seed: 1234\n\ntemporal_config:\n    feature_start_time: '2010-01-04'\n    feature_end_time: '2018-03-01'\n    label_start_time: '2015-02-01'\n    label_end_time: '2018-03-01'\n\n    model_update_frequency: '1y'\n    training_label_timespans: ['1month']\n    training_as_of_date_frequencies: '1month'\n\n    test_durations: '1month'\n    test_label_timespans: ['1month']\n    test_as_of_date_frequencies: '1month'\n\n    max_training_histories: '5y'\n\ncohort_config:\n  query: |\n    select distinct(entity_id)\n    from permits\n    where\n    tsrange(start_time, end_time, '[]') @&gt; {as_of_date}\n  name: 'permits_in_last_year'\n\nlabel_config:\n  query: |\n    select\n    entity_id,\n    bool_or(result = 'fail')::integer as outcome\n    from inspections\n    where '{as_of_date}'::timestamp &lt;= date\n    and date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\n    group by entity_id\n  name: 'failed_inspection'\n\nfeature_aggregations:\n    -\n        prefix: 'test'\n        from_obj: 'permits'\n        knowledge_date_column: 'date'\n        aggregates_imputation:\n            all:\n                type: 'zero_noflag'\n        aggregates: [{quantity: '1', metrics: ['sum']}]\n        intervals: ['3month']\n</code></pre> <p>The above feature aggregation should just create a feature with the value <code>1</code> for each entity, but what's important here is that it's a valid feature config that allows us to make complete matrices. To make matrices using all of this configuration, you can run:</p> <pre><code>from triage.experiments import SingleThreadedExperiment\nfrom triage import create_engine\nimport yaml\n\nwith open('&lt;your_experiment_config.yaml&gt;') as fd:\n    experiment_config = yaml.full_load(fd)\nexperiment = SingleThreadedExperiment(\n    experiment_config=experiment_config,\n    db_engine=create_engine(...),\n    project_path='./'\n)\nexperiment.generate_matrices()\n</code></pre> <p>The matrix generation process will run all of the cohort/label/feature generation above, and then save matrices to your project_path's <code>matrices</code> directory. By default, these are CSVs and should have a few columns: 'entity_id', 'date', 'test_1_sum', and 'failed_inspection'. The 'entity_id' and 'date' columns represent the index of this matrix, and 'failed_inspection' is the label.  Each of these CSV files has a YAML file starting with the same hash representing metadata about that matrix. If you want to look for just the train matrices to inspect the results of the <code>include_missing_labels_in_train_as</code> flag, try this command (assuming you can use bash):</p> <pre><code>$ grep \"matrix_type: train\" *.yaml\n3343ebf255af6dbb5204a60a4390c7e1.yaml:matrix_type: train\n6ee3cd406f00f0f47999513ef5d49e3f.yaml:matrix_type: train\n74e2a246e9f6360124b96bea3115e01f.yaml:matrix_type: train\na29c9579aa67e5a75b2f814d906e5867.yaml:matrix_type: train\na558fae39238d101a66f9d2602a409e6.yaml:matrix_type: train\nf5bb7bd8f251a2978944ba2b82866153.yaml:matrix_type: train\n</code></pre> <p>You can then open up those files and ensure that the labels for each entity_id/date pair match what you expect.</p>"},{"location":"experiments/cohort-labels/#wrapup","title":"Wrapup","text":"<p>Cohorts and Labels require a lot of care to define correctly as they constitute a large part of the problem framing. Even if you leave all of your feature generation the same, you can completely change the problem you're modeling by changing the label and cohort. Testing your cohort and label config can give you confidence that you're framing the problem the way you expect.</p>"},{"location":"experiments/experiment-config/","title":"Configuration","text":"<p>Explain the parameters for running Triage experiments</p> <p>Triage is a great tool to make our life easier by semi-automating many different tasks when we are doing predictive anlaytics projects, so that the users can focus more on the problem formulation and modeling than implementation. The configuration helps users define the parameters in an experiment. To run a full Triage experiment, users are required to define <code>experiment.yaml</code> and <code>audition.yaml</code>. The <code>postmodeling_config.yaml</code> and <code>postmodeling_crosstabs.yaml</code> are optional, only for users who want to use <code>triage.postmodeling</code> module after experiment. </p>"},{"location":"experiments/experiment-config/#experiment-configuration","title":"Experiment Configuration","text":"<p>Also check out the example file <code>experiment.yaml</code>.</p>"},{"location":"experiments/experiment-config/#config-version","title":"Config Version","text":"<ul> <li><code>config_version</code>: The experiment configuration changes from time to time, and we upgrade the <code>triage.experiments.CONFIG_VERSION</code> variable whenever drastic changes that break old configuration files are released. Be sure to assign the config version that matches the <code>triage.experiments.CONFIG_VERSION</code> in the triage release you are developing against!</li> </ul>"},{"location":"experiments/experiment-config/#experiment-metadata","title":"Experiment Metadata","text":"<ul> <li><code>model_comment</code> (optional): will end up in the model_comment column of the models table for each model created in this experiment.</li> <li><code>random_seed</code> (optional): will be set in Python at the beginning of the experiment and affect the generation of all model seeds. If omitted, a random value will be chosen and recorded in the database to allow for future replication.</li> </ul>"},{"location":"experiments/experiment-config/#time-splitting","title":"Time Splitting","text":"<p>This section defines a set of parameters used in label generation and temporal cross-validation. </p> <p>Dates should be specified as date strings of the format <code>YYYY-MM-DD</code>. Intervals should be specified as strings of the Postgres interval input format.</p> <p>Some parameters accept lists of arguments. If multiple arguments are specified, Triage will run experiments on each available permutation of time splitting arguments. Arguments to parameters that take lists must be specified as lists, even if specifying a single variable.</p> <p>Note that these default values were selected to simplify the beginnings of your project, and are by no means suggested values.</p> Parameter Description Type Default Example Takes List feature_start_time Earliest date included in features date Earliest date in feature_aggregations from_objs '1995-01-01' No feature_end_time Latest date included in features date Latest date in feature_aggregations from_objs '2015-01-01' No label_start_time Earliest date for which labels are available date Latest date in feature_aggregations from_objs '2012-01-01' No label_end_time Day AFTER last label date (all dates in any model are before this date) date latest date in feature_aggregations from_objs '2015-01-01' No model_update_frequency How frequently to retrain models interval '100y' '4 months' No training_as_of_date_frequencies Time between as of dates for the same entity in train matrix interval ['100y'] ['1 week'] Yes test_as_of_date_frequencies Time between as of dates for the same entity in test matrix interval ['100y'] ['1 week'] Yes max_training_histories Length of time included in a train matrix interval ['0d'] ['1 month'] Yes test_durations Length of time included in a test matrix interval ['0d'] ['1 month'] Yes training_label_timespans Amount of time required to determine an entity's label. Affects training sets. See the Label Deep Dive for more information. interval No default ['1 month'] Yes test_label_timespans Same as above, affecting test sets. interval No default ['1 month'] Yes <p>Note that if your label timespan is the same in both testing and training, you can simply set a single <code>label_timespans</code> parameter instead of specifying both <code>training_label_timespans</code> and <code>test_label_timespans</code>.</p> <p></p> <p>This diagram represents three train/test sets generated by the following temporal config. Date and interval parameters are labeled where they affect the plot.</p> <pre><code>temporal_config:\n    feature_start_time: '2014-01-01'\n    feature_end_time: '2015-03-01'\n\n    label_start_time: '2014-03-01'\n    label_end_time: '2015-03-01'\n\n    model_update_frequency: '1 month'\n    training_as_of_date_frequencies: ['1 day']\n    test_as_of_date_frequencies: ['1 day']\n\n    max_training_histories: ['3 months']\n    test_durations: ['3 months']\n\n    label_timespans: ['2 months']\n</code></pre>"},{"location":"experiments/experiment-config/#cohort-config","title":"Cohort Config","text":"<p>Cohorts are configured by passing a query with placeholders for the as_of_date.</p> <ul> <li><code>cohort_conifg</code>:<ul> <li><code>filepath</code>: The <code>filepath</code> key should have a relative link (to the working directory where you're running triage) to a file containing a sql query parameterized with an <code>'{as_of_date}'</code>, to select the entity_ids that should be included for a given date. The <code>{as_of_date}</code> will be replaced with each <code>as_of_date</code> that the experiment needs. The returned <code>entity_id</code> must be an integer.</li> <li><code>query</code>: Instead of passing a file with your cohort query, you can include the query directly in the configuration file; this is not recommended as it required more maintanence to ensure that cohorts are consistent across experiments vs. storing cohort queries in their own files. It is retained for backwards compatibility with existing triage projects. Only one of <code>filepath</code> or <code>query</code> is permitted.</li> <li><code>name</code>: You may enter a <code>name</code> for your configuration. This will be included in the metadata for each matrix and used to group models. If you don't pass one, the string <code>default</code> will be used.</li> </ul> </li> </ul> <p>The cohort config section of your experiment config can optionally be omitted. If you don't specify it, it will default to all the entities in your event's tables  (from_obj in feature_aggregations). The name by default in this case will be 'all_entities'. As with the time splitting defaults, this may be a good starting point, but you will likely want to refine your cohort definition as you iterate on your project.</p>"},{"location":"experiments/experiment-config/#label-generation","title":"Label Generation","text":"<p>Labels are configured by passing a query with placeholders for the <code>as_of_date</code> and <code>label_timespan</code>.</p> <ul> <li><code>label_config</code>:<ul> <li><code>filepath</code>: A relative link to a file containing a query returning two columns: <code>entity_id</code> and <code>outcome</code>, based on a given <code>as_of_date</code> and <code>label_timespan</code>. The <code>as_of_date</code> and <code>label_timespan</code> must be represented by placeholders marked by curly brackets. The example in <code>experiment.yaml</code> reproduces the inspection outcome boolean-or logic. In addition, you can configure what label is given to entities that are in the matrix (see cohort_config section) but that do not show up in this label query.</li> <li><code>query</code>: Instead of passing a file with your  query, you can include the query directly in the configuration file; this is not recommended as it required more maintanence to ensure that labels are consistent across experiments vs. storing label queries in their own files. It is retained for backwards compatibility with existing triage projects. Only one of <code>filepath</code> or <code>query</code> is permitted.</li> <li><code>include_missing_labels_in_train_as</code>: However, passing the key <code>include_missing_labels_in_train_as</code> allows you to pick True or False. By default, these will show up as missing/null.</li> <li><code>name</code>: In addition to these configuration options, you can pass a name to apply to the label configuration that will be present in matrix metadata for each matrix created by this experiment, under the <code>label_name</code> key. The default label_name is <code>outcome</code>.</li> </ul> </li> </ul>"},{"location":"experiments/experiment-config/#feature-generation","title":"Feature Generation","text":"<p>The aggregate features to generate for each train/test split. Implemented by wrapping collate. Most terminology here is taken directly from collate.</p> <p>Each entry describes a collate.SpacetimeAggregation object, and the arguments needed to create it. Generally, each of these entries controls the features from one source table, though in the case of multiple groups may result in multiple output tables.</p> <p>Rules specifying how to handle imputation of null values must be explicitly defined in your config file. These can be specified in two places: either within each feature or overall for each type of feature (<code>aggregates_imputation</code>, <code>categoricals_imputation</code>, <code>array_categoricals_imputation</code>). In either case, a rule must be given for each aggregation function/metric (e.g., <code>sum</code>, <code>max</code>, <code>avg</code>, etc) used, or a catch-all can be specified with <code>all</code>. Aggregation function-specific rules will take precedence over the <code>all</code> rule and feature-specific rules will take precedence over the higher-level rules. Several examples are provided below. The supported aggregation functions/metrics are subject to the aggreagtion functions of the version of postgres being used.</p> <p>Available Imputation Rules: - <code>mean</code>: The average value of the feature (for SpacetimeAggregation the mean is taken within-date). - <code>constant</code>: Fill with a constant value from a required <code>value</code> parameter. - <code>zero</code>: Fill with zero. - <code>zero_noflag</code>: Fill with zero without generating an imputed flag. This option should be used only for cases where null values are explicitly known to be zero such as absence of an entity from an events table indicating that no such event has occurred. - <code>null_category</code>: Only available for categorical features. Just flag null values with the null category column. - <code>binary_mode</code>: Only available for aggregate column types. Takes the modal value for a binary feature. - <code>error</code>: Raise an exception if any null values are encountered for this feature.</p> <ul> <li> <p><code>feature_aggregations</code>: </p> <ul> <li><code>prefix</code>: prefix given to the resultant tables</li> <li><code>from_obj</code>: from_obj is usually a source table but can be an expression, such as a join (ie <code>cool_stuff join other_stuff using (stuff_id)</code>)</li> <li><code>knowledge_date_column</code>: The date column to use for specifying which records to include in temporal features. It is important that the column used specifies the date at which the event is known about, which may be different from the date the event happened.</li> <li><code>aggregates_imputation</code>: top-level imputation rules that will apply to all aggregates functions can also specify <code>categoricals_imputation</code> or <code>array_categoricals_imputation</code>. You must specify at least one of the top-level or feature-level imputation to cover every feature being defined.<ul> <li><code>all</code>: The <code>all</code> rule will apply to all aggregation functions, unless overridden by more specific one. This is a default/fallback imputation method for any aggregation from this <code>from_obj</code><ul> <li><code>type</code>: every imputation rule must have a <code>type</code> parameter, while some (like 'constant') have other required parameters (<code>value</code> here)</li> <li><code>value</code></li> </ul> </li> <li><code>some aggregation function</code>: The reudction function used by the aggreagate such as <code>sum</code>, <code>count</code>, <code>avg</code>, <code>max</code> etc. Function-specific rules will take precedence over the catch-all rule.</li> </ul> </li> <li> <p><code>aggregates</code>: aggregates and categoricals define the actual features created. So at least one is required. Aggregates of numerical columns.</p> <ul> <li>No keys are used on this line, which marks the first aggregate in a list<ul> <li><code>quantity</code>: Each quantity is a number of some and the list of metrics are applied to each quantity, e.g. <code>homeless::INT</code></li> <li><code>imputation</code>:  Imputation rules specified at the level of specific features will take precedence over the higer-level rules specified above. Note that the 'count' and 'sum' metrics will be imputed differently here.<ul> <li><code>count</code>:<ul> <li><code>type</code>: <code>mean</code></li> </ul> </li> <li><code>sum</code>:<ul> <li><code>type</code>: <code>constant</code></li> <li><code>value</code>: <code>137</code></li> </ul> </li> </ul> </li> <li><code>metrics</code>: the metrics/aggreagtion functions listed here will be used for the current aggregation only and must be defined separately for all aggregations.<ul> <li><code>count</code></li> <li><code>sum</code></li> </ul> </li> <li><code>coltype</code>: <code>smallint</code> (Optional, if you want to control the column type in the generated features tables)</li> </ul> </li> <li>No keys are used on this line, which marks the second aggregate in a list<ul> <li><code>quantity</code>: <code>some_flag</code> (Since we're specifying <code>aggregates_imputation</code> above, a feature-specific imputation rule can be omitted)</li> <li><code>metrics</code>: <ul> <li><code>max</code></li> <li><code>sum</code></li> </ul> </li> </ul> </li> </ul> </li> <li> <p><code>categoricals</code>:  Categorical features. The column given can be of any type, but the choices must comparable to that type for equality within SQL The result will be one feature for each choice/metric combination.</p> </li> <li>(First column)<ul> <li><code>column</code>: Note that we haven't specified a top level <code>categoricals_imputation</code> set of rules, so we have to include feature-specific imputation rules for both of our categoricals here.</li> <li><code>imputation</code>:<ul> <li><code>sum</code>:<ul> <li><code>type</code>: <code>null_category</code></li> </ul> </li> <li><code>max</code>:<ul> <li><code>type</code>: <code>mean</code></li> </ul> </li> </ul> </li> <li><code>choices</code>:</li> <li><code>metrics</code>: <code>sum</code></li> </ul> </li> <li>(Second column)<ul> <li><code>column</code>: <code>shape</code> (As with the top-level imputation rules, <code>all</code> can be used for the feature-level rules to specify the same type of imputation for all aggregation functions)</li> <li><code>imputation</code>:<ul> <li><code>all</code>:     <code>type</code>: <code>zero</code></li> </ul> </li> <li><code>choice_query</code>: <code>select distinct shape from cool stuff</code></li> <li><code>metrics</code>: <ul> <li><code>sum</code></li> </ul> </li> </ul> </li> <li><code>intervals</code>: The time intervals over which to aggregate features</li> <li><code>groups</code>: A list of different columns to separately group by</li> </ul> </li> </ul>"},{"location":"experiments/experiment-config/#feature-grouping-optional","title":"Feature Grouping (optional)","text":"<p>define how to group features and generate combinations</p> <ul> <li> <p><code>feature_group_definition</code>: <code>feature_group_definition</code> allows you to create groups/subset of your features by different criteria. One can either specify <code>tables</code> or <code>prefix</code> (If omitted, all unique prefixes present in <code>feature_aggregations</code> will be used). </p> <ul> <li><code>tables</code>: allows you to send a list of collate feature tables (collate builds these by appending <code>aggregation_imputed</code> to the prefix)</li> <li><code>prefix</code>: allows you to specify a list of feature name prefixes. Triage will consider prefixes sharing their initial string as the same (e.g., <code>bookings</code> will encompass <code>bookings_charges</code> as well as <code>bookings</code>). In other words, it's not actually maintaining a list of prefixes and searching for exact matches but trusting that prefixes do not overlap in this way.</li> </ul> </li> <li> <p><code>feature_group_strategies</code>: strategies for generating combinations of groups. available: all, leave-one-out, leave-one-in, all-combinations</p> </li> </ul>"},{"location":"experiments/experiment-config/#user-metadata","title":"User Metadata","text":"<p>These are arbitrary keys/values that you can have Triage apply to the metadata for every matrix in the experiment. Any keys you include here can be used in the <code>model_group_keys</code> below. User metadata makes Triage extensible to support ad hoc needs for your use case. In the early days of Triage, it was often used as a placeholder for yet-to-be developed features, such as setting a flag for a test run when there was not yet an option to preview experiment components before running. </p> <p>Nowadays, user metadata is often used to store details of the system or data state that fall outside of Triage's purview. For example, on cross-systems projects, Triage is often run after an entity-matching algorithm that links users across different databases. When that entity-matching model is updated, the linkage of Triage <code>entity_id</code>s to source data records may change, invalidating the predictions tables.  To avoid this, you can add the model id of your entity matching model to the <code>user_metadata</code> that signifies which set of <code>entity_id</code>s was used to create a given model/model group. That way, you can don't accidentally match old predictions to new ids during postmodel exploration. </p> <ul> <li><code>user_metadata</code>: <ul> <li><code>entity_model_id</code>: <code>em-0242ac120002</code> </li> </ul> </li> </ul>"},{"location":"experiments/experiment-config/#model-grouping-optional","title":"Model Grouping (optional)","text":"<p>Model groups are a way of partitioning trained models in a way that makes for easier analysis.</p> <p><code>model_group_keys</code> defines a list of training matrix metadata and classifier keys that should be considered when creating a model group.</p> <p>There is an extensive default configuration, which is aimed at producing groups whose constituent models are equivalent to each other in all ways except for when they were trained. This makes the analysis of model stability easier.</p> <p>To accomplish this, the following default keys are used: <code>class_path</code>, <code>parameters</code>, <code>feature_names</code>, <code>feature_groups</code>, <code>cohort_name</code>, <code>state</code>, <code>label_name</code>, <code>label_timespan</code>, <code>as_of_date_frequency</code>, <code>max_training_history</code></p> <p>If you want to override this list, you can supply a <code>model_group_keys</code> value. All of the defaults are available, along with some other temporal information that could be useful for more specialized analyses:</p> <p><code>first_as_of_time</code>, <code>last_as_of_time</code>, <code>matrix_info_end_time</code>, <code>as_of_times</code>, <code>feature_start_time</code></p> <p>You can also use any pieces of user_metadata that you included in this experiment definition, as they will be present in the matrix metadata.  - <code>model_group_keys</code>: [<code>feature_groups</code>, <code>label_definition</code>]</p>"},{"location":"experiments/experiment-config/#bias-audit-config-optional","title":"Bias Audit Config (optional)","text":"<p>Every evaluation will include a bias audit (using the Aequitas toolkit). To run the bias audit it is necessary to define the protected groups by defining attributes (e.g. race) for every entity</p> <p>from_obj parameter: it can be a table name or a query (such as with features generators) The from_obj is expected to have the protected attributes in a single table with a entity_id and knowledge date column</p> <p>Triage will use the most recent entry of the source table with date &lt; than current as_of_date as the value for those attributes in a given as of date</p> <p>Running the bias audit might be slow, so the user should specify which thresholds should be used for the bias audit</p> <p>Please have a look to Aequitas documentation for further information about the ref_groups_method https://dssg.github.io/aequitas/config.html By default uses the min_metric, meaning for each bias metric it uses as reference the group with minimum metric value (e.g. the group defined by race that has the lower FPR) Alternatively it can be 'majority' (picks the largest group to serve as reference) or 'predefined' (needs a list of key values, see below)</p>"},{"location":"experiments/experiment-config/#model-grid-presets","title":"Model Grid Presets","text":"<p>Triage now comes with a set of predefined recommended grids named: quickstart, small, medium, large See the documentation for recommended uses cases for those.</p> <p>If you set this configuration section you SHOULD NOT include the grid_config section</p> <ul> <li><code>model_grid_presets</code>: One of 'quickstart', 'small', 'medium', 'large'</li> </ul>"},{"location":"experiments/experiment-config/#grid-configuration","title":"Grid Configuration","text":"<p>The classifier/hyperparameter combinations that should be trained</p> <p>Each top-level key should be a class name, importable from triage. sklearn is available, and if you have another classifier package you would like available, contribute it to requirement/main.txt</p> <ul> <li><code>grid_config</code>: Each lower-level key is a hyperparameter name for the given classifier, and each value is a list of potential values. All possible combinations of classifiers and hyperparameters are trained. Please check out the grid_config section in <code>experiment.yaml</code> as for a detailed example.</li> </ul> <p>NOTE: Triage now include a new parameter named 'model_grid_presets' (see above) you can't have both at the same time. The experiment will fail if you forget this.</p>"},{"location":"experiments/experiment-config/#prediction","title":"Prediction","text":"<p>How predictions are computed for train and test matrices? This is used only for stored predcitions and only affect postmodeling analysis (not model scoring), so if you are not stroing predictions, this will not affect anything.</p> <ul> <li><code>prediction</code>: Rank tiebreaking - In the predictions.rank_abs and rank_pct columns, ties in the score are broken either at random or based on the <code>worst</code> or <code>best</code> options. <code>worst</code> is the default.</li> </ul> <p><code>worst</code> will break ties with the ascending label value, so if you take the top k predictions, and there are ties across the k threshold, the predictions above the threshold will be negative labels if possible.</p> <p><code>best</code> will break ties with the descending label value, so if you take the top k predictions, and there are ties across the k threshold, the predictions above the threshold will be positive labels if possible.</p> <p><code>random</code> will choose one random ordering to break ties. The result will be affected by current state of Postgres' random number generator. Before ranking, the generator is seeded based on the model's random seed.</p>"},{"location":"experiments/experiment-config/#model-scoring","title":"Model Scoring","text":"<p>How each trained model is scored?</p> <p>Each entry in <code>testing_metric_groups</code> needs a list of one of the metrics defined in catwalk.evaluation.ModelEvaluator.available_metrics (contributions welcome!) Depending on the metric, either thresholds or parameters.</p> <p><code>Parameters</code>: specify any hyperparameters needed. For most metrics, which are simply wrappers of sklearn functions, these are passed directly to sklearn. </p> <ul> <li><code>thresholds</code> are more specific: The list is dichotomized and only the top percentile or top n entities are scored as positive labels</li> </ul> <p>subsets, if passed, will add evaluations for subset(s) of the predictions to the subset_evaluations tables, using the same testing and training metric groups as used for overall evaluations but with any thresholds reapplied only to entities in the subset on the relevant as_of_dates. For example, when calculating precision@5_pct for the subset of women, the ModelEvaluator will count as positively labeled the top 5% of women, rather than any women in the top 5% overall. This is useful if, for example, different interventions will be applied to different subsets of entities (e.g., one program will provide subsidies to the top 500 women with children and another program will provide shelter to the top 150 women without children) and you would like to see whether a single model can be used for both applications. Subsets can also be used to see how a model's performance would be affected if the requirements for intervention eligibility became more restricted.</p>"},{"location":"experiments/experiment-config/#individual-importances","title":"Individual Importances","text":"<p>How feature importances for individuals should be computed. This entire section can be left blank, in which case the defaults will be used.</p> <ul> <li><code>individual_importance</code>:<ul> <li><code>methods</code>: Refer to how to compute individual importances. Each entry in this list should represent a different method. Available methods are in the catwalk library's: <code>catwalk.individual_importance.CALCULATE_STRATEGIES</code> list. Will default to <code>uniform</code>, or just the global importances. Empty list means don't calculate individual importances. Individual importances take up the largest amount of database space, so an empty list is a good idea unless you need them.</li> <li><code>n_ranks</code>: The number of top features per individual to compute importances for. Will default to 5.</li> </ul> </li> </ul>"},{"location":"experiments/feature-testing/","title":"Testing a Feature Aggregation","text":"<p>Developing features for Triage experiments can be a daunting task. There are a lot of things to configure, a small amount of configuration can result in a ton of SQL, and it can take a long time to validate your feature configuration in the context of an Experiment being run on real data.</p> <p>To speed up the process of iterating on features, you can run a list of feature aggregations, without imputation, on just one as-of-date. This functionality can be accessed through the <code>triage</code> command line tool or called directly from code (say, in a Jupyter notebook) using the <code>FeatureGenerator</code> component.</p>"},{"location":"experiments/feature-testing/#using-triage-cli","title":"Using Triage CLI","text":"<p>The command-line interface for testing features takes in two arguments:     - An experiment config file. Refer to the example_experiment_config.yaml's <code>feature_aggregations</code> section. It consists of a YAML list, with one or more feature_aggregation rows present.     - An as-of-date. This should be in the format <code>2016-01-01</code>.</p> <p>Example: <code>triage experiment featuretest example/config/experiment.yaml 2016-01-01</code></p> <p>All given feature aggregations will be processed for the given date. You will see a bunch of queries pass by in your terminal, populating tables in the <code>features_test</code> schema which you can inspect afterwards.</p> <p></p>"},{"location":"experiments/feature-testing/#using-python-code","title":"Using Python Code","text":"<p>If you'd like to call this from a notebook or from any other Python code, the arguments look similar but are a bit different. You have to supply your own sqlalchemy database engine to create a 'FeatureGenerator' object, and then call the <code>create_features_before_imputation</code> method with your feature config as a list of dictionaries, along with an as-of-date as a string. Make sure your logging level is set to INFO if you want to see all of the queries.</p> <pre><code>from triage.component.architect.feature_generators import FeatureGenerator\nfrom triage.util.db import create_engine\nimport logging\nimport yaml\n\nlogging.basicConfig(level=logging.INFO)\n\n# create a db_engine \ndb_url = 'your db url here'\ndb_engine = create_engine(db_url)\n\nfeature_config = [{\n    'prefix': 'aprefix',\n    'aggregates': [\n        {\n        'quantity': 'quantity_one',\n        'metrics': ['sum', 'count'],\n    ],\n    'categoricals': [\n        {\n            'column': 'cat_one',\n            'choices': ['good', 'bad'],\n            'metrics': ['sum']\n        },\n    ],\n    'intervals': ['all'],\n    'knowledge_date_column': 'knowledge_date',\n    'from_obj': 'data'\n}]\n\nFeatureGenerator(db_engine, 'features_test').create_features_before_imputation(\n    feature_aggregation_config=feature_config,\n    feature_dates=['2016-01-01']\n)\n</code></pre>"},{"location":"experiments/features/","title":"Feature Generation Recipe Book","text":"<p>This document is a collection of 'collate' aggregate features that we have found useful to create in Triage that may not be apparent at first.</p> <p>For an introduction to feature generation in Triage, refer to Dirty Duck Feature Generation</p>"},{"location":"experiments/features/#age","title":"Age","text":"<p>You can calculate age from a date of birth column using the <code>collate_date</code> special variable. This variable is marked as a placeholder in the feature quantity input, but is replaced with each as-of-date when features are being calculated. Combined with the Postgres <code>age</code> function, this calculates a person's age at each as-of-date as a feature.</p> <p>For this example, let's assume you have a column called 'dob' that is a timestamp (or anything that can be cast to a date) in your source table. The <code>feature_aggregation</code>'s quantity would be: </p> <p><code>EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE))</code></p> <p>If Triage is calculating this for the as-of-date '2016-01-01', it will internally expand the <code>collate_date</code> out to: <code>EXTRACT(YEAR FROM AGE('2016-01-01'::DATE, dob::DATE))</code></p> <p>In context, a feature aggregate that uses age may look more like:</p> <pre><code>    aggregates:\n      - # age in years \n        quantity:\n          age: \"EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE))\"\n        metrics: ['max']\n</code></pre> <p>Here, we call the feature 'age' and since everything in collate is defined as an aggregate, we pick 'max'; Any records for the same person and as-of-date should have the same 'dob', so there are many aggregates you can use that will arrive at the same answer (e.g. 'min', 'avg'). In these cases 'max' is the standard aggregate metric of choice in Triage.</p>"},{"location":"experiments/prediction-ranking/","title":"Prediction Ranking","text":"<p>The predictions tables in the <code>train_results</code> and <code>test_results</code> schemas contain several different flavors of rankings, covering absolute vs percentile ranking and whether or not ties exist.</p>"},{"location":"experiments/prediction-ranking/#ranking-columns","title":"Ranking columns","text":"Column name Behavior rank_abs_with_ties Absolute ranking, with ties. Ranks will skip after a set of ties, so if two entities are tied at rank 3, the next entity after them will have rank 5. rank_pct_with_ties Percentile ranking, with ties. Percentiles will skip after a set of ties, so if two entities out of ten are tied at 0.1 (tenth percentile), the next entity after them will have 0.3 (thirtieth percentile). At most five decimal places. rank_abs_no_ties Absolute ranking, with no ties. Ties are broken according to a configured choice: 'best', 'worst', or 'random', which is recorded in the <code>prediction_metadata</code> table rank_pct_no_ties Percentile ranking, with no ties. Ties are broken according to a configured choice: 'best', 'worst', or 'random', which is recorded in the <code>prediction_metadata</code> table. At most five decimal places."},{"location":"experiments/prediction-ranking/#viewing-prediction-metadata","title":"Viewing prediction metadata","text":"<p>The <code>prediction_metadata</code> table contains information about how ties were broken. There is one row per model/matrix combination. For each model and matrix, it records:</p> <ul> <li><code>tiebreaker_ordering</code> - The tiebreaker ordering rule (e.g. 'random',   'best', 'worst') used for the corresponding predictions.</li> <li><code>random_seed</code> - The random seed, if 'random' was the ordering   used. Otherwise None</li> <li><code>predictions_saved</code> - Whether or not predictions were saved. If it's   false, you won't expect to find any predictions, but the row is   inserted as a record that the prediction was performed.</li> </ul> <p>There is one <code>prediction_metadata</code> table in each of the <code>train_results</code>, <code>test_results</code> schemas (in other words, wherever there is a companion <code>predictions</code> table).</p>"},{"location":"experiments/prediction-ranking/#subsequent-runs","title":"Subsequent runs","text":"<p>If you run Triage Experiments with <code>replace=False</code>, and you change nothing except for the <code>rank_tiebreaker</code> in experiment config, ranking will be redone and the row in <code>prediction_metadata</code> updated. You don't have to run a full experiment if that's all you want to do; you could follow the directions for backfilling ranks above, which will redo the ranking for an individual model/matrix pair. However, changing the <code>rank_tiebreaker</code> in experiment config and re-running the experiment is a handy way of redoing all of them if that's what is useful.</p>"},{"location":"experiments/running/","title":"Running an Experiment","text":""},{"location":"experiments/running/#prerequisites","title":"Prerequisites","text":"<p>To use a Triage experiment, you first need:</p> <ul> <li>Python 3.8+</li> <li>A PostgreSQL (v9.6+) database with your source data (events, geographical data, etc) loaded.</li> <li>Ample space on an available disk (or S3) to store the needed matrices and models for your experiment</li> <li>An experiment definition (see Experiment configuration)</li> </ul> <p>You may run a Triage experiment two ways: through the Triage command line tool, or through instantiating an Experiment object in your own Python code and running it. The rest of this document will introduce experiment runs of increasing complexity, through both the CLI and Python interfaces.</p>"},{"location":"experiments/running/#simple-example","title":"Simple Example","text":"<p>To run an experiment, you need to tell triage at a minimum where to find the experiment file (in YAML format), and how to connect to the database, In this simple example, we're assuming that the experiment will be run with only one process, and that the matrices and models should be stored on the local filesystem.</p>"},{"location":"experiments/running/#cli","title":"CLI","text":"<p>The Triage CLI defaults database connection information to a file stored in 'database.yaml', so with this you can omit any mention of the database. In addition, if you leave out the project path. In addition, the 'project path' (where matrices and models are stored) defaults to the current working directory. So this is the simplest possible invocation:</p> <pre><code>triage experiment example/config/experiment.yaml\n</code></pre> <p>If you have the database information stored somewhere else, you may pass it to the top-level 'triage' command:</p> <pre><code>triage -d mydbconfig.yaml experiment example/config/experiment.yaml\n</code></pre> <p>Assuming you want the matrices and models stored somewhere else, pass it as the <code>--project-path</code>:</p> <pre><code>triage -d mydbconfig.yaml experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data'\n</code></pre>"},{"location":"experiments/running/#python","title":"Python","text":"<p>When running an experiment in Python, the database information is passed in the form of a SQLAlchemy database engine, and the experiment information is passed as a dictionary rather as YAML specifically.</p> <pre><code>from triage.experiments import SingleThreadedExperiment\n\nexperiment = SingleThreadedExperiment(\n    config=experiment_config, # a dictionary\n    db_engine=create_engine(...), # http://docs.sqlalchemy.org/en/latest/core/engines.html\n    project_path='/path/to/directory/to/save/data'\n)\nexperiment.run()\n</code></pre> <p>Either way you run it, you are likely to see a bunch of log output.  Once the feature/cohor/label/matrix building is done and the experiment has moved onto modeling, check out the <code>triage_metadata.models</code> and <code>test_results.evaluations</code> tables as data starts to come in. You'll see the simple models (Decision Trees, Scaled Logistic Regression, baselines) populate first, followed by your big models, followed by the rest. You can start to look at the simple model results first to get a handle on what basic classifiers can do for your feature space while you wait for the Random Forests to run.</p>"},{"location":"experiments/running/#multicore-example","title":"Multicore example","text":"<p>Triage also offers the ability to locally parallelize both CPU-heavy and database-heavy tasks. Triage uses the pebble library to perform both of these, but they are separately configurable as the database tasks will more likely be bounded by the number of connections/cores available on the database server instead of the number of cores available on the experiment running machine.</p>"},{"location":"experiments/running/#cli_1","title":"CLI","text":"<p>The Triage CLI allows parallelization to be specified through the <code>--n-processes</code> and <code>--n-db-processes</code> parameters.</p> <pre><code>triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --n-db-processes 4 --n-processes 8\n</code></pre>"},{"location":"experiments/running/#python_1","title":"Python","text":"<p>In Python, you can use the <code>MultiCoreExperiment</code> instead of the <code>SingleThreadedExperiment</code>, and similarly pass the <code>n_processes</code> and <code>n_db_processes</code> parameters. We also recommend using <code>triage.create_engine</code>. It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the database URL only, which may cancel other settings you have used to configure your engine.</p> <pre><code>from triage.experiments import MultiCoreExperiment\nfrom triage import create_engine\n\nexperiment = MultiCoreExperiment(\n    config=experiment_config, # a dictionary\n    db_engine=create_engine(...),\n    project_path='/path/to/directory/to/save/data',\n    n_db_processes=4,\n    n_processes=8,\n)\nexperiment.run()\n</code></pre> <p>The pebble library offers an interface around Python3's <code>concurrent.futures</code> module that adds in a very helpful tool: watching for killed subprocesses . Model training (and sometimes, matrix building) can be a memory-hungry task, and Triage can not guarantee that the operating system you're running on won't kill the worker processes in a way that prevents them from reporting back to the parent Experiment process. With Pebble, this occurrence is caught like a regular Exception, which allows the Process pool to recover and include the information in the Experiment's log.</p>"},{"location":"experiments/running/#using-s3-to-store-matrices-and-models","title":"Using S3 to store matrices and models","text":"<p>Triage can operate on different storage engines for matrices and models, and besides the standard filesystem engine comes with S3 support out of the box. To use this, just use the <code>s3://</code> scheme for your <code>project_path</code> (this is similar for both Python and the CLI).</p>"},{"location":"experiments/running/#cli_2","title":"CLI","text":"<pre><code>triage experiment example/config/experiment.yaml --project-path 's3://bucket/directory/to/save/data'\n</code></pre>"},{"location":"experiments/running/#python_2","title":"Python","text":"<pre><code>from triage.experiments import SingleThreadedExperiment\n\nexperiment = SingleThreadedExperiment(\n    config=experiment_config, # a dictionary\n    db_engine=create_engine(...),\n    project_path='s3://bucket/directory/to/save/data'\n)\nexperiment.run()\n</code></pre>"},{"location":"experiments/running/#validating-an-experiment","title":"Validating an Experiment","text":"<p>Configuring an experiment is complex, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So when you have completed your experiment config and want to test it out, it's best to validate the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the <code>cat_complaints</code> table in a feature aggregation but it doesn't exist, I'll see something like this:</p> <pre><code>*** ValueError: from_obj query does not run.\nfrom_obj: \"cat_complaints\"\nFull error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist\nLINE 1: explain select * from cat_complaints\n                              ^\n [SQL: 'explain select * from cat_complaints']\n</code></pre>"},{"location":"experiments/running/#cli_3","title":"CLI","text":"<p>The CLI, by default, validates before running. You can tweak this behavior, and make it not validate, or make it only validate.</p> <pre><code>triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --no-validate\n</code></pre> <pre><code>triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --validate-only\n</code></pre>"},{"location":"experiments/running/#python_3","title":"Python","text":"<p>The python interface will also validate by default when running an experiment. If you would prefer to skip this step, you can pass <code>skip_validation=True</code> when constructing your experiment.</p> <p>You can also run this validation step directly. Experiments expose a <code>validate</code> method that can be run as needed. Experiment instantiation doesn't change from the run examples at all.</p> <pre><code>experiment.validate()\n</code></pre> <p>By default, the <code>validate</code> method will stop as soon as it encounters an error ('strict' mode). If you would like it to validate each section without stopping (i.e. if you have only written part of the experiment configuration), call <code>validate(strict=False)</code> and all of the errors will be changed to warnings.</p> <p>We'd like to add more validations for common misconfiguration problems over time. If you got an unexpected error that turned out to be related to a confusing configuration value, help us out by adding to the validation module and submitting a pull request!</p>"},{"location":"experiments/running/#restarting-an-experiment","title":"Restarting an Experiment","text":"<p>If an experiment fails for any reason, you can restart it.</p> <p>By default, all work will be recreated. This includes label queries, feature queries, matrix building, model training, etc. However, if you pass the <code>replace=False</code> keyword argument, the Experiment will reuse what work it can.</p> <ul> <li>Cohort Table: The Experiment refers to a cohort table namespaced by the cohort name and a hash of the cohort query, and in that way allows you to reuse cohorts between different experiments if their label names and queries are identical. When referring to this table, it will check on an as-of-date level whether or not there are any existing rows for that date, and skip the cohort query for that date if so. For this reason, it is not aware of specific entities or source events so if the source data has changed, ensure that <code>replace</code> is set to True. </li> <li>Labels Table: The Experiment refers to a labels table namespaced by the label name and a hash of the label query, and in that way allows you to reuse labels between different experiments if their label names and queries are identical. When referring to this table, it will check on a per-<code>as_of_date</code>/<code>label timespan</code> level whether or not there are any existing rows, and skip the label query if so. For this reason, it is not aware of specific entities or source events so if the label query has changed or the source data has changed, ensure that <code>replace</code> is set to True.</li> <li>Features Tables: The Experiment will check on a per-table basis whether or not it exists and contains rows for the entire cohort, and skip the feature generation if so. It does not look at the column list for the feature table or inspect the feature data itself. So, if you have modified any source data that affects a feature aggregation, or added any columns to that aggregation, you won't want to set <code>replace</code> to False. However, it is cohort-and-date aware so you can change around your cohort and temporal configuration safely.</li> <li>Matrix Building: Each matrix's metadata is hashed to create a unique id. If a file exists in storage with that hash, it will be reused.</li> <li>Model Training: Each model's metadata (which includes its train matrix's hash) is hashed to create a unique id. If a file exists in storage with that hash, it will be reused.</li> </ul>"},{"location":"experiments/running/#cli_4","title":"CLI","text":"<pre><code>triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --replace\n</code></pre>"},{"location":"experiments/running/#python_4","title":"Python","text":"<pre><code>from triage.experiments import SingleThreadedExperiment\n\nexperiment = SingleThreadedExperiment(\n    config=experiment_config, # a dictionary\n    db_engine=create_engine(...),\n    project_path='s3://bucket/directory/to/save/data',\n    replace=True\n)\nexperiment.run()\n</code></pre>"},{"location":"experiments/running/#optimizing-an-experiment","title":"Optimizing an Experiment","text":""},{"location":"experiments/running/#skipping-prediction-syncing","title":"Skipping Prediction Syncing","text":"<p>By default, the Experiment will save predictions to the database. This can take a long time if your test matrices have a lot of rows, and isn't quite necessary if you just want to see the high-level performance of your grid. By switching <code>save_predictions</code> to <code>False</code>, you can skip the prediction saving. You'll still get your evaluation metrics, so you can look at performance. Don't worry, you can still get your predictions back later by rerunning the Experiment later at default settings, which will find your already-trained models, generate predictions, and save them.</p> <p>CLI: <code>triage experiment myexperiment.yaml --no-save-predictions</code></p> <p>Python: <code>SingleThreadedExperiment(..., save_predictions=False)</code></p>"},{"location":"experiments/running/#running-parts-of-an-experiment","title":"Running parts of an Experiment","text":"<p>If you would like incrementally build, or just incrementally run parts of the Experiment look at their outputs, you can do so. Running a full experiment requires the experiment config to be filled out, but when you're getting started using Triage it can be easier to build the experiment piece by piece and see the results as they come in. Make sure logging is set to INFO level before running this to ensure you get all the log messages. Additionally, because the default behavior of triage is to run config file validation (which expects a complete experiment configuration) and fill in missing values in some sections with defaults, you will need to pass <code>partial_run=True</code> when constructing your experiment object for a partial experiment (this will also avoid cleaning up intermediate tables from the run, equivalent to <code>cleanup=False</code>).</p> <p>Running parts of an experiment is only supported through the Python interface.</p>"},{"location":"experiments/running/#python_5","title":"Python","text":"<ol> <li> <p><code>experiment.run()</code> will run until it no longer has enough configuration to proceed. You will see information in the logs telling you about the steps it was able to perform. You can additionally view the intermediate tables that are built in the database, which are modified with the experiment hash that the experiment calculates, but this will be printed out in the log messages.</p> <ul> <li><code>labels_*&lt;experiment_hash&gt;*</code> for the labels generated per entity and as of date.</li> <li><code>tmp_sparse_states_*&lt;experiment_hash&gt;*</code> for the membership in each cohort per entity and as_of_date</li> </ul> </li> <li> <p>To reproduce the entire Experiment piece by piece, you can run the following. Each one of these methods requires some portion of experiment config to be passed:</p> <ul> <li> <p><code>experiment.split_definitions</code> will parse temporal config and create time splits. It only requires <code>temporal_config</code>.</p> </li> <li> <p><code>experiment.generate_cohort()</code> will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates. It requires <code>temporal_config</code> and <code>cohort_config</code>.</p> </li> <li> <p><code>experiment.generate_labels()</code> will use the label config and as of dates from the temporal config to generate an internal labels table. It requires <code>temporal_config</code> and <code>label_config</code>.</p> </li> <li> <p><code>experiment.generate_preimputation_features()</code> will use the feature aggregation config and as of dates from the temporal config to generate internal features tables. It requires <code>temporal_config</code> and <code>feature_aggregations</code>.</p> </li> <li> <p><code>experiment.generate_imputed_features()</code> will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables. It requires <code>temporal_config</code> and <code>feature_aggregations</code>.</p> </li> <li> <p><code>experiment.build_matrices()</code> will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices.  It requires <code>temporal_config</code>, <code>cohort_config</code>, <code>label_config</code>, and <code>feature_aggregations</code>, though it will also use <code>feature_group_definitions</code>, <code>feature_group_strategies</code>, and <code>user_metadata</code> if present.</p> </li> <li> <p><code>experiment.train_and_test_models()</code> will use the generated matrices, grid config and evaluation metric config to train and test all needed models. It requires all configuration keys.</p> </li> </ul> </li> </ol>"},{"location":"experiments/running/#evaluating-results-of-an-experiment","title":"Evaluating results of an Experiment","text":"<p>After the experiment run, a variety of schemas and tables will be created and populated in the configured database:</p> <ul> <li>triage_metadata.experiments - The experiment configuration, a hash, and some run-invariant details about the configuration</li> <li>triage_metadata.experiment_runs - Information about the experiment run that may change from run to run, pertaining to the run environment, status, and results</li> <li>triage_metadata.matrices - Each train or test matrix that is built has a row here, with some basic metadata</li> <li>triage_metadata.experiment_matrices - A many-to-many table between experiments and matrices. This will have a row if the experiment used the matrix, regardless of whether or not it had to build it</li> <li>triage_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved.</li> <li>triage_metadata.experiment_models - A many-to-many table between experiments and models. This will have a row if the experiment used the model, regardless of whether or not it had to build it</li> <li>triage_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but have different training windows. Look at these to see how classifiers perform over different training windows.</li> <li>triage_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration.</li> <li>triage_metadata.subsets - Each evaluation subset that was used for model scoring has its configuation and a hash written here</li> <li>train_results.feature_importances - The sklearn feature importances results for each trained model</li> <li>train_results.predictions - Prediction probabilities for train matrix entities generated against trained models</li> <li>train_results.prediction_metadata - Metadata about the prediction stage for a model and train matrix, such as tiebreaking configuration</li> <li>train_results.evaluations - Metric scores of trained models on the training data.</li> <li>test_results.predictions - Prediction probabilities for test matrix entities generated against trained models</li> <li>test_results.prediction_metadata - Metadata about the prediction stage for a model and test matrix, such as tiebreaking configuration</li> <li>test_results.evaluations - Metric scores of trained models over given testing windows and subsets</li> <li>test_results.individual_importances - Individual feature importance scores for test matrix entities.</li> </ul> <p>Here's an example query, which returns the top 10 model groups by precision at the top 100 entities:</p> <pre><code>    select\n        model_groups.model_group_id,\n        model_groups.model_type,\n        model_groups.hyperparameters,\n        max(test_evaluations.value) as max_precision\n    from triage_metadata.model_groups\n        join triage_metadata.models using (model_group_id)\n        join test_results.evaluations using (model_id)\n    where\n        metric = 'precision@'\n        and parameter = '100_abs'\n    group by 1,2,3\n    order by 4 desc\n    limit 10\n</code></pre>"},{"location":"experiments/running/#inspecting-an-experiment-before-running","title":"Inspecting an Experiment before running","text":"<p>Before you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples:</p> <ul> <li><code>experiment.all_as_of_times</code> for debugging temporal config. This will show all dates that features and labels will be calculated at.</li> <li><code>experiment.feature_dicts</code> will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment</li> <li><code>experiment.matrix_build_tasks</code> will output a list representing each matrix that will be built.</li> </ul>"},{"location":"experiments/running/#optimizing-experiment-performance","title":"Optimizing Experiment Performance","text":""},{"location":"experiments/running/#profiling-an-experiment","title":"Profiling an Experiment","text":"<p>Experiment running slowly? Try the <code>profile</code> keyword argument, or <code>--profile</code> in the command line. This will output a cProfile file to the project path's <code>profiling_stats</code> directory.  This is a binary format but can be read with a variety of visualization programs.</p> <p>snakeviz - A browser based graphical viewer. tuna - Another browser based graphical viewer gprof2dot - A command-line tool to convert files to graphviz format pyprof2calltree - A command-line tool to convert files to Valgrind log format, for viewing in established viewers like KCacheGrind</p> <p>Looking at the profile through a visualization program, you can see which portions of the experiment are taking up the most time. Based on this, you may be able to prioritize changes. For instance, if cohort/label/feature table generation are taking up the bulk of the time, you may add indexes to source tables, or increase the number of database processes. On the other hand, if model training is the culprit, you may temporarily try a smaller grid to get results more quickly.</p>"},{"location":"experiments/running/#materialize_subquery_fromobjs","title":"materialize_subquery_fromobjs","text":"<p>By default, experiments will inspect the <code>from_obj</code> of every feature aggregation to see if it looks like a subquery, create a table out of it if so, index it on the <code>knowledge_date_column</code> and <code>entity_id</code>, and use that for running feature queries. This can make feature generation go a lot faster if the <code>from_obj</code> takes a decent amount of time to run and/or there are a lot of as-of-dates in the experiment. It won't do this for <code>from_objs</code> that are just tables, or simple joins (e.g. <code>entities join events using (entity_id)</code>) as the existing indexes you have on those tables should work just fine.</p> <p>You can turn this off if you'd like, which you may want to do if the <code>from_obj</code> subqueries return a lot of data and you want to save as much disk space as possible. The option is turned off by passing <code>materialize_subquery_fromobjs=False</code> to the Experiment.</p>"},{"location":"experiments/running/#build-features-independently-of-cohort","title":"Build Features Independently of Cohort","text":"<p>By default the feature queries generated by your feature configuration on any given date are joined with the cohort table on that date, which means that no features for entities not in the cohort are saved. This is to save time and database disk space when your cohort on any given date is not very large and allow you to iterate on feature building quickly by default. However, this means that anytime you change your cohort, you have to rebuild all of your features. Depending on your experiment setup (for instance, multiple large cohorts that you experiment with), this may be time-consuming. Change this by passing <code>features_ignore_cohort=True</code> to the Experiment constructor, or <code>--save-all-features</code> to the command-line.</p>"},{"location":"experiments/running/#parallelize-big-models","title":"Parallelize Big Models","text":"<p>The model training runs in three batches, each holding different classifiers. By default, different types of classifiers go into each batch: 1. Simple classifiers that are quick to train, such as Logistic Regression, Decision Trees, and any baseline classifiers. These are first to allow the Triage user to look at their results in the database when they complete and get quick feedback on the features/cohort/labels. 2. 'Big' classifiers, such as Random Forests, ExtraTrees, and different boosting classifiers such as LightGBM and XGBoost. These come next as they generally have the best results.  3. All other classifiers. These are parallelized with <code>n_processes</code>.</p> <p>The following options relate to parallelization:</p> <ul> <li><code>--n-processes</code> (CLI) / <code>n_processes</code> (Python) - Controls how many parallel processes are used to train in batches 1 and 3. It is generally safe to set this to a high number to get simple classifier results as fast as possible.</li> <li><code>--n-bigtrain-processes</code> (CLI) / <code>n_bigtrain_processes</code> (Python) - Controls how many parallel processes are used to train in batch 2. The default is 1, which makes sense if n_jobs on the hyperparameters is set to -1. In this way, the big classifiers will run serially which is safer from a memory perspective. But power users could set <code>n_bigtrain_processes</code> to something greater than 1 if memory isn't a problem.</li> <li><code>--add-bigtrain-classes</code> (CLI) / <code>additional_bigtrain_classnames</code> (Python) - Adds a classpath into batch 2. The default list of 'big' classifiers should be good for most Triage users, but if you have a boosting library that isn't recognized by Triage yet, you could add it here. Note: The CLI option is a repeatable option, which requires a <code>--</code> at the end so the shell can understand when the option list is done. Example: <code>triage --add-bigtrain-classes my.class.path1 my.class.path2 -- experiment_config.yaml</code></li> </ul>"},{"location":"experiments/running/#experiment-classes","title":"Experiment Classes","text":"<ul> <li>SingleThreadedExperiment: An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline.</li> <li>MultiCoreExperiment: An experiment that makes use of the pebble library to parallelize various time-consuming steps. Takes an <code>n_processes</code> keyword argument to control how many workers to use.</li> <li>RQExperiment: An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra ( <code>pip install triage[rq]</code> )</li> </ul>"},{"location":"experiments/temporal-validation/","title":"Temporal Validation Deep Dive","text":"<p>A temporal validation deep dive is currently available in the Dirty Duck tutorial. Dirty Duck - Temporal Cross-validation</p> <p>You can produce the time graphs detailed in the Dirty Duck deep dive using the Triage CLI or through calling Python code directly. The graphs use matplotlib, so you'll need a matplotlib backend to use. Refer to the matplotlib docs for more details.</p>"},{"location":"experiments/temporal-validation/#python-code","title":"Python Code","text":"<p>Plotting is supported through the <code>visualize_chops</code> function, which takes a fully configured Timechop object. You may store the configuration for this object in a YAML file if you wish and load from a file, but in this example we directly set the parameters as arguments to the Timechop object. This would enable faster iteration of time config in a notebook setting.</p> <pre><code>from triage.component.timechop.plotting import visualize_chops\nfrom triage.component.timechop import Timechop\n\nchopper = Timechop(\n    feature_start_time='2010-01-01'\n    feature_end_time='2015-01-01'   # latest date included in features\n    label_start_time='2012-01-01' # earliest date for which labels are avialable\n    label_end_time='2015-01-01' # day AFTER last label date (all dates in any model are &lt; this date)\n    model_update_frequency='6month' # how frequently to retrain models\n    training_as_of_date_frequencies='1day' # time between as of dates for same entity in train matrix\n    test_as_of_date_frequencies='3month' # time between as of dates for same entity in test matrix\n    max_training_histories=['6month', '3month'] # length of time included in a train matrix\n    test_durations=['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end)\n    training_label_timespans=['1month'] # time period across which outcomes are labeled in train matrices\n    test_label_timespans=['7day'] # time period across which outcomes are labeled in test matrices\n)\n\nvisualize_chops(chopper)\n</code></pre> <p>If you'd like to view the <code>as_of_dates</code> created in each of the training-validation sets, you can use the <code>chop_time</code> method on the <code>chopper</code> object. This may be useful if the dates in the visualization are hard to read. The code below walks through the output of <code>chop_time</code>.</p> <pre><code>train_val_sets = chopper.chop_time() # outputs a list\n\n# How many distinct train-validation sets were created?\nlen(train_val_sets) \n\n# Get the most recent train-val set (last element)\nmost_recent_set = train_val_sets[-1] \n\n# Info about the training and test data for the most recent set\n# including earliest/latest as_of_dates, all the included as_of_dates\nmost_recent_set['train_matrix'] \nmost_recent_set['test_matrices'] \n</code></pre>"},{"location":"experiments/temporal-validation/#triage-cli","title":"Triage CLI","text":"<p>The Triage CLI exposes the <code>showtimechops</code> command which just takes a YAML file as input. This YAML file is expected to have a <code>temporal_config</code> section with Timechop parameters. You can use a full experiment config, or just create a YAML file with only temporal config parameters; the temporal config just has to be present. Here, we use the example_experiment_config.yaml from the Triage repository root as an example.</p> <p><code>triage experiment example_experiment_config.yaml --show-timechops</code></p>"},{"location":"experiments/temporal-validation/#result","title":"Result","text":"<p>Using either method, you should see output similar to this:</p> <p></p>"},{"location":"experiments/upgrade-to-v5/","title":"Upgrading your experiment configuration to v5","text":"<p>This document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible.</p> <p>In the experiment configuration v5, several things were changed:</p> <ul> <li><code>state_config</code> becomes <code>cohort_config</code>, and receives new options</li> <li><code>label_config</code> is changed to take a parameterized query</li> <li><code>model_group_keys</code> is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them.</li> </ul>"},{"location":"experiments/upgrade-to-v5/#state_config-cohort_config","title":"state_config -&gt; cohort_config","text":"<p>Upgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level <code>state_config</code> key, whereas now it is in the optional <code>dense_states</code> key within the top-level <code>cohort_config</code> key.</p> <p>Old:</p> <pre><code>state_config:\n    table_name: 'states'\n    state_filters:\n        - 'state_one AND state_two'\n        - '(state_one OR state_two) AND state_three'\n</code></pre> <p>New:</p> <pre><code>cohort_config:\n   dense_states:\n        table_name: 'states'\n        state_filters:\n        - 'state_one AND state_two'\n        - '(state_one OR state_two) AND state_three'\n</code></pre>"},{"location":"experiments/upgrade-to-v5/#label_config","title":"label_config","text":"<p>The label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional <code>include_missing_labels_in_train_as</code> boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed.</p> <p>Old:</p> <pre><code>events_table: 'events'\n</code></pre> <p>New:</p> <pre><code>label_config:\n    query: |\n        select\n        events.entity_id,\n        bool_or(outcome::bool)::integer as outcome\n        from events\n        where '{as_of_date}' &lt;= outcome_date\n            and outcome_date &lt; '{as_of_date}'::timestamp + interval '{label_timespan}'\n            group by entity_id\n</code></pre>"},{"location":"experiments/upgrade-to-v5/#model_group_keys","title":"model_group_keys","text":"<p>The model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly.</p> <p>Old (empty, using defaults):</p> <p>New:</p> <pre><code>model_group_keys: ['class_path', 'parameters', 'feature_names']\n</code></pre> <p>Old (more standard in practice, adding some temporal parameters):</p> <pre><code>model_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history']\n</code></pre> <p>New: <pre><code>model_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history']\n</code></pre></p>"},{"location":"experiments/upgrade-to-v5/#upgrading-the-experiment-config-version","title":"Upgrading the experiment config version","text":"<p>At this point, you should be able to bump the top-level experiment config version to v5:</p> <p>Old:</p> <pre><code>config_version: 'v4'\n</code></pre> <p>New:</p> <pre><code>config_version: 'v5'\n</code></pre>"},{"location":"experiments/upgrade-to-v6/","title":"Upgrading your experiment configuration to v6","text":"<p>This document details the steps needed to update a triage v5 configuration to v6, mimicking the old behavior.</p> <p>Experiment configuration v6 includes only one change from v5: When specifying the <code>cohort_config</code>, if a <code>query</code> is given , the <code>{af_of_date}</code> is no longer quoted or casted by Triage. Instead, the user must perform the quoting and casting, as is done already for the <code>label_config</code>.</p> <p>Old:</p> <pre><code>cohort_config:\n    query: |\n        SELECT DISTINCT entity_id\n          FROM semantic.events\n         WHERE event = 'booking'\n           AND startdt &lt;@ daterange(({as_of_date} - '3 years'::interval)::date, {as_of_date})\n           AND enddt &lt; {as_of_date}\n         LIMIT 100\n    name: 'booking_last_3_years_limit_100'\n</code></pre> <p>New:</p> <pre><code>cohort_config:\n    query: |\n        SELECT DISTINCT entity_id\n          FROM semantic.events\n         WHERE event = 'booking'\n           AND startdt &lt;@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date)\n           AND enddt &lt; '{as_of_date}'\n         LIMIT 100\n    name: 'booking_last_3_years_limit_100'\n</code></pre>"},{"location":"experiments/upgrade-to-v6/#upgrading-the-experiment-config-version","title":"Upgrading the experiment config version","text":"<p>At this point, you should be able to bump the top-level experiment config version to v6:</p> <p>Old:</p> <pre><code>config_version: 'v5'\n</code></pre> <p>New:</p> <pre><code>config_version: 'v6'\n</code></pre>"},{"location":"experiments/upgrade-to-v7/","title":"Upgrading your experiment configuration to v7","text":"<p>This document details the steps needed to update a triage v6 configuration to v7, mimicking the old behavior.</p> <p>Experiment configuration v7 includes only one change from v6: the addition of a mandatory random_seed, that is set at the beginning of the experiment and affects all subsequent random numbers. It is expected to be an integer.</p> <p>Old: <pre><code>config_version: 'v6'\n\n# EXPERIMENT METADATA\n</code></pre></p> <p>New: <pre><code>config_version: 'v7'\n\n# EXPERIMENT METADATA\n# random_seed will be set in Python at the beginning of the experiment and \n# affect the generation of all model seeds\nrandom_seed: 23895478\n</code></pre></p>"},{"location":"experiments/upgrade-to-v8/","title":"Upgrading your experiment configuration to v8","text":"<p>This document details the steps needed to update a triage v6 configuration to v8, mimicking the old behavior.</p> <p>Experiment configuration v8 includes only one change from v7: the <code>groups</code> key is no longer supported in the feature configuration (all features must be grouped only at the <code>entity_id</code> level).</p> <p>Old: <pre><code>config_version: 'v7'\n\n# FEATURE GENERATION\nfeature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates_imputation:\n      count:\n        type: 'zero_noflag'\n\n    aggregates:\n      -\n        quantity:\n          total: \"*\"\n        metrics:\n          - 'count'\n\n    intervals: ['all']\n\n    groups:\n      - 'entity_id'\n</code></pre></p> <p>New: <pre><code>config_version: 'v8'\n\n# FEATURE GENERATION\nfeature_aggregations:\n  -\n    prefix: 'inspections'\n    from_obj: 'semantic.events'\n    knowledge_date_column: 'date'\n\n    aggregates_imputation:\n      count:\n        type: 'zero_noflag'\n\n    aggregates:\n      -\n        quantity:\n          total: \"*\"\n        metrics:\n          - 'count'\n\n    intervals: ['all']\n</code></pre></p>"},{"location":"postmodeling/","title":"Postmodeling","text":"<p>The bulk of postmodeling is documented on its README. Read it here</p>"},{"location":"postmodeling/#crosstabs","title":"Crosstabs","text":"<p>One of the features in postmodeling, detailed in the README above, is the ability to view crosstabs. Using the <code>.crosstabs</code> property on the <code>ModelEvaluator</code> requires the <code>test_results.crosstabs</code> table to be created first.  You can do this either with the CLI or in a Python console:</p> <p>The model crosstabs populates a table in a postgres database containing <code>model_id</code>, <code>as_of_date</code>, <code>threshold_unit</code>, <code>threshold_value</code>, and <code>feature_column</code>, such as the mean and standard deviation of the given feature for the predicted-high-risk and predicted-low-risk group. In other words, this table provides simple descriptives of the feature distributions for the high-risk and low-risk entities, given a specific model and decision threshold.</p> <p>The crosstabs config consist of the following parameters:</p> <ol> <li> <p>Output schema and table - the name of the schema and table in the postgresdb where the results should be pushed to.</p> </li> <li> <p>Lists of thresholds (abs and/or pct) to serve as cutoff for high risk (positive) and low risk(negative) predictions.</p> </li> <li> <p>(optional) a list of entity_ids to subset for the crosstabs the analysis</p> </li> <li> <p>Models list query must return a column <code>model_id</code>. You can pass an explicit array of model ids using <code>unnest(ARRAY[1,2,3]):: int</code> or you can query by all model ids from a given model group or by dates, it's up to you (as long as it returns a column <code>model_id</code>)</p> </li> <li> <p>A list of dates query. Very similar to the previous point, you can either unnest a pre-defined list of dates or execute a more complex query that returns a column <code>as_of_date</code>.</p> </li> <li> <p>The models_dates_join_query is supposed to be fixed. Just change this if you are really sure.  This query is necessary because different model_ids can be used for predicting at multiple as_of_dates we need to make sure that model_id, as_of_date pairs really exist in a table containing predictions. </p> </li> <li> <p>The features query is used to specify a feature table or (joins of multiple feature tables) that should be joined with the models_dates_join_query results.</p> </li> <li> <p>Finally, the predictions query should return a model_id, as_of_date, entity_id, score, label_value, rank_abs and rank_pct columns.</p> </li> </ol>"},{"location":"postmodeling/#cli","title":"CLI","text":"<p><code>triage crosstabs example/config/postmodeling_crosstabs.yaml</code> will run crosstabs for the given config YAML. The config YAML is highly dependent on what model ids and as-of-dates and feature tables are in the database. That example file needs to be modified to work with your experiment and postmodeling interests. Consult the instructions above for help in modifying the file.</p>"},{"location":"postmodeling/#python","title":"Python","text":"<p>This can be run using the <code>triage.component.postmodeling.crosstabs.run_crosstabs</code> function, which takes in a database engine and a loaded CrosstabsConfigLoader object. Example:</p> <pre><code>from triage.component.postmodeling.crosstabs import CrosstabsConfigLoader, run_crosstabs\nfrom sqlalchemy import create_engine\n\ndb_engine = create_engine(&lt;mydburl&gt;)\nconfig = CrosstabsConfigLoader(config_file='example/config/postmodeling_crosstabs.yaml')\nrun_crosstabs(db_engine, config)\n</code></pre>"},{"location":"postmodeling/postmodeling-config/","title":"Postmodeling & Crosstabs Configuration","text":""},{"location":"postmodeling/postmodeling-config/#postmodeling-configuration","title":"Postmodeling Configuration","text":"<p>The Triage Postmodeling module is controlled by two config files: <code>postmodeling_config.yaml</code> and <code>postmodeling_crosstabs.yaml</code>.</p>"},{"location":"postmodeling/postmodeling-config/#postmodeling-configuration-file","title":"Postmodeling Configuration File","text":"<p>Configuration for the Triage Postmodeling module. An example <code>postmodeling_config.yaml</code> file can be found here.</p> <ul> <li><code>project_path</code>: Project path defined in triage with matrices and models</li> <li><code>audition_output_path</code>: Audition output path</li> <li><code>model_group_id</code>: List of model_id's [optional if a audition_output_path is given]</li> <li><code>thresholds</code>: Thresholds for defining positive predictions</li> <li><code>baseline_query</code>: SQL query for defining a baseline for comparison in plots. It needs a metric and parameter</li> <li><code>max_depth_error_tree</code>: For error trees, how depth the decision trees should go?</li> <li><code>n_features_plots</code>: Number of features for importances</li> <li><code>figsize</code>: Default size for plots</li> <li><code>fontsize</code>: Default fontsize for plots</li> </ul>"},{"location":"postmodeling/postmodeling-config/#postmodeling-crosstabs-configuration-file","title":"Postmodeling Crosstabs Configuration File","text":"<p>Configuration for crosstabs in Triage's Postmodeling module. An example <code>postmodeling_crosstabs.yaml</code> file can be found here.</p> <ul> <li><code>output</code>: Define the schema and table for crosstabs</li> <li><code>thresholds</code>: Thresholds for defining positive predictions</li> <li><code>entity_id_list</code>: (optional) a list of <code>entity_ids</code> to subset on the crosstabs analysis</li> <li><code>models_list_query</code>: SQL query for getting <code>model_id</code>s</li> <li><code>as_of_dates_query</code>: SQL query for getting <code>as_of_date</code>s</li> <li><code>models_dates_join_query</code>: don't change the default query unless strictly necessary. It is just validating pairs of (<code>model_id</code>, <code>as_of_date</code>) in a predictions table</li> <li><code>features_query</code>: features_query must join <code>models_dates_join_query</code> with 1 or more features table using <code>as_of_date</code></li> <li><code>predictions_query</code>: the predictions query must return <code>model_id</code>, <code>as_of_date</code>, <code>entity_id</code>, <code>score</code>, <code>label_value</code>, <code>rank_abs</code> and <code>rank_pct</code>. It must join <code>models_dates_join_query</code> using both <code>model_id</code> and <code>as_of_date</code>. </li> </ul>"},{"location":"predictlist/","title":"Retrain and Predict","text":"<p>Use an existing model group to retrain a new model on all the data up to the current date and then predict forward into the future.</p>"},{"location":"predictlist/#examples","title":"Examples","text":"<p>Both examples assume you have already run a Triage Experiment in the past, and know these two pieces of information: 1. A <code>model_group_id</code> from a Triage model group that you want to use to retrain a model and generate prediction 2. A <code>prediction_date</code> to generate your predictions on.</p>"},{"location":"predictlist/#cli","title":"CLI","text":"<p><code>triage retrainpredict &lt;model_group_id&gt; &lt;prediction_date&gt;</code></p> <p>Example: <code>triage retrainpredict 30 2021-04-04</code></p> <p>The <code>retrainpredict</code> will assume the current path to be the 'project path' to train models and write matrices, but this can be overridden by sending the <code>--project-path</code> option</p>"},{"location":"predictlist/#python","title":"Python","text":"<p>The <code>Retrainer</code> class from <code>triage.predictlist</code> module can be used to retrain a model and predict forward.</p> <pre><code>from triage.predictlist import Retrainer\nfrom triage import create_engine\n\nretrainer = Retrainer(\n    db_engine=create_engine(&lt;your-db-info&gt;),\n    project_path='/home/you/triage/project2'\n    model_group_id=36,\n)\nretrainer.retrain(prediction_date='2021-04-04')\nretrainer.predict(prediction_date='2021-04-04')\n</code></pre>"},{"location":"predictlist/#output","title":"Output","text":"<p>The retrained model is sotred similariy to the matrices created during an Experiment: - Raw Matrix saved to the matrices directory in project storage - Raw Model saved to the trained_model directory in project storage - Retrained Model info saved in a table (triage_metadata.models) where model_comment = 'retrain_2021-04-04 21:19:09.975112' - Predictions saved in a table (triage_production.predictions) - Prediction metadata (tiebreaking, random seed) saved in a table (triage_produciton.prediction_metadata)</p>"},{"location":"predictlist/#predictlist","title":"Predictlist","text":"<p>If you would like to generate a list of predictions on already-trained Triage model with new data, you can use the 'Predictlist' module.</p>"},{"location":"predictlist/#predict-foward-with-existed-model","title":"Predict Foward with Existed Model","text":"<p>Use an existing model object to generate predictions on new data.</p>"},{"location":"predictlist/#examples_1","title":"Examples","text":"<p>Both examples assume you have already run a Triage Experiment in the past, and know these two pieces of information: 1. A <code>model_id</code> from a Triage model that you want to use to generate predictions 2. An <code>as_of_date</code> to generate your predictions on.</p>"},{"location":"predictlist/#cli_1","title":"CLI","text":"<p><code>triage predictlist &lt;model_id&gt; &lt;as_of_date&gt;</code></p> <p>Example: <code>triage predictlist 46 2019-05-06</code></p> <p>The predictlist will assume the current path to be the 'project path' to find models and write matrices, but this can be overridden by sending the <code>--project-path</code> option.</p>"},{"location":"predictlist/#python_1","title":"Python","text":"<p>The <code>predict_forward_with_existed_model</code> function from the <code>triage.predictlist</code> module can be used similarly to the CLI, with the addition of the database engine and project storage as inputs. <pre><code>from triage.predictlist import generate predict_forward_with_existed_model \nfrom triage import create_engine\n\npredict_forward_with_existed_model(\n    db_engine=create_engine(&lt;your-db-info&gt;),\n    project_path='/home/you/triage/project2'\n    model_id=46,\n    as_of_date='2019-05-06'\n)\n</code></pre></p>"},{"location":"predictlist/#output_1","title":"Output","text":"<p>The Predictlist is stored similarly to the matrices created during an Experiment: - Raw Matrix saved to the matrices directory in project storage - Predictions saved in a table (triage_production.predictions) - Prediction metadata (tiebreaking, random seed) saved in a table (triage_production.prediction_metadata)</p>"},{"location":"predictlist/#notes","title":"Notes","text":"<ul> <li>The cohort and features for the Predictlist are all inferred from the Experiment that trained the given model_id (as defined by the experiment_models table).</li> <li>The feature list ensures that imputation flag columns are present for any columns that either needed to be imputed in the training process, or that needed to be imputed in the predictlist dataset.</li> </ul>"}]}