---
# CONFIG_VERSION
# The experiment configuration changes from time to time, and we upgrade the
# triage.experiments.CONFIG_VERSION variable whenever drastic changes that break
# old configuration files are released. Be sure to assign the config version
# that matches the triage.experiments.CONFIG_VERSION in the triage release 
# you are developing against!
config_version: 'v3'

# EXPERIMENT METADATA
# model_comment (optional) will end up in the model_comment column of the
# models table for each model created in this experiment
model_comment: 'test'

# TIME SPLITTING
# The time window to look at, and how to divide the window into
# train/test splits
temporal_config:
    feature_start_time: '1995-01-01' # earliest date included in features
    feature_end_time: '2015-01-01'   # latest date included in features
    label_start_time: '2012-01-01' # earliest date for which labels are avialable
    label_end_time: '2015-01-01' # day AFTER last label date (all dates in any model are < this date)
    model_update_frequency: '6month' # how frequently to retrain models
    training_as_of_date_frequencies: '1day' # time between as of dates for same entity in train matrix
    test_as_of_date_frequencies: '3month' # time between as of dates for same entity in test matrix
    max_training_histories: ['6month', '3month'] # length of time included in a train matrix
    test_durations: ['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end)
    training_label_timespans: ['1month'] # time period across which outcomes are labeled in train matrices
    test_label_timespans: ['7day'] # time period across which outcomes are labeled in test matrices

# LABEL GENERATION
# Information needed to generate labels
#
# An events table is expected, with the columns:
#   entity_id - an identifier for which the labels are applied to
#   outcome_date - The date at which some outcome was known
#   outcome - A boolean outcome
# These are used to generate appropriate labels for each train/test split
events_table: 'events'


# FEATURE GENERATION
# The aggregate features to generate for each train/test split
#
# Implemented by wrapping collate: https://github.com/dssg/collate
# Most terminology here is taken directly from collate
#
# Each entry describes a collate.SpacetimeAggregation object, and the
# arguments needed to create it. Generally, each of these entries controls
# the features from one source table, though in the case of multiple groups
# may result in multiple output tables
#
# Rules specifying how to handle imputation of null values must be explicitly
# defined in your config file. These can be specified in two places: either
# within each feature or overall for each type of feature (aggregates_imputation,
# categoricals_imputation, array_categoricals_imputation). In either case, a rule must be given for 
# each aggregation function (e.g., sum, max, avg, etc) used, or a catch-all
# can be specified with `all`. Aggregation function-specific rules will take
# precedence over the `all` rule and feature-specific rules will take 
# precedence over the higher-level rules. Several examples are provided below.
#
# Available Imputation Rules:
#   * mean: The average value of the feature (for SpacetimeAggregation the 
#           mean is taken within-date).
#   * constant: Fill with a constant value from a required `value` parameter.
#   * zero: Fill with zero.
#   * null_category: Only available for categorical features. Just flag null 
#                    values with the null category column. 
#   * binary_mode: Only available for aggregate column types. Takes the modal 
#                  value for a binary feature.
#   * error: Raise an exception if any null values are encountered for this 
#            feature.
feature_aggregations:
    -
        # prefix given to the resultant tables
        prefix: 'prefix'
        # from_obj is usually a source table but can be an expression, such as
        # a join (ie 'cool_stuff join other_stuff using (stuff_id)')
        from_obj: 'cool_stuff'
        # The date column to use for specifying which records to include
        # in temporal features. It is important that the column used specifies
        # the date at which the event is known about, which may be different
        # from the date the event happened.
        knowledge_date_column: 'open_date'

        # top-level imputation rules that will apply to all aggregates functions
        # can also specify categoricals_imputation or array_categoricals_imputation
        # 
        # You must specified at least one of the top-level or feature-level imputation
        # to cover ever feature being defined.
        aggregates_imputation:
            # The `all` rule will apply to all aggregation functions, unless over-
            # ridden by a more specific one
            all:
                # every imputation rule must have a `type` parameter, while some
                # (like 'constant') have other required parameters (`value` here)
                type: 'constant'
                value: 0
            # specifying `max` here will take precedence over the `all` rule for
            # aggregations using a MAX() function
            max:
                type: 'mean'

        # aggregates and categoricals define the actual features created. So
        # at least one is required
        #
        # Aggregates of numerical columns. Each quantity is a number of some
        # sort, and the list of metrics are applied to each quantity
        aggregates:
            -
                quantity: 'homeless::INT'
                # Imputation rules specified at the level of specific features
                # will take precedence over the higer-level rules specified
                # above. Note that the 'count' and 'sum' metrics will be
                # imputed differently here.
                imputation:
                    count:
                        type: 'mean'
                    sum:
                        type: 'constant'
                        value: 137
                metrics:
                    - 'count'
                    - 'sum'
            - 
                # since we're specifying `aggregates_imputation` above,
                # a feature-specific imputation rule can be omitted
                quantity: 'some_flag'
                metrics:
                    - 'max'
                    - 'sum'
        # Categorical features. The column given can be of any type, but the
        # choices must comparable to that type for equality within SQL
        # The result will be one feature for each choice/metric combination
        categoricals:
            -
                column: 'color'
                # note that we haven't specified a top level `categoricals_imputation`
                # set of rules, so we have to include feature-specific imputation
                # rules for both of our categoricals here.
                imputation:
                    sum:
                        type: 'null_category'
                    max:
                        type: 'mean'
                choices:
                    - 'red'
                    - 'blue'
                    - 'green'
                metrics:
                    - 'sum'
            -
                column: 'shape'
                # as with the top-level imputation rules, `all` can be used
                # for the feature-level rules to specify the same type of
                # imputation for all aggregation functions
                imputation:
                    all:
                        type: 'zero'
                choice_query: 'select distinct shape from cool_stuff'
                metrics:
                    - 'sum'
        # The time intervals over which to aggregate features
        intervals:
            - '1 year'
            - '2 years'
            - 'all'
        # A list of different columns to separately group by
        groups:
            - 'entity_id'

# FEATURE GROUPING
# define how to group features and generate combinations
# feature_group_definition allows you to create groups/subset of your features
# by different criteria.
# for instance,
# - 'tables' allows you to send a list of collate feature tables (collate builds these by appending 'aggregation_imputed' to the prefix)
# - 'prefix' allows you to specify a list of feature name prefixes
feature_group_definition:
    tables: ['prefix_aggregation_imputed']

# strategies for generating combinations of groups
# available: all, leave-one-out, leave-one-in
feature_group_strategies: ['all']

# STATE MANAGEMENT (optional)
# If you want to only include rows in your matrices in a specific state,
# provide:
# 1. a dense state table that defines when entities were in specific states
#   should have columns entity_id/state/start_time/end_time
# 2. a list of state filtering SQL clauses to iterate through. Assuming the
#   states are boolean columns (the experiment will convert the one you pass in
#   to this format), write a SQL expression for each state
#   configuration you want, ie '(permitted OR suspended) AND licensed'
state_config:
    table_name: 'states'
    state_filters:
        - 'state_one AND state_two'
        - '(state_one OR state_two) AND state_three'

# USER METADATA
# These are arbitrary keys/values that you can have Triage apply to the
# metadata for every matrix in the experiment. Any keys you include here can
# be used in the 'model_group_keys' below. For example, if you run experiments
# that share a temporal configuration but that use different label definitions
# (say, labeling building inspections with *any* violation as positive or
# labeling only building inspections with severe health and safety violations
# as positive), you can use the user metadata keys to indicate that the matrices
# from these experiments have different labeling criteria. The matrices from the
# two experiments will have different filenames (and not be overwritten or
# inappropriately reused), and if you add the label_definition key to the model
# group keys, models made on different label definition will have different
# groups. In this way, user metadata can be used to expand Triage beyond its
# explicitly supported functionality.
user_metadata:
    label_definition: 'severe_violations'

# MODEL GROUPING
# Model groups are aimed at defining models which are equivalent across time splits.
# In other words, you will probably want to define model groups by any  parameters
# that distinguish models *other than the beginning end dates of their data.*
# By default, the classifier module name, hyperparameters, and feature names are used.
# 
#
# model_group_keys defines a list of *additional* matrix metadata keys that
# should be considered when creating a model group. For example, if the models are
# built on matrices with different history lengths (train durations), different
# labeling windows (e.g., inspection violations in the next month, next year, or
# next two years) the frequency of rows for each entity(train example frequency), or
# the definition of a positive label.
model_group_keys:
    - 'train_duration'
    - 'label_window'
    - 'example_frequency'
    - 'label_definition'

# GRID CONFIGURATION
# The classifier/hyperparameter combinations that should be trained
#
# Each top-level key should be a class name, importable from triage. sklearn is
# available, and if you have another classifier package you would like available,
# contribute it to requirement/main.txt
#
# Each lower-level key is a hyperparameter name for the given classifier, and
# each value is a list of potential values. All possible combinations of
# classifiers and hyperparameters are trained.
grid_config:
    'sklearn.ensemble.ExtraTreesClassifier':
        n_estimators: [100,100]
        criterion: [gini, entropy]
        max_depth: [1,5,10,20,50]
        max_features: [sqrt,log2]
        min_samples_split: [2,5,10]
    # catwalk's custom model wrapper ScaledLogisticRegression will
    # automatically scale the train data prior to fitting the model
    # and then use the same scaling for the test data
    'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression':
        penalty: ['l1', 'l2'],
        C: [0.01, 1]
    # catwalk's PercentileRankOneFeature baseline will score each entity as
    # as its percentile on the named feature. This is useful for comparing
    # predictive models to simply ranking entities on a single feature.
    'triage.component.catwalk.baselines.rankers.PercentileRankOneFeature':
        feature: ['feature_one', 'feature_two'],
        descend: True
    # catwalk's SimpleThresholder baseline will evaluate each entity against
    # a list of rules and classify entities as 1 based on whether they meet
    # any or all of these rules, depending on whether 'or' or 'and' is
    # passed as the logical_operator. This is useful for comparing
    # predictive modeling against simple rule-based classification.
    'triage.component.catwalk.baselines.thresholders.SimpleThresholder':
        rules: [['feature_one > 3', 'feature_two <= 5']]
        logical_operator: 'and'


# MODEL SCORING
# How each trained model is scored
#
# Each entry in 'metric_groups' needs a list of one of the metrics defined in
# catwalk.evaluation.ModelEvaluator.available_metrics (contributions welcome!)
# Depending on the metric, either thresholds or parameters
#
# Parameters specify any hyperparameters needed. For most metrics,
# which are simply wrappers of sklearn functions, these
# are passed directly to sklearn.
#
# Thresholds are more specific: The list is subset and only the
# top percentile or top n entities are scored
#
# sort_seed, if passed, will seed the random number generator for each model's
# metric creation phase. This affects how entities with the same probabilities
# are sorted
scoring:
    sort_seed: 5
    metric_groups:
        -
            metrics: [precision@, recall@]
            thresholds:
                percentiles: [5.0, 10.0]
                top_n: [5, 10]
        -
            metrics: [f1]
        -
            metrics: [fbeta@]
            parameters:
                -
                    beta: 0.75
                -
                    beta: 1.25


# INDIVIDUAL IMPORTANCES
# How feature importances for individuals should be computed
# There are two variables here:
# methods: Refer to *how to compute* individual importances.
#   Each entry in this list should represent a different method.
#   Available methods are in the catwalk library's:
#   `catwalk.individual_importance.CALCULATE_STRATEGIES` list
#   Will default to 'uniform', or just the global importances.
#
# n_ranks: The number of top features per individual to compute importances for
#   Will default to 5
#
# This entire section can be left blank,
# in which case the defaults will be used.
individual_importance:
    methods: ['uniform']
    n_ranks: 5
