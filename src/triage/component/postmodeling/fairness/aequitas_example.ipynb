{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Aequitas for Bias and Fairness in Data Science Systems\n",
    "## Exploring Aequitas's visualization tools both to audit a single model for fairness as well as lookinng at trade-offs between fairness and accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this notebook do?\n",
    "\n",
    "This notebook describes how to use the results of Aequitas to visualize fairness metrics for your models. In order to use, you must have first run Triage with a bias_audit_config and populated the Aequitas database tables. This particular visualization example uses the 2014 Donors Choose data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies, import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import triage.component.postmodeling.fairness.aequitas_utils as au\n",
    "import aequitas.plot as ap\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Database and Select Data From Aequitas\n",
    "Your database configuration file should be in the following form:\n",
    "<h3 align=\"center\">\n",
    "host: __ \n",
    "user: __\n",
    "db: __\n",
    "pass: __\n",
    "port: __\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'database.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-672130ae729c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdbconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'database.yaml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdbconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'postgres://'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnect_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdbconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'database.yaml'"
     ]
    }
   ],
   "source": [
    "# Configure Database\n",
    "dbconfig = {}\n",
    "with open('database.yaml') as file:\n",
    "    dbconfig = yaml.load(file, Loader=yaml.FullLoader)\n",
    "dbconfig = {\n",
    "    'host': dbconfig['host'],\n",
    "    'username': dbconfig['user'],\n",
    "    'database': dbconfig['db'],\n",
    "    'password': dbconfig['pass'],\n",
    "    'port': dbconfig['port']\n",
    "    }\n",
    "conn = sqlalchemy.create_engine('postgres://', connect_args=dbconfig)\n",
    "\n",
    "# Load Data\n",
    "bdf = au.get_aequitas_results(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Attributes to Audit and Reference Group for each Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_and_reference_groups={'poverty_level':'lower', 'metro_type':'suburban_rural', 'teacher_sex':'male'}\n",
    "attributes_to_audit = list(attributes_and_reference_groups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select fairness metric(s) that we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['tpr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define  Disparity Tolerance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity_tolerance = 1.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Audit Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we are going to focus our analysis on the fairness metric(s) of interest in this case study: TPR across different groups. The aequitas plot module exposes the disparities_metrics() plot, which displays both the disparities and the group-wise metric results side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Fairness in Poverty Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5d7a77bab4b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisparity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'poverty_level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfairness_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisparity_tolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ap' is not defined"
     ]
    }
   ],
   "source": [
    "ap.disparity(bdf, metrics, 'poverty_level', fairness_threshold = disparity_tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.absolute(bdf, metrics, 'poverty_level', fairness_threshold = disparity_tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Fairness in Metro_Type (where the school is based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.disparity(bdf, metrics, 'metro_type', fairness_threshold = disparity_tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.absolute(bdf, metrics, 'metro_type', fairness_threshold = disparity_tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Fairness in the Sex of the Teacher submitting the project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ap.disparity(bdf, metrics, 'teacher_sex', fairness_threshold = disparity_tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.absolute(bdf, metrics, 'teacher_sex', fairness_threshold = disparity_tolerance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}