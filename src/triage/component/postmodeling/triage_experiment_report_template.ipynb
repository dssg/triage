{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1151d4c-01f1-4412-8d0b-821fe436185b",
   "metadata": {},
   "source": [
    "## Modeling Report Triage\n",
    "\n",
    "This notebook summarizes the following aspects of the modeling experiment: \n",
    "\n",
    "- The predictors we created\n",
    "- The temporal crossvalidation setup we used to validate our models\n",
    "- The models we ran\n",
    "- The results we got interms of the efficiency, effectiveness, and equity metrics\n",
    "- A deeper dive into what the ML models are learning from the data to make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65e6cc-865e-42ca-a0e8-07ee5af775cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from sqlalchemy.engine.url import URL\n",
    "from triage.util.db import create_engine\n",
    "\n",
    "\n",
    "from triage.component.postmodeling.modeling_report_functions import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 120})\n",
    "%matplotlib inline\n",
    "\n",
    "# suppress logging messages \n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d292beb-6652-4922-a0fe-ca3edc1bad92",
   "metadata": {},
   "source": [
    "## Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc689d-c660-4767-be13-16ab4c1bfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = URL(\n",
    "            'postgres',\n",
    "            host=os.getenv('PGHOST'),\n",
    "            username=os.getenv('PGUSER'),\n",
    "            database=os.getenv('PGDATABASE'),\n",
    "            password=os.getenv('PGPASSWORD'),\n",
    "            port=5432,\n",
    "        )\n",
    "\n",
    "db_engine = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa2eaf-13e5-45c4-970a-1345e97c8754",
   "metadata": {},
   "source": [
    "## 1. Parameters for the Report\n",
    "\n",
    "The following values are the default parameters for the report. If you are using this interactively, you can change the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c2dd2-b87d-46a9-9b47-c80f45f3d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most recent completed experiment hash\n",
    "# Note that this has to be a list\n",
    "experiment_hashes = [get_most_recent_experiment_hash(db_engine)]\n",
    "\n",
    "# Model Performance metric and threshold defaulting to reacll@1_pct\n",
    "performance_metric = 'recall@'\n",
    "threshold = '1_pct'\n",
    "\n",
    "# Bias metric defaults to tpr_disparity and bias metric values for all groups generated (if bias audit specified in the experiment config)\n",
    "bias_metric = 'tpr_disparity'\n",
    "bias_priority_groups=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d5c35-5190-434c-bcb0-b24dd235ba9a",
   "metadata": {},
   "source": [
    "#### 1.1 Updating the parameters based on the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc5745-fa2c-4ceb-a750-93fe00b38783",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_report_parameters_from_config(db_engine, experiment_hashes[0])\n",
    "\n",
    "if params['performance_metric'] is not None:\n",
    "    performance_metric = params['performance_metric']\n",
    "\n",
    "if params['threshold'] is not None:\n",
    "    threshold = params['threshold']\n",
    "\n",
    "if params['bias_metric'] is not None:\n",
    "    bias_metric = params['bias_metric']\n",
    "\n",
    "if params['priority_groups'] is not None:\n",
    "    bias_priority_groups = params['priority_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a026e7c-618e-41be-8301-19a82a8741df",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metric, threshold, bias_metric, bias_priority_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acfb937-f1d5-4e65-b354-8ea0652b1511",
   "metadata": {},
   "source": [
    "## 2. Visualizing the Temporal Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ede889-9c20-4053-8d6e-9eee5940320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_validation_splits(db_engine, experiment_hashes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148929a-831d-478a-ac3b-4039348e8274",
   "metadata": {},
   "source": [
    "## 3. Modeling Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3da62-5eb9-4427-8d1e-e403fc7923b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_summary = summarize_cohorts(db_engine, experiment_hashes[0], generate_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73f6d1-1bbc-4bf3-9e6d-5788600088bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795ebe4-ea51-4b43-8091-a57c7f8dd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_summary[['cohort_size', 'baserate']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c7722-bb9b-45ce-aedf-0220ac11ee12",
   "metadata": {},
   "source": [
    "## 4. Predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c478f4-cf59-4d63-a788-0ef06d1cb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list_all_features(db_engine, experiment_hashes[0])\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384a493-a418-4cd1-8426-f6fd9110cbc8",
   "metadata": {},
   "source": [
    "### 4.1 Missingness of Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b9536-c759-4d69-bc3d-e7531c5e9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_missingness_stats(db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cdeac-4c71-4615-b0a8-caff2d33333e",
   "metadata": {},
   "source": [
    "## 5. Model Groups Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57511ca-df6d-4162-986e-9e210ee9dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_model_groups(db_engine, experiment_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756979c0-f8e0-4ef6-959e-ea4c2e4a1622",
   "metadata": {},
   "source": [
    "## 6. All Models Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626865b4-d002-4fb1-a7fa-d33fe29d0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_models(db_engine, experiment_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6552a7-7768-4c92-afe9-3b873ecb02ae",
   "metadata": {},
   "source": [
    "## 7. Model Performance\n",
    "\n",
    "### 7(a) Overall Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3264cf3-8535-4ecb-930e-897e1156c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric='precision@'\n",
    "parameter = '100_abs'\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 4), dpi=200)\n",
    "plot_performance_all_models(db_engine, experiment_hashes, metric, parameter)\n",
    "# ax.legend(loc='upper center', fontsize='medium', ncol=3, bbox_to_anchor=[0.5, 1.3], frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d502f88-c71e-4ec1-9cdc-95b0d7b47f77",
   "metadata": {},
   "source": [
    "### 7(b) Cohort subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebeb0d-e324-46c6-a0fe-337d8e9e389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset_performance(db_engine, experiment_hashes, parameter,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33776e-9162-4953-8394-6870bfa613d9",
   "metadata": {},
   "source": [
    "## 8. Model Performance vs Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e28a90-2e63-4d96-9978-9b17d6a48d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance_against_bias(\n",
    "    engine=db_engine,\n",
    "    experiment_hashes=experiment_hashes,\n",
    "    metric=metric,\n",
    "    parameter=parameter,\n",
    "    bias_metric=bias_metric,\n",
    "    groups=None # This attribute need to be updated for \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fd40f-2202-4b20-beaa-931e2945fee4",
   "metadata": {},
   "source": [
    "## 9. Precision-Recall Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24188e24-32f8-49a3-8f23-fde83618c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prk_curves(db_engine, experiment_hashes, step_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88975e61-aacd-445d-aebe-0616dd2b14de",
   "metadata": {},
   "source": [
    "## 10. Initial Model Selection and Further analysis on best models\n",
    "For the purposes of this report, by default, we pick the best performing model from each model type based on average performance to generate additional outputs about the developed models. We would not assume the existence of predictions at this stage. Therefore, we will not do analysis such as list comparisons, crosstabs, score distribution type stuff. we'll look at more higher level comparisons between the different model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf05a9f-d29d-4b02-908d-b85913305881",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = get_best_hp_config_for_each_model_type(db_engine, experiment_hashes, metric, parameter)\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf28dc-1ea2-4417-a87f-a27c50e9ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    rep = PostmodelingReport(\n",
    "        engine=db_engine,\n",
    "        experiment_hashes=experiment_hashes,\n",
    "        model_groups=best_models.index.tolist()\n",
    "    )\n",
    "except Exception as e:\n",
    "    rep = None\n",
    "    logging.error('No best models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d459e-daf3-48ee-9861-f87aaee8b29b",
   "metadata": {},
   "source": [
    "### 10.1 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e029a5-b940-4d1e-bf61-d90b4ba2ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rep:\n",
    "    rep.plot_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c21136-325e-4ea0-98ce-4d574bfd450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rep: \n",
    "    rep.plot_feature_group_importance(n_top_groups=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f9991-9d78-46c5-a5c6-3fa1ed58b60f",
   "metadata": {},
   "source": [
    "### 10.2 Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fc0ae-53e7-4de0-9e94-845d50a04625",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rep:\n",
    "    rep.plot_recall_curves_overlaid(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a3d53-074b-4714-a3cb-15ab6305735d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80a829-112c-4275-ad48-5665b6251cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8e035-f61e-4eda-8e3d-e9aa21d7b836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
