# Post-modeling Analysis

> ¿What is the distribution of my scores? ¿What is generating a higher FPR in
> model x compared to model y? ¿What is the single most important feature in my
> models?

This questions, and other ones, are the kind of inquiries that the `triage`user
may have in mind when scrolling trough the models selected by the `Audition`
component. Choosing the right model for deployment and exploring its quirks and
behavior is a pivotal task. `postemodeling` will help to answer some of this
questions by exploring the outcomes of the model, and exploring "deeply" into
the algorithm behavior. 

This library lays at the end of the `triage` pipeline and will use the output of
`Audition` and some of its selection rules as a main input. The
`postmodeling_tutorial.ipynb` notebook contains a user guide with some questions
that the component is able to answer about the models, but the methods are
expandable and allow the user to develop new insights.  

Here is a list of preliminary functions included in the Component and their
status:

## Components

We can analyze models by getting its individual metrics or comparing between
them. The first step would include a set of basic metric for each model, such
as:
 - [ ] Raw score distributions: histogram of the scores
 - [x] Score distributions: compare the distributions of labels in the test
   matrix across the predicted score.
 - [ ] Feature importances: 
 
 ...Where possible, `triage` will calculate the general feature importances for
 each model (for Random Forest and other decision trees, this process is made
 extracting the `_feature_importances` from `sklearn`). Nonetheless, when
 available we can also user `feature_groups` as a way to improve the
 interpretability of the feature importances. 
 
 - [ ] Feature importances using standard deviations, and other ways to get
   these. 
 - [ ] Matrix metrics: basic descriptives of the test matrix (n, labels, etc.)
 - [x] Model metrics: ROC Curve, Precision vs. Recall, Recall and FDR vs.
   Depth
 - [ ] Error trees with different labels (i.e. complete residuals, FPR, and
   FNR).
 
The second step comprises different ways of comparing models: 
* List comparisons: Compare list generated by each model:
 - [x] Jaccard Similarity over *top_k* predictions and features
 - [x] Overall rank correlation over *top_k* predictions and features
 - [ ] Rank Correlation where we can label [0 vs. 1]
 - [ ] KL divergence to compare distributions in feature importances

Postmodeling also involves the correction of scores and a more in-depth analysis
of the selected models. Some of this tasks can be also achieved using this
library, like:

 - [ ] Probability Calibration: probability calculations for score deciles
 - [ ] Crosstabs: t-test comparing feature values between classified groups.
 - [ ] Bias report: *Aequitas* 

A [notebook](https://github.com/dssg/triage/blob/cli_postmodeling/src/triage/component/postmodeling/contrast/postmodeling_tutorial.ipynb)
is included with a tutorial of the library using the [Dirty
Duck](https://github.com/dssg/dirtyduck) data. 

