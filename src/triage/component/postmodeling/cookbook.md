# Post-modeling 


## Introduction 

Post-modeling takes part in the "Model Selection" stage in a AI/ML/DS project life cycle. in this the phase we select the model that we would like to validate on a field trial from a subset of models previously selected based mainly in performance and bias metrics aligned with the goals of the project defined in terms of efficiency, effectiveness, and equity.  

<img src="project_life_cycle.png" alt="drawing" width="350"/>

The goal of this phase is to understand and characterize the entities shown on the top k of the lists generated by the selected subset of models, so that you: 

+ Know the characteristics of the entities that your model will serve 
+ Know the differences in the values of the features used by your model between the entities in your top k vs the rest of the list 
+ Know the differences between the entities in your top k and other top k lists from different models 
+ Identify the entities that are present in different lists 
+ Know the features associated with your errors in top k (FP) and the errors in the rest of your list (FN)
+ Know the entities flagged by your model and missed by others 
+ Know the entities flagged by other models missed by yours

In order to get all this information, you will require to do different type of analyses 

Types of analyses in this playbook: 

+ [Crosstabs](#crosstabs) 
+ [Lists similarities](#lists-similarities)
+ [Score distribution](#score-distribution)
+ [List characteristics](#list-characteristics) 
+ [Outcomes on label window](#outcomes-on-label-window)
+ [Outcomes after label window](#outcomes-after-label-window)
+ [Error analysis FPs] 
+ [Error analysis FNs] 


### Crosstabs 

This analysis gives you information about the differences in values of the features used in a particular model, is calcualed as the mean ratio between the top *k* entities and the rest of the list (by default). You can also do crosstabs between the top *k* lists of different models, as long as they share the features used as predictors. 

To generate crosstabs in a particular model (`model_id`) you will need: 

ü•ï **Ingredients:** 

+ A `model id`
+ Predictions generated and stored in the database. In case you don't have them you can follow [this recipe](#recipe-generating-predictions-and-storing-them-in-db-after-the-experiment-has-run-in-triage).
+ A connection to the DB. In case you don't have one you can follow [this recipe](#recipe-creating-a-database-engine)

üë©‚Äçüç≥ **How to cook:** 

```python
from triage.component.postmodeling.base import SingleModelAnalyzer

# in case your matrices and trained models are stored in S3 buckets
project_path = 's3://name-of-your-bucket/triage_output/'
# in case your matrices and trained models are stored in local File System
#project_path = "/mnt/data/path/to/your/project/triage_output/'
thresholds = {'rank_abs_no_ties': 100} # top 100 abs 

model_analyzer = SingleModelAnalyzer(model_id_, db_conn)
model_analyzer.crosstabs_pos_vs_neg(project_path, thresholds)
```

üç≤ **What to look for** 

Triage should have created (or append rows) to the table `crosstabs` in your DB with the crosstabs for all the model ids in your model groups. You can retrieve the calculations with the following snippet of code: 

```python
q = f"""
    select 
        model_group_id,
        model_id,
        model_type, 
        train_end_time, 
        metric,
        feature,
        value 
    from triage_metadata.experiment_models a 
    join triage_metadata.models b
        using (model_hash)
    join test_results.crosstabs c
        using (model_id)
    where experiment_hash in ('{scorecard_simple_threshold_hash}', '{ml_experiments_hash}', '{new_model_all_features_hash}')
    and model_group_id in ({model_group_scorecard_simple_threshold}, {model_group_original_rf}, {model_group_new_model_all_features})
    and metric = 'mean_ratio'
    limit 10
"""

pd.read_sql(q, db_conn)
```
Be aware that crosstabs generates the mean ratios for **all** the features used in your models, it can be overwhelming to go through all features to get relevant information that help you characterize your top *k* entities. We suggest to get the top 20 features with the biggest difference in values between the top *k* and the rest of the list (`mean_ratio` metric in the crosstabs table) and adjust as necessary. 

$\rightarrow$ Bear in mind that some of mean ratios have infinity values (because the denominator was 0), if you don't want to retrieve those and focus only on the features that have the biggest difference you can add a condition when retrieving your results from the DB like the following snippet of code in which we are retrieving for each time split on a model group (each model id) the top 20 most different features ratios. 

```python
q = f"""
   with most_diff as (
        select 
            model_group_id,
            model_id,
            model_type, 
            train_end_time,
            dense_rank() over (partition by model_id order by value desc) as rank_, 
            metric,
            feature,
            value 
        from triage_metadata.experiment_models a 
        join triage_metadata.models b
            using (model_hash)
        join test_results.crosstabs c
            using (model_id)
        where experiment_hash in ('{scorecard_simple_threshold_hash}', '{ml_experiments_hash}', '{new_model_all_features_hash}')
        and model_group_id in ({model_group_scorecard_simple_threshold}, {model_group_original_rf}, {model_group_new_model_all_features})
        and metric = 'mean_ratio'
        and value is not null
        and value != 'infinity'
    ), 

    top_20 as (
        select * 
        from most_diff 
        where rank_ < 21 
        and metric in ('mean_ratio', 'mean_predicted_positive', 'mean_predicted_negative', 'support_predicted_positive') 
    )

    select 
        model_group_id,
        model_id, 
        model_type,
        train_end_time::date,
        rank_, 
        a.metric, 
        feature,
        a.value
    from test_results.crosstabs a 
    join top_20 b
        using (model_id, feature) 
"""

crosstabs = pd.read_sql(q, db_conn)
```

### Lists similarities

This analysis will let you identify how similar &ndash;or disimilar&ndash; two top *k* lists are. Based on three different metrics: 

1. **Jaccard similarity:** This is telling you how similar are the lists based on the set created by the union of two lists. [recipe](#jaccard-similiarity)
2. **Overlap:** How much intersection is between a pair of top k lists. This is telling you how many entities are present in both lists. You can also think about this as a way to see if the lists are highlighting different entities. This is very useful when you're trying to answer if one model could replace another one &ndash;like a current process implemented by the partner&ndash;.
3. **Rank correlation:** How similar is the ranking between lists. This is telling you if the order in which the entities in one top *k* list is roughly the same &ndash;increasing or decresing monotonically (1, -1)&ndash; or if the order doesn't have any correlation at all(value of 0). 

We'll go through the recipies of each metric: 

#### Jaccard similiarity

ü•ï **Ingredients**

+ A list of `model_group_id`s  
+ A metric and threshold of interest, e.g., `precision@100_abs` 

üë©‚Äçüç≥ **How to cook:** 

üç≤ **What to look for**


#### Overlap 

ü•ï **Ingredients**

+  A list of `model_group_id`s

üë©‚Äçüç≥ **How to cook:** 

üç≤ **What to look for**

#### Rank correlation

ü•ï **Ingredients**

üë©‚Äçüç≥ **How to cook:** 

üç≤ **What to look for**


### Score distribution

### List characteristics

### Outcomes on label window 

### Outcomes after label window

### Basic extra recipes


#### Recipe: Creating a database engine 

To communicate with the database you will need to create a database engine. To create it, you will need: 

ü•ï **Ingredients**

+ An SQLAlchemy URL **or** a credentials `yaml` file **or** environment variables whith the database credentials 

üë©‚Äçüç≥ **How to cook:** 

```python 
from triage.util.db import create_engine
from sqlalchemy.engine.url import URL

# if credentials are stored in a credentials yaml file
db_url = URL(
            'postgres',
            host=dbconfig['host'],
            username=dbconfig['user'],
            database=dbconfig['db'],
            password=dbconfig['pass'],
            port=dbconfig['port'],
        )

# if credentials of the DB are stored in environment variables
# db_url = URL(
#     'postgres',
#     host=os.getenv('PGHOST'),
#     username=os.getenv('PGUSER'),
#     database=os.getenv('PGDATABASE'),
#     password=os.getenv('PGPASSWORD'),
#     port=os.getenv('PGPORT'),
# )

db_conn = create_engine(db_url)
```

üç≤ **What to look for** 

You can verify that you created a succesfull connection to your DB with the following python code: 

```python
q = """
    select run_hash, start_time
    from triage_metadata.triage_runs
    order by 2 desc
    limit 1 
    """

pd.read_sql(q, db_conn)
```

If the code runs, your connection was succesfull!

#### Recipe: Generating predictions and storing them in DB after the experiment has run in Triage

As a good practice, you don't save the predictons of your experiments until you have selected a subset of them based (mainly) on their performance $\rightarrow$ You can use our Audition module to select this subsets of "best" models based on different strategies. 

To generate predictions for a specified model (`model_id`) you can use the following recipe on a notebook (we suggest the one your are using for your postmodeling analysis üôÇ): 

ü•ï **Ingredients:** You are going to need: 

+ A database connection 
+ The model group that has the model id that you are interested on generating the predictions for (it will generate the predictions for all the timechops, that's why it requires the model group)
+ The model hash (is optional, but is better to put use it)
+ The project path from where your feature matrices and trained models are stored 

üë©‚Äçüç≥ **How to cook:** 

```python
from triage.component.postmodeling.add_predictions import add_predictions

# in case your matrices and trained models are stored in S3 buckets
project_path = 's3://name-of-your-bucket/triage_output/'
# in case your matrices and trained models are stored in local File System
#project_path = "/mnt/data/path/to/your/project/triage_output/' 

 add_predictions(db_conn, 
                 [51, 135],  #list of model groups, even if you only send 1 model group send it as a list
                 project_path, 
                 ['f2614123549000597dbda80cb6e629b4', 'e367965c86a197dbf624245d5bea0203'] # list of experiment hashes associated with the model groups (optional!)
                )
```

üç≤ **What to look for** 

Now you can go to your database and confirm that the predictions have been generated and saved in the database with the following sql script: 

```sql
select as_of_date, count(*)
from triage_metadata.experiment_models a 
join triage_metadata.models b
    using (model_hash)
join test_predictions.predictions c
    using (model_id)
where experiment_hash in ('f2614123549000597dbda80cb6e629b4', 'e367965c86a197dbf624245d5bea0203')
and group_model_id in (51, 135)
group by 1
order by 1
```

