# Post-modeling 


## Introduction 

Post-modeling takes part in the "Model Selection" stage in a AI/ML/DS project life cycle. in this the phase we select the model that we would like to validate on a field trial from a subset of models previously selected based mainly in performance and bias metrics aligned with the goals of the project defined in terms of efficiency, effectiveness, and equity.  

<img src="project_life_cycle.png" alt="drawing" width="350"/>

The goal of this phase is to understand and characterize the entities shown on the top k of the lists generated by the selected subset of models, so that you: 

+ Know the characteristics of the entities that your model will serve 
+ Know the differences in the values of the features used by your model between the entities in your top k vs the rest of the list 
+ Know the differences between the entities in your top k and other top k lists from different models 
+ Identify the entities that are present in different lists 
+ Know the features associated with your errors in top k (FP) and the errors in the rest of your list (FN)
+ Know the entities flagged by your model and missed by others 
+ Know the entities flagged by other models missed by yours

In order to get all this information, you will require to do different type of analyses 

Types of analyses in this playbook: 

+ Crosstabs 
+ Overlaps 
+ List characteristics 
+ Outcomes on label window 
+ Outcomes after label window 
+ Error analysis FPs 
+ Error analysis FNs 


### Crosstabs 

This analysis gives you information about the differences in values of the features used in a particular model, is calcualed as the mean ratio between the top *k* entities and the rest of the list (by default). You can also do crosstabs between the top *k* lists of different models, as long as they share the features used as predictors. 

To generate crosstabs on a particular model (`model_id`) you will need: 

+ A `model id`
+ Predictions generated and stored in the database. In case you don't have them you can follow [this recipe]().
+ A connection to the DB



#### Recipe: Generating predictions and storing them in DB after the experiment has run in Triage

As a good practice, you don't save the predictons of your experiments until you have selected a subset of them based (mainly) on their performance $\rightarrow$ You can use our Audition module to select this subsets of "best" models based on different strategies. 

To generate predictions for a specified model (`model_id`) you can use the following recipe on a notebook (we suggest the one your are using for your postmodeling analysis ðŸ™‚): 

Ingredients: You are going to need: 

+ A database connection 
+ The model group that has the model id that you are interested on generating the predictions for (it will generate the predictions for all the timechops, that's why it requires the model group)
+ The model hash (is optional, but is better to put use it)
+ The project path from where your feature matrices and trained models are stored 

How to cook: To generate the predictions you 


~~~
from triage.component.postmodeling.add_predictions import add_predictions

# in case your matrices and trained models are stored in S3 buckets
project_path = 's3://name-of-your-bucket/triage_output/'
# in case your matrices and trained models are stored in local File System
#project_path = "/mnt/data/path/to/your/project/triage_output/' 

 add_predictions(db_conn, 
                 [51, 135],
                 project_path, 
                 ['f2614123549000597dbda80cb6e629b4', 'e367965c86a197dbf624245d5bea0203'])
~~~

