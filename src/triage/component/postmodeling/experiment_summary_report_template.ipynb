{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1151d4c-01f1-4412-8d0b-821fe436185b",
   "metadata": {},
   "source": [
    "# Triage Experiment Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65e6cc-865e-42ca-a0e8-07ee5af775cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from yaml import safe_load\n",
    "from sqlalchemy.engine.url import URL\n",
    "from triage.util.db import create_engine\n",
    "from triage.component.postmodeling.experiment_summarizer import ExperimentReport, get_most_recent_experiment_hash, load_report_parameters_from_config\n",
    "\n",
    "\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 120})\n",
    "\n",
    "# suppress logging messages that make report long\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d292beb-6652-4922-a0fe-ca3edc1bad92",
   "metadata": {},
   "source": [
    "## Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"database.yaml\", \"r\") as f:\n",
    "    config = safe_load(f)\n",
    "\n",
    "db_url = URL(\n",
    "            'postgres',\n",
    "            host=config['host'],\n",
    "            username=config['user'],\n",
    "            password=config['pass'],\n",
    "            database=config['db'],\n",
    "            port=config['port']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc689d-c660-4767-be13-16ab4c1bfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you have your db credentials as environment variables uncomment this code\n",
    "# db_url = URL(\n",
    "#             'postgres',\n",
    "#             host=os.getenv('PGHOST'),\n",
    "#             username=os.getenv('PGUSER'),\n",
    "#             database=os.getenv('PGDATABASE'),\n",
    "#             password=os.getenv('PGPASSWORD'),\n",
    "#             port=5432,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_engine = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa2eaf-13e5-45c4-970a-1345e97c8754",
   "metadata": {},
   "source": [
    "## 1. Initializing the Report\n",
    "Initializing the Report generation class. Before we do that, we need to establish some parameters\n",
    "\n",
    "#### 1.1 Default Parameters\n",
    "The following values are the default parameters for the report. If you are using this interactively, you can change the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c2dd2-b87d-46a9-9b47-c80f45f3d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most recent completed experiment hash\n",
    "# Note that this has to be a list\n",
    "# You can replace the get_most_recent_... function call with your list of experiments to analyse different experiments\n",
    "experiment_hashes = [get_most_recent_experiment_hash(db_engine)]\n",
    "\n",
    "\n",
    "# Model Performance metric and threshold defaulting to reacll@1_pct\n",
    "# You can update these to use different metrics, e.g., precision@, 100_abs\n",
    "performance_metric = 'recall@'\n",
    "threshold = '1_pct'\n",
    "\n",
    "# Bias metric defaults to tpr_disparity and bias metric values for all groups generated (if bias audit specified in the experiment config)\n",
    "bias_metric = 'tpr_disparity'\n",
    "bias_priority_groups=None\n",
    "### bias_priority_groups example \n",
    "# bias_priority_groups = {'race': ['African American'], \n",
    "#                         'gender': ['Female']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d5c35-5190-434c-bcb0-b24dd235ba9a",
   "metadata": {},
   "source": [
    "#### 1.2 Updating the parameters based on the experiment configuration (YAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9dcee",
   "metadata": {},
   "source": [
    "If you set the following parameters in the experiment config, the following codeblock will update the parameters \n",
    "\n",
    "```yaml\n",
    "    scoring:\n",
    "        # Append these key-value pairs to the scoring section\n",
    "        priority_metric: 'recall@'\n",
    "        priority_parameter: '1_pct' \n",
    "      \n",
    "    bias_audit:\n",
    "        ## Append these key-value pairs to the bias_audit section (if a bias audit is performed)\n",
    "        priority_metric: 'tpr_disparity'\n",
    "\n",
    "        priority_groups:\n",
    "          'race':\n",
    "            - 'African American'\n",
    "          'gender':\n",
    "            - 'Female'\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc5745-fa2c-4ceb-a750-93fe00b38783",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_report_parameters_from_config(db_engine, experiment_hashes[0])\n",
    "\n",
    "if params['performance_metric'] is not None:\n",
    "    performance_metric = params['performance_metric']\n",
    "\n",
    "if params['threshold'] is not None:\n",
    "    threshold = params['threshold']\n",
    "\n",
    "if params['bias_metric'] is not None:\n",
    "    bias_metric = params['bias_metric']\n",
    "\n",
    "if params['priority_groups'] is not None:\n",
    "    bias_priority_groups = params['priority_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a026e7c-618e-41be-8301-19a82a8741df",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metric, threshold, bias_metric, bias_priority_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = ExperimentReport(\n",
    "    engine=db_engine,\n",
    "    experiment_hashes=experiment_hashes,\n",
    "    performance_priority_metric=performance_metric,\n",
    "    threshold=threshold,\n",
    "    bias_priority_metric=bias_metric,\n",
    "    bias_priority_groups=bias_priority_groups\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad98e3b-b5f9-431a-b949-bc6c5f23ffd6",
   "metadata": {},
   "source": [
    "## 2. Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3f302-b3c9-4fd4-925e-64b82c184acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.generate_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acfb937-f1d5-4e65-b354-8ea0652b1511",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Temporal Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ede889-9c20-4053-8d6e-9eee5940320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.timesplits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148929a-831d-478a-ac3b-4039348e8274",
   "metadata": {},
   "source": [
    "## 4. Modeling Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3da62-5eb9-4427-8d1e-e403fc7923b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_summary = rep.cohorts(generate_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795ebe4-ea51-4b43-8091-a57c7f8dd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_summary[['cohort_size', 'baserate']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2833d-7152-4b57-96e4-1e6c96a96c04",
   "metadata": {},
   "source": [
    "#### Cohort Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207f53e-460c-4d15-8e45-eca409d81e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.subsets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c7722-bb9b-45ce-aedf-0220ac11ee12",
   "metadata": {},
   "source": [
    "## 4. Predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c478f4-cf59-4d63-a788-0ef06d1cb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = rep.features()\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384a493-a418-4cd1-8426-f6fd9110cbc8",
   "metadata": {},
   "source": [
    "#### 4.1 Missingness of Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b9536-c759-4d69-bc3d-e7531c5e9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.feature_missingness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cdeac-4c71-4615-b0a8-caff2d33333e",
   "metadata": {},
   "source": [
    "## 5. Model Groups Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57511ca-df6d-4162-986e-9e210ee9dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.model_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756979c0-f8e0-4ef6-959e-ea4c2e4a1622",
   "metadata": {},
   "source": [
    "## 6. All Models Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626865b4-d002-4fb1-a7fa-d33fe29d0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6552a7-7768-4c92-afe9-3b873ecb02ae",
   "metadata": {},
   "source": [
    "## 7. Model Performance\n",
    "\n",
    "#### 7.1 Overall Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3264cf3-8535-4ecb-930e-897e1156c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = rep.model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f51a5e-aca8-40c8-a293-85969af81259",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d502f88-c71e-4ec1-9cdc-95b0d7b47f77",
   "metadata": {},
   "source": [
    "#### 7.2 Cohort subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebeb0d-e324-46c6-a0fe-337d8e9e389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_subset_performance(db_engine, experiment_hashes, parameter,metric)\n",
    "subset_evaluations = rep.model_performance_subsets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33776e-9162-4953-8394-6870bfa613d9",
   "metadata": {},
   "source": [
    "## 8. Model Performance vs Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e28a90-2e63-4d96-9978-9b17d6a48d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_metrics = rep.efficiency_and_equity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88975e61-aacd-445d-aebe-0616dd2b14de",
   "metadata": {},
   "source": [
    "## 9. Initial Model Selection and Further analysis on best models\n",
    "For the purposes of this report, by default, we pick the best performing model from each model type based on average performance to generate additional outputs about the developed models. We would not assume the existence of predictions at this stage. Therefore, we will not do analysis such as list comparisons, crosstabs, score distribution type stuff. we'll look at more higher level comparisons between the different model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf05a9f-d29d-4b02-908d-b85913305881",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.get_best_hp_config_for_each_model_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826ff1f-909d-44b2-a71f-a92ddf7b93c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f70ef-dc7e-48bd-a54f-f3a66344cd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0f303-adb4-43a6-b7bc-5afcc108d720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8e035-f61e-4eda-8e3d-e9aa21d7b836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
