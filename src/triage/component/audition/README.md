# Audition

Choosing the best classifier models

[![Build Status](https://travis-ci.org/dssg/audition.svg?branch=master)](https://travis-ci.org/dssg/audition)
[![codecov](https://codecov.io/gh/dssg/audition/branch/master/graph/badge.svg)](https://codecov.io/gh/dssg/audition)
[![codeclimate](https://codeclimate.com/github/dssg/audition.png)](https://codeclimate.com/github/dssg/audition)

## Overview
Audition is a tool for picking the best trained classifiers from a predictive analytics experiment. Often, production-scale experiments will come up with thousands of trained models, and sifting through all of those results can be time-consuming even after calculating the usual basic metrics like precision and recall. Which metrics matter most? Should you prioritize the best metric value over time or treat recent data as most important? Is low metric variance important? The answers to questions like these may not be obvious up front. Audition introduces a structured, semi-automated way of filtering models based on what you consider important, with an interface that is easy to interact with from a Jupyter notebook (with plots), but is driven by configuration that can easily be scripted.

### Use Case
We’ve just built hundreds of models over time - each model group trains on a given train set and scores the test set and calculates several metrics.

### What does it do?
**Input**:

* List of model groups built over different train periods and tested on different test periods (train end times)
	* The train end times for each model group should be the same as the list or subset of the list, otherwise those models with unmatched train end times would be pruned out in the first round.
* Model selection rules
* Metric(s) of interest

**Process**:

1. Gets rid of really bad model groups wrt the metric of interest. A model group is discarded if:
	* It’s never close to the “best” model (define close to best) or
	* If it’s metric is below a certain number (define min threshold)  at least once

We iterate over various values of these parameters to end up with a reasonable number of model groups to pass ot the next step

2. Apply existing (or new, custom) selection rules to the model groups passed from step 1. Current supported rules are “best_most_recent”, “best_average” for one or two metrics, best_recent_average, “lowest_variance”, high_avg_low_variance, “most_frequent_best_distance”
    1. For each rule
        1. For each time period
            1. It applies the rule to select a model for that time period (based on performance of that model so far)
            2. It calculates the evaluation metric for that selected model on the next time period
            3. calculates regret (how much worse is the selected model compared to the best model in the next time period)
                1. Absolute value
                2. rank/percentile [todo]

3. Now we have a regret for each rule for each time period. We now have to decide on which rule to use. Do all/most rules pick the same/similar model groups? If so, then audition should output those model groups

Output:
* Top k model groups for each rule and average regret for each rule

## Preparing Experiment Results

Note: If you are familiar with the DSaPP 'results schema', you can skip this section and head to the [Using the Auditioner](#using) section.

Audition expects to be able to read experiment metadata from a relational database. This includes information about models, what we call 'model groups', and evaluations. The full experiment schema used by DSaPP post-modeling tools such as Audition is defined in the [results-schema](github.com/dssg/results-schema) repository, and is automatically populated after a [triage.Experiment](github.com/dssg/triage). However, even without using those tools, you can populate these tables for other experiments. Here's an overview of Audition's direct dependencies:

* `results.model_groups` - Everything unique about a classifier model, except for its train date. We define this as:
	* `model_group_id` - An autogenerated integer surrogate key for the model group, used as a foreign key in other tables
	* `model_type` - The name of the class, e.g 'sklearn.ensemble.RandomForestClassifier'
	* `hyperparameters` - This is a dictionary (stored in the database as JSON) that describes how the class is configured (e.g. {'criterion': 'gini', 'min_samples_split': 2})
	* `feature_list` - A list of feature names (stored in the database as an array).
	* `model_config` - A catch-all for anything else you want to uniquely define as a model group, for instance different pieces of time config. Use of this column is not strictly necessary.
* `results.models` - An instantiation of a model group at a specific training time. In production, our version of this table has quite a few column, but the only columns used by Audition are below. If you reproduce this table, these three should be sufficient:
	* `model_id` - An autogenerated integer surrogate key for the model, used as a foreign key in the 'evaluations' table.
	* `model_group_id` - A foreign key to the model_groups table.
	* `train_end_time` - A timestamp that signifies when this model was trained.
* `results.evaluations` - This is where metrics (such as precision, recall) and their values for specific models get stored.
	* `model_id` - A foreign key to the models table
	* `evaluation_start_time` - The start of the time period from which testing data was taken from.
	* `evaluation_end_time` - The end of the time period from which testing data was taken from.
	* `metric` - The name of a metric, e.g. 'precision@' for thresholded precision. Audition needs some knowledge about what direction indicates 'better' results for this metric, so it wishes that the metric is one of the options in [catwalk.ModelEvaluator.available_metrics](github.com/dssg/catwalk/blob/master/catwalk/evaluation.py#L43). If you use a metric that is not available here, it will assume that greater is better.
	* `parameter` - A string that indicates any parameters for the metric. For instance, `100_abs` indicates top-100 entities, while `5_pct` indicates top 5 percentile of entities. These are commonly used for metrics like precision and recall to prioritize the evaluation of models to how they affect the actions likely to be taken as a result of the model.
	* `value` - A float that represents the value of the metric and parameters applied to the model and evaluation time.

## <a name="using"></a>Using the Auditioner

### Setting Up the Auditioner
The primary interface for Audition is the `Auditioner` class, and a recommended way to start using it is within a Jupyter notebook so you can see plots.

The Auditioner requires a few inputs to get started:

* db_engine (sqlalchemy.engine) A database engine with access to a results schema of a completed modeling run
* model_group_ids (list) A large list of model groups to audition. No effort should
	be needed to pick 'good' model groups, but they should all be groups that could
	be used if they are found to perform well. They should also each have evaluations
	for any train end times you wish to include in analysis
* train_end_times (list) A list of train end times that all of the given model groups
	contain evaluations for and that you want to be deemed important in the analysis
* initial_metric_filters (list) A list of metrics to filter model groups on, and how to filter them. These are all expected to be in the evaluations table already for all specified models. Each entry should be a dict with the keys:
	* metric (string) -- model evaluation metric, such as 'precision@'
	* parameter (string) -- model evaluation metric parameter,
		such as '300_abs'
	* max_from_best (float) The maximum value that the given metric can be worse than the best model for a given train end time. To start out without filtering, this could be the theoretical maximum range of the metric, often '1.0'.
	* threshold_value (float) The worst absolute value that the given metric should be. Depending on the metric, this could be a minimum (precision) or a maximum (false positives)

Gets rid of really bad model groups wrt the metric of interest. A model group is discarded if:

1. It’s never close to the “best” model (define max_from_best) or
2. If it’s metric is below a certain number (define threshold_value) at least once

```python
aud = Auditioner(
    db_engine = conn,
    model_group_ids=model_groups,
    train_end_times=end_times,
    initial_metric_filters=
    	[{'metric': 'precision@',
    	  'parameter': '50_abs',
    	  'max_from_best': 1.0,
    	  'threshold_value': 0.0}],
    models_table='models',
    distance_table=best_dist'
)
# Output the thresholded model group ids
ids = aud.thresholded_model_group_ids
```
The default setting won't filter out any model group. If you see the number of model groups decrease, you should check if every model group in `model_group_ids` has the same `train_end_times`.

### Iterating on the Metric Filters
Starts to get rids of very bad models by applying `set_one_metric_filter`.

```python
aud.set_one_metric_filter(
    metric='precision@',
    parameter='50_abs',
    max_from_best=0.5,
    threshold_value=0.0)
# Output the thresholded model group ids
ids = aud.thresholded_model_group_ids
```
If that didn't thin things out too much, let's get a bit more agressive with both parameters. If we want to have multiple filters, then use `update_metric_filters` to apply a set of filters to the model groups we're considering in order to eliminate poorly performing ones. The model groups will be plotted again after updating the filters.

```python
aud.update_metric_filters([{
    'metric': 'precision@',
    'parameter': '50_abs',
    'max_from_best': 0.5,
    'threshold_value': 0.12
}])
# Output the thresholded model group ids
ids = aud.thresholded_model_group_ids
```
### Creating a Selection Rule Grid
The goal of audition is to narrow a very large number of model groups to a small number of best candidates, ideally making use of the full time series of information. There are several ways one could consider doing so, using over-time averages of the metrics of interest, weighted averages to balance between metrics, the distance from best metrics, and balancing metric average values and stability.

Audition formalizes this idea through "selection rules" that take in the data up to a given point in time, apply some rule to choose a model group, and evaluate the performance of that chosen model in the subsequent time window, the regret. You can register, evaluate, and update selection rules associated with the Auditioner object as shown below.

Audition will run simulations of different model group selection rules to show users and let users assess which rule(s) is the best for their needs. Next, Audition will output numbers of best model in current time period for each rule.

First, we need to create a selection rule grid which will be passed to aud.register_selection_rule_grid() to run the simulations. The selection rule grid is only recognized as a list of dictionaries of all the parameters. One can create this giant grid by hands, but Audition also provides some helper functions to create the grid easily.

```python
from triage.component.audition.rules_maker import SimpleRuleMaker, RandomGroupRuleMaker, TwoMetricsRuleMaker, create_selection_grid

Rule1 = SimpleRuleMaker()
Rule1.add_rule_best_current_value(metric='precision@', parameter='50_abs', n=3)
Rule1.add_rule_best_average_value(metric='precision@', parameter='50_abs', n=3)
Rule1.add_rule_lowest_metric_variance(metric='precision@', parameter='50_abs', n=3)
Rule1.add_rule_most_frequent_best_dist(
	metric='precision@',
	parameter='50_abs',
	dist_from_best_case=[0.05],
	n=3
)
Rule1.add_rule_best_avg_recency_weight(
	metric='precision@',
	parameter='50_abs',
	curr_weight=[1.5, 2.0, 5.0],
	decay_type=['linear'],
	n=1
)
Rule1.add_rule_best_avg_var_penalized(
	metric='precision@',
	parameter='50_abs',
	stdev_penalty=0.5,
	n=1
)
Rule2 = RandomGroupRuleMaker(n=1)

Rule3 = TwoMetricsRuleMaker()
Rule3.add_rule_best_average_two_metrics(
	metric1='precision@',
	parameter1='50_abs',
	metric2='precision@',
	parameter2='100_abs',
	metric1_weight=[0.5],
	n=1
)
seln_rules = create_selection_grid(Rule1, Rule2, Rule3)
```

Currently we have 7 rules:

1. `best_current_value`: Pick the model group with the best current metric value.
2. `best_average_value`: Pick the model with the highest average metric value so far.
3. `lowest_metric_variance`: Pick the model with the lowest metric variance so far.
4. `most_frequent_best_dist`: Pick the model that is most frequently within `dist_from_best_case` from the best-performing model group across test sets so far.
5. `best_average_two_metrics`: Pick the model with the highest average combined value to date of two metrics weighted together using `metric1_weight`.
6. `best_avg_var_penalized`: Pick the model with the highest average metric value so far, penalized for relative variance as:
<br>`avg_value - (stdev_penalty) * (stdev - min_stdev)`<br>
where `min_stdev` is the minimum standard deviation of the metric across all model groups
7. `best_avg_recency_weight`: Pick the model with the highest average metric value so far, penalized for relative variance as:
<br>`avg_value - (stdev_penalty) * (stdev - min_stdev)`<br>
where `min_stdev` is the minimum standard deviation of the metric across all
model groups

**Register rules**

```python
aud.register_selection_rule_grid(seln_rules, plot=True)

```

### Exporting results

```python
aud.selection_rule_model_group_ids
```

It will give us the results like

```
{'best_average_two_metrics_precision@_50_abs_precision@_100_abs_0.5': [246],
 'best_average_value_precision@_50_abs': [246, 235, 232],
 'best_avg_recency_weight_precision@_50_abs_1.5_linear': [246],
 'best_avg_recency_weight_precision@_50_abs_2.0_linear': [246],
 'best_avg_recency_weight_precision@_50_abs_5.0_linear': [202],
 'best_avg_var_penalized_precision@_50_abs_0.5': [246],
 'best_current_value_precision@_50_abs': [200, 2713, 2714],
 'lowest_metric_variance_precision@_50_abs': [2757, 2754, 2767],
 'most_frequent_best_dist_precision@_50_abs_0.05': [237, 238, 2713],
 'random_model_group': [224]}
```
The result will have each rule give you the best `n` model groups ids based on the metric and parameter following that rule for the most recent time period.
For example, rules like `best_average_value_precision@_50_abs`, `best_current_value_precision@_50_abs`, `lowest_metric_variance_precision@_50_abs`, and `most_frequent_best_dist_precision@_50_abs_0.05`, since we specified `n=3` when we created rules, return 3 model group ids. The rest of rules only has one model group id as a result.

```python
aud.average_regret_for_rules
```

This will give us an average regret for each rule.

```
{
  'precision@50_abs':{
    'best_average_two_metrics_precision@_50_abs_precision@_100_abs_0.5': 0.10533333333333333,
	 'best_average_value_precision@_50_abs': 0.09866666666666668,
	 'best_avg_recency_weight_precision@_50_abs_1.5_linear': 0.10933333333333334,
	 'best_avg_recency_weight_precision@_50_abs_2.0_linear': 0.1023401360544218,
	 'best_avg_recency_weight_precision@_50_abs_5.0_linear': 0.092,
	 'best_avg_var_penalized_precision@_50_abs_0.5': 0.09866666666666667,
	 'best_current_value_precision@_50_abs': 0.08666666666666667,
	 'lowest_metric_variance_precision@_50_abs': 0.12000000000000001,
	 'most_frequent_best_dist_precision@_50_abs_0.05': 0.10666666666666669,
	 'random_model_group': 0.12266666666666667}
 }
```
